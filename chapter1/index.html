
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://your-username.github.io/RL-Mkdocs/chapter1/">
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../chapter2/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Chapter 1: Reinforcement Learning basics - Reinforcement Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/css/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#explain-the-concept-of-reinforcement-learning-in-a-formal-way" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Reinforcement Learning Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 1: Reinforcement Learning basics
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="blue"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lightblue"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/srirams05/RL-Mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Chapter 1: Reinforcement Learning basics

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../chapter2/" class="md-tabs__link">
        
  
  
    
  
  Chapter 2: TestMath

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Reinforcement Learning Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/srirams05/RL-Mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Chapter 1: Reinforcement Learning basics
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Chapter 1: Reinforcement Learning basics
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#explain-the-concept-of-reinforcement-learning-in-a-formal-way" class="md-nav__link">
    <span class="md-ellipsis">
      Explain the concept of Reinforcement Learning in a formal way.
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-function-rsa-dependence" class="md-nav__link">
    <span class="md-ellipsis">
      Reward function \(R(s,a)\) dependence
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-policy-picdot" class="md-nav__link">
    <span class="md-ellipsis">
      The policy \(\pi(\cdot)\)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trajectory" class="md-nav__link">
    <span class="md-ellipsis">
      Trajectory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transition-probability-and-policy-probability" class="md-nav__link">
    <span class="md-ellipsis">
      Transition probability and Policy Probability
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#state-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      State value function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probability-tree-for-v_pis" class="md-nav__link">
    <span class="md-ellipsis">
      Probability tree for \(v_{\pi}(s)\)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation - Details
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation-to-system-of-linear-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation \(\to\) System of Linear Equations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation-matrix-vector-form" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation matrix-vector form
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comapring-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Comapring Policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#return-value" class="md-nav__link">
    <span class="md-ellipsis">
      Return value
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations-for-q_pi-and-v_pi" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equations for \(q_{\pi}\) and \(v_{\pi}\)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#existence-of-optimal-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Existence of optimal policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#uniqueness-of-optimal-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Uniqueness of Optimal policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-operator-i" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Operator I
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-operator-ii" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Operator II
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#uniqueness-revisited" class="md-nav__link">
    <span class="md-ellipsis">
      Uniqueness revisited
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mathcalt-is-a-contraction-mapping" class="md-nav__link">
    <span class="md-ellipsis">
      \(\mathcal{T}\) is a contraction mapping
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-operator-matrix-vector-form" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Operator - matrix vector form
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../chapter2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 2: TestMath
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Chapter 1: Reinforcement Learning basics</h1>

<h2 id="explain-the-concept-of-reinforcement-learning-in-a-formal-way">Explain the concept of Reinforcement Learning in a formal way.<a class="headerlink" href="#explain-the-concept-of-reinforcement-learning-in-a-formal-way" title="Permanent link">&para;</a></h2>
<p>Below is a step-by-step formal explanation of the basic concepts in Reinforcement Learning (RL). We will frame RL in terms of a Markov Decision Process (MDP), which is the standard mathematical model used.</p>
<ol>
<li>Markov Decision Process (MDP)<br />
   A Markov Decision Process is characterized by the tuple  </li>
</ol>
<div class="arithmatex">\[
(\mathcal{S},\,\mathcal{A},\,P,\,R,\,\gamma),
\]</div>
<p>where:<br />
   • <span class="arithmatex">\(\mathcal{S}\)</span> is the state space. Each element <span class="arithmatex">\(s \in \mathcal{S}\)</span> represents a possible configuration of the environment.<br />
   • <span class="arithmatex">\(\mathcal{A}\)</span> is the action space. Each element <span class="arithmatex">\(a \in \mathcal{A}\)</span> is a possible action the agent can take.<br />
   • <span class="arithmatex">\(P(s' \mid s,a)\)</span> is the transition probability function, which gives the probability of transitioning from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(s'\)</span> when action <span class="arithmatex">\(a\)</span> is taken. Formally,  </p>
<div class="arithmatex">\[
P(s' \mid s,a) \;=\;\Pr(S_{t+1} = s' \,\big\vert\, S_t = s,\, A_t = a).
\]</div>
<p>• <span class="arithmatex">\(R(s,a)\)</span> is the reward function, which specifies the reward the agent receives for taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. Sometimes it is also written as the expected immediate reward <span class="arithmatex">\(R: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>.<br />
   • <span class="arithmatex">\(\gamma \in [0,1]\)</span> is the discount factor, which balances the importance of immediate rewards versus future rewards. A smaller <span class="arithmatex">\(\gamma\)</span> emphasizes present rewards, while a value near <span class="arithmatex">\(1\)</span> gives more weight to future rewards.</p>
<ol>
<li>
<p>Agent-Environment Interaction<br />
   In Reinforcement Learning, an agent interacts with an environment (modeled as an MDP) over a sequence of discrete time steps <span class="arithmatex">\(t = 0,1,2,\dots\)</span>. The agent observes the state <span class="arithmatex">\(S_t\)</span>, takes an action <span class="arithmatex">\(A_t\)</span>, receives a reward <span class="arithmatex">\(R_{t+1}\)</span>, and the environment transitions to a new state <span class="arithmatex">\(S_{t+1}\)</span>. The agent’s goal is to learn a strategy (policy) to choose actions that maximize cumulative reward.</p>
</li>
<li>
<p>Policy (<span class="arithmatex">\(\pi\)</span>)<br />
   A policy <span class="arithmatex">\(\pi\)</span> is a mapping from states to probabilities of selecting each available action. Formally,  </p>
</li>
</ol>
<div class="arithmatex">\[
\pi(a \mid s) \;=\;\Pr(A_t = a \,\big\vert\,S_t = s).
\]</div>
<p>A (deterministic) policy can also be written as a function $ \pi : \mathcal{S} \to \mathcal{A}$ that picks a specific action in each state.</p>
<ol>
<li>Return (Cumulative Discounted Reward)<br />
   The return <span class="arithmatex">\(G_t\)</span> from a time step <span class="arithmatex">\(t\)</span> is defined as the total discounted reward from time <span class="arithmatex">\(t\)</span> onward. That is,  </li>
</ol>
<div class="arithmatex">\[
G_t \;=\; R_{t+1} \;+\;\gamma\,R_{t+2} \;+\;\gamma^2\,R_{t+3} \;+\;\ldots 
   \;=\; \sum_{k=0}^{\infty} \gamma^{\,k}\, R_{t+k+1}.
\]</div>
<ol>
<li>Value Functions<br />
   Value functions quantify how “good” it is to be in a certain state or to take a certain action in a given state. Two common value functions are:  </li>
</ol>
<p>a) State-Value Function<br />
   The value of a state <span class="arithmatex">\(s\)</span> under a policy <span class="arithmatex">\(\pi\)</span>, denoted <span class="arithmatex">\(v_\pi(s)\)</span>, is the expected return when starting in state <span class="arithmatex">\(s\)</span> and following <span class="arithmatex">\(\pi\)</span> thereafter. Formally,  </p>
<div class="arithmatex">\[
v_\pi(s) \;=\; \mathbb{E}\bigl[G_t \,\big\vert\, S_t = s,\;\pi \bigr].
\]</div>
<p>b) Action-Value Function<br />
   The action-value function <span class="arithmatex">\(q_\pi(s,a)\)</span> is the expected return when starting from state <span class="arithmatex">\(s\)</span>, taking action <span class="arithmatex">\(a\)</span>, and thereafter following policy <span class="arithmatex">\(\pi\)</span>. Formally,  </p>
<div class="arithmatex">\[
q_\pi(s,a) \;=\; 
   \mathbb{E}\bigl[G_t \,\big\vert\, S_t = s,\;A_t = a,\;\pi \bigr].
\]</div>
<ol>
<li>Optimal Value Functions and Optimal Policy<br />
   The goal in reinforcement learning is typically to find an optimal policy <span class="arithmatex">\(\pi^*\)</span> that maximizes the expected return from every state. We define the optimal state-value function <span class="arithmatex">\(v_*(s)\)</span> and the optimal action-value function <span class="arithmatex">\(q_*(s,a)\)</span> by  </li>
</ol>
<div class="arithmatex">\[
v_*(s) \;=\; \max_{\pi} \;v_\pi(s),
   \quad
   q_*(s,a) \;=\; \max_{\pi} \;q_\pi(s,a).
\]</div>
<p>An optimal policy <span class="arithmatex">\(\pi^*\)</span> is any policy that achieves these optimal values.</p>
<ol>
<li>Bellman Optimality Equations<br />
   The optimal value functions satisfy the Bellman optimality equations:  </li>
</ol>
<div class="arithmatex">\[
v_*(s) \;=\; \max_{a \in \mathcal{A}} \Bigl[\,R(s,a)\;+\;\gamma\sum_{s'}P(s' \mid s,a)\,v_*(s')\Bigr],
\]</div>
<div class="arithmatex">\[
q_*(s,a) \;=\; R(s,a)\;+\;\gamma\sum_{s'} P(s' \mid s,a)\,\max_{a'} q_*(s',a').
\]</div>
<p>These equations express a recursive relationship between the value of a state (or state-action pair) and the values of subsequent states (state-action pairs).</p>
<ol>
<li>Learning in Reinforcement Learning<br />
   The crux of Reinforcement Learning is that the agent does not have full knowledge of the MDP (i.e., it does not initially know <span class="arithmatex">\(P\)</span> or <span class="arithmatex">\(R\)</span>). The agent must estimate the optimal policy by interacting with the environment:<br />
   • It observes states and rewards.<br />
   • It updates estimates (e.g., of <span class="arithmatex">\(v_*(s)\)</span> or <span class="arithmatex">\(q_*(s,a)\)</span>).<br />
   • It adjusts its policy to choose better actions in the future.  </li>
</ol>
<p>Popular algorithms include:<br />
   • Temporal-Difference Methods (e.g., Q-Learning, SARSA).<br />
   • Monte Carlo Methods.<br />
   • Policy Gradient Methods.  </p>
<p>In summary, Reinforcement Learning is about an agent learning to map states of an environment to actions in order to maximize the cumulative reward. Mathematically, RL methods are designed to solve or approximate solutions to the MDP’s optimal value functions—and thus find an optimal or near-optimal policy—when the agent must learn about the environment solely through experience.</p>
<h2 id="reward-function-rsa-dependence">Reward function <span class="arithmatex">\(R(s,a)\)</span> dependence<a class="headerlink" href="#reward-function-rsa-dependence" title="Permanent link">&para;</a></h2>
<p><strong><em>It seems to me that the reward function <span class="arithmatex">\(R(s,a)\)</span> should also depend upon <span class="arithmatex">\(s’\)</span>? For a given <span class="arithmatex">\((s,a)\)</span>, the agent could transition to multiple <span class="arithmatex">\(s’\)</span> and therefore it suggests that <span class="arithmatex">\(R\)</span> should also depend on <span class="arithmatex">\(s’\)</span>. Clarify this.</em></strong></p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step clarification of why, in many MDP or Reinforcement Learning formulations, one sometimes sees the reward function written as <span class="arithmatex">\(R(s,a)\)</span> (i.e., depending only on <span class="arithmatex">\((s,a)\)</span>) and other times as <span class="arithmatex">\(R(s,a,s')\)</span> (i.e., also including the next state):</p>
<ol>
<li>Different but Equivalent MDP Definitions</li>
</ol>
<p>In the most general Markov Decision Process (MDP) definition, the one-step reward after taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> and ending up in state <span class="arithmatex">\(s'\)</span> can indeed depend on all three: </p>
<div class="arithmatex">\[
R(s,a,s') : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \;\rightarrow\; \mathbb{R}.
\]</div>
<p>This is sometimes written as <span class="arithmatex">\(R_t = R(S_t, A_t, S_{t+1})\)</span>.</p>
<p>However, many treatments simplify notation by writing</p>
<div class="arithmatex">\[
R(s,a) \quad\text{or}\quad R(s) \quad\text{or}\quad R(a)
\]</div>
<p>depending on which elements really matter for the reward in the domain under consideration.</p>
<ol>
<li>Why Using R(s,a) Can Be Enough</li>
</ol>
<p>In some problems, the one-step reward is essentially fixed once you pick <span class="arithmatex">\((s,a)\)</span>, regardless of the next state <span class="arithmatex">\(s'\)</span>. More precisely, one might define:</p>
<div class="arithmatex">\[
R(s,a) \;=\; \mathbb{E}\bigl[\,R_t \,\big\vert\,S_t=s,\,A_t=a\bigr],
\]</div>
<p>meaning that this function gives the expected immediate reward upon taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. If the environment is stochastic (i.e., multiple possible <span class="arithmatex">\(s'\)</span>), then</p>
<div class="arithmatex">\[
R(s,a) \;=\;\sum_{s'} P(s' \mid s,a)\,\tilde{r}(s,a,s'),
\]</div>
<p>where <span class="arithmatex">\(\tilde{r}\)</span> is some underlying reward for transitioning to <span class="arithmatex">\(s'\)</span>. Thus, even though the actual reward might depend on <span class="arithmatex">\(s'\)</span>, one can bundle the expectation over <span class="arithmatex">\(s'\)</span> into <span class="arithmatex">\(R(s,a)\)</span>. This keeps the notation more compact while still retaining the necessary information.</p>
<ol>
<li>When R(s,a,s') is Used Explicitly</li>
</ol>
<p>If you want to specify exactly how the reward depends on the next state <span class="arithmatex">\(s'\)</span>, or if your problem inherently requires that detail (for instance, certain transitions might yield different bonuses or penalties), you can define</p>
<div class="arithmatex">\[
R(s,a,s').
\]</div>
<p>Then, for each transition <span class="arithmatex">\((s,a)\to s'\)</span>, you would get a specific reward <span class="arithmatex">\(R(s,a,s')\)</span> rather than an expected value. This form is more explicit but can be more cumbersome in some derivations.  </p>
<ol>
<li>Equivalence in Theory</li>
</ol>
<p>Mathematically, the two formulations
(a) <span class="arithmatex">\(R(s,a)\,=\,\mathbb{E}[R(s,a,s')\,\vert\,s,a]\)</span>
and 
(b) <span class="arithmatex">\(R(s,a,s')\)</span>
are equivalent if you interpret <span class="arithmatex">\(R(s,a)\)</span> as that conditional expectation over the randomized transition to <span class="arithmatex">\(s'\)</span>. </p>
<p>Therefore, one formulation or the other does not fundamentally change the problem; it just changes how explicitly you write the dependence on <span class="arithmatex">\(s'\)</span>.</p>
<ol>
<li>Example: Chess</li>
</ol>
<p>Taking chess as an example:<br />
• One might write <span class="arithmatex">\(R(s,a) = 0\)</span> for all intermediate moves (since no immediate reward is given until checkmate). At the end of the game, you get +1, -1, or 0.<br />
• If you wanted an incremental reward for capturing certain pieces, you might define <span class="arithmatex">\(R(s,a,s')\)</span> that encodes a bonus whenever a capture move transitions you to a state <span class="arithmatex">\(s'\)</span> with fewer opponent pieces. Alternatively, you could incorporate that into an expected sense, but it becomes somewhat clearer to define it as a function of the specific transition.</p>
<ol>
<li>Summary</li>
</ol>
<p>• It is correct that there can be multiple next states <span class="arithmatex">\(s'\)</span> for a given <span class="arithmatex">\((s,a)\)</span>.<br />
• Hence, in the most general MDP definition, we often see <span class="arithmatex">\(R(s,a,s')\)</span>.<br />
• Many RL texts and simpler MDP formulations use <span class="arithmatex">\(R(s,a)\)</span> to represent the expected immediate reward (averaging over the distribution of next states <span class="arithmatex">\(s'\)</span>), making notation a bit simpler for analysis.  </p>
<p>In short: yes, the underlying reward can depend on <span class="arithmatex">\(s'\)</span>. One either writes <span class="arithmatex">\(R(s,a,s')\)</span> explicitly or uses the shorter <span class="arithmatex">\(R(s,a)\)</span> to denote the expectation over <span class="arithmatex">\(s'\)</span>. Both are valid formulations of the same concept.</p>
<h2 id="the-policy-picdot">The policy <span class="arithmatex">\(\pi(\cdot)\)</span><a class="headerlink" href="#the-policy-picdot" title="Permanent link">&para;</a></h2>
<p><strong><em>The policy <span class="arithmatex">\(\pi(a \vert s) = \text{Pr}(A_t = a \vert S_t = s)\)</span> seems to be defined for each state <span class="arithmatex">\(a\)</span>. Once a policy is executed at <span class="arithmatex">\(a\)</span>, then we are in a new state, say <span class="arithmatex">\(s’\)</span>. Is the same policy <span class="arithmatex">\(\pi\)</span> defined for <span class="arithmatex">\(s’\)</span> as well? In other words, is <span class="arithmatex">\(\pi(\cdot)\)</span> defined for the entire state space?</em></strong></p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step explanation clarifying whether the same policy <span class="arithmatex">\(\pi\)</span> is defined for every state in the state space:</p>
<ol>
<li>What a Policy Is<br />
   • In Reinforcement Learning, a policy <span class="arithmatex">\(\pi\)</span> is defined as a mapping from states to a probability distribution over actions. Formally:  </li>
</ol>
<div class="arithmatex">\[
\pi:\;\mathcal{S}\;\to\;\Delta(\mathcal{A}),
\]</div>
<p>where <span class="arithmatex">\(\Delta(\mathcal{A})\)</span> is the set of all probability distributions over the action set <span class="arithmatex">\(\mathcal{A}\)</span>.  </p>
<p>• Equivalently, we often write <span class="arithmatex">\(\pi(a \mid s)\)</span> to denote the probability of choosing action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>.</p>
<ol>
<li>
<p>Applying <span class="arithmatex">\(\pi\)</span> in an MDP Scenario<br />
   • Consider the Markov Decision Process (MDP) with state space <span class="arithmatex">\(\mathcal{S}\)</span>. At any step <span class="arithmatex">\(t\)</span>, the agent observes the current state <span class="arithmatex">\(S_t\in \mathcal{S}\)</span>.<br />
   • The policy <span class="arithmatex">\(\pi\)</span> is used to select an action <span class="arithmatex">\(A_t\)</span>. Even if <span class="arithmatex">\(\pi\)</span> is stochastic, <span class="arithmatex">\(\pi(a \mid s)\)</span> tells us how likely each action <span class="arithmatex">\(a\)</span> is, given <span class="arithmatex">\(s\)</span>.</p>
</li>
<li>
<p>Transition to the Next State<br />
   • After executing the chosen action <span class="arithmatex">\(A_t\)</span> in state <span class="arithmatex">\(S_t\)</span>, the environment transitions to a new state <span class="arithmatex">\(S_{t+1}=s'\)</span>.<br />
   • Now, at state <span class="arithmatex">\(s'\)</span>, the same policy <span class="arithmatex">\(\pi\)</span> applies; namely, to choose the next action <span class="arithmatex">\(A_{t+1}\)</span>, we again use <span class="arithmatex">\(\pi(a \mid s')\)</span>.<br />
   • Thus, one and the same <span class="arithmatex">\(\pi\)</span> determines actions for every state the agent might encounter.</p>
</li>
<li>
<p><span class="arithmatex">\(\pi\)</span> Is Defined for the Entire State Space<br />
   • By definition, a (stationary) reinforcement-learning policy gives a decision rule for every possible state in <span class="arithmatex">\(\mathcal{S}\)</span>.<br />
   • That is, for each state <span class="arithmatex">\(s\in\mathcal{S}\)</span>, <span class="arithmatex">\(\pi\)</span> prescribes how the agent should act.<br />
   • So, yes—<span class="arithmatex">\(\pi\)</span> is indeed “the same policy” for all states, in the sense that there is a single function <span class="arithmatex">\(\pi(\cdot)\)</span> that applies throughout the state space.</p>
</li>
<li>
<p>Illustration with Deterministic Policies<br />
   • If <span class="arithmatex">\(\pi\)</span> is deterministic, then for every state <span class="arithmatex">\(s\)</span>, <span class="arithmatex">\(\pi(s)\)</span> is exactly one action (rather than a distribution).<br />
   • In either case—deterministic or stochastic—<span class="arithmatex">\(\pi\)</span> unequivocally answers: “What do we do if we end up in state <span class="arithmatex">\(s\)</span>?”</p>
</li>
<li>
<p>Policy Improvement or Changes Over Time<br />
   • In some learning algorithms, the policy <span class="arithmatex">\(\pi\)</span> may be updated as the agent gains experience (for instance, by gradient steps or by policy iteration).<br />
   • However, at any particular time during training (or after training), the agent’s “current” policy is still a single function defined over all states.</p>
</li>
<li>
<p>Conclusion<br />
   • Yes, once the agent transitions to the new state <span class="arithmatex">\(s'\)</span>, the same policy <span class="arithmatex">\(\pi\)</span> is again used to pick the next action.<br />
   • Formally, a policy is defined over the entire state space <span class="arithmatex">\(\mathcal{S}\)</span>, ensuring that no matter which state is encountered, the agent knows how it should proceed.</p>
</li>
</ol>
<h2 id="trajectory">Trajectory<a class="headerlink" href="#trajectory" title="Permanent link">&para;</a></h2>
<p><strong><em>Suppose, the agent is at state <span class="arithmatex">\(s_t\)</span> at time <span class="arithmatex">\(t\)</span> under a policy <span class="arithmatex">\(\pi\)</span>. Per this policy the action <span class="arithmatex">\(a_t\)</span> is applied and the agent moves to state <span class="arithmatex">\(s_{t+1}\)</span> getting the reward <span class="arithmatex">\(R_{t+1}\)</span>. In this way, the agent traces out a trajectory in state space from which the Return <span class="arithmatex">\(G_t\)</span> for this trajectory can be calculated. Is my understanding correct?</em></strong></p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step confirmation and elaboration on your understanding, with all mathematical symbols in LaTeX:</p>
<ol>
<li>
<p>Agent in State <span class="arithmatex">\({s_t}\)</span> at Time <span class="arithmatex">\(t\)</span><br />
   At any discrete time step <span class="arithmatex">\(t\)</span>, the agent observes the current state <span class="arithmatex">\(s_t\)</span>.</p>
</li>
<li>
<p>Action <span class="arithmatex">\(a_t\)</span> per Policy <span class="arithmatex">\(\pi\)</span><br />
   According to the agent’s policy <span class="arithmatex">\(\pi\)</span>, the agent selects and executes an action <span class="arithmatex">\(a_t\)</span>.<br />
   • If <span class="arithmatex">\(\pi\)</span> is stochastic, then  </p>
</li>
</ol>
<div class="arithmatex">\[
a_t \sim \pi(\cdot \,\vert\, s_t),
\]</div>
<p>meaning <span class="arithmatex">\(a_t\)</span> is sampled from the probability distribution <span class="arithmatex">\(\pi(a\,\vert\,s_t)\)</span>.<br />
   • If <span class="arithmatex">\(\pi\)</span> is deterministic, then     </p>
<div class="arithmatex">\[
a_t \;=\;\pi(s_t).
\]</div>
<ol>
<li>Transition and Reward<br />
   After the agent executes action <span class="arithmatex">\(a_t\)</span> in state <span class="arithmatex">\(s_t\)</span>, the environment transitions to a new state <span class="arithmatex">\(s_{t+1}\)</span> and provides a (possibly stochastic) reward <span class="arithmatex">\(R_{t+1}\)</span>. Formally,  </li>
</ol>
<div class="arithmatex">\[
s_{t+1} \;\sim\; P(\,\cdot \,\vert\, s_t,\, a_t),
\]</div>
<div class="arithmatex">\[
R_{t+1} \;=\; R\bigl(s_t,\,a_t,\,s_{t+1}\bigr)
   \quad
   (\text{or an expected form }R(s,a)\text{, depending on the formulation}).
\]</div>
<ol>
<li>Trajectory (Episode)<br />
   By repeating this process over time, one obtains a trajectory (or sequence) of states, actions, and rewards:  </li>
</ol>
<div class="arithmatex">\[
\bigl(s_0,\,a_0,\,R_1,\,s_1,\,a_1,\,R_2,\,s_2,\,a_2,\,R_3,\dots\bigr).
\]</div>
<p>Such a sequence typically continues until a terminal condition (end of episode) is reached or indefinitely in an ongoing task.</p>
<ol>
<li>Return <span class="arithmatex">\(G_t\)</span><br />
   The return <span class="arithmatex">\(G_t\)</span> from time <span class="arithmatex">\(t\)</span> is the total discounted reward obtained by following policy <span class="arithmatex">\(\pi\)</span> from time <span class="arithmatex">\(t\)</span> onward:  </li>
</ol>
<div class="arithmatex">\[
G_t \;=\; R_{t+1} \;+\;\gamma\,R_{t+2} \;+\;\gamma^2\,R_{t+3} \;+\;\dots 
   \;=\; \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]</div>
<p>where <span class="arithmatex">\(0 \,\le\, \gamma \,\le\, 1\)</span> is the discount factor that balances immediate and future rewards.</p>
<ol>
<li>Conclusion<br />
   Hence, your understanding is correct: by following the policy <span class="arithmatex">\(\pi\)</span>, the agent traces out a trajectory $ (s_t, a_t, R_{t+1}, s_{t+1}, \dots) $ in the state space. From this trajectory, one can calculate the return <span class="arithmatex">\(G_t\)</span>. This process—collecting states, actions, and rewards—is at the heart of Reinforcement Learning, where the agent’s goal is to maximize the expected return.  </li>
</ol>
<h2 id="transition-probability-and-policy-probability">Transition probability and Policy Probability<a class="headerlink" href="#transition-probability-and-policy-probability" title="Permanent link">&para;</a></h2>
<p>In the MDP formulation, we can regard <span class="arithmatex">\(S_t \in \mathcal{S}\)</span> as a random variable. Similarly, <span class="arithmatex">\(A_t \in \mathcal{A}\)</span> at time <span class="arithmatex">\(t\)</span>. We have 2 probabilistic definitions:</p>
<p>1) State Transition:  <span class="arithmatex">\(\text{Pr}(S_{t+1} = s ^{\prime} \vert S_t = s, A_t = a)\)</span>. This gives the probability of a particular value of the state <span class="arithmatex">\(s^{\prime}\)</span> at time <span class="arithmatex">\(t+1\)</span> given specific values of the state and action at time <span class="arithmatex">\(t\)</span>. Therefore we have:</p>
<div class="arithmatex">\[
\sum_{s^{\prime} \in \mathcal{S}} \text{Pr}(S_{t+1} = s ^{\prime} \vert S_t = s, A_t = a) = 1
\]</div>
<p>2) Policy function: <span class="arithmatex">\(\pi(a \vert s) = \text{Pr}(A_t = a \vert S_t = s)\)</span>. Therefore we have:</p>
<div class="arithmatex">\[
\sum_{a \in \mathcal{A}}  \text{Pr}(A_t = a \vert S_t = s) =1
\]</div>
<p>Is this correct?</p>
<p>Below is a step-by-step confirmation and explanation:</p>
<p>1) State Transition Probability<br />
• In a Markov Decision Process (MDP), the transition function  </p>
<div class="arithmatex">\[
\Pr\bigl(S_{t+1} = s' \,\big\vert\, S_t = s,\;A_t = a\bigr)
\]</div>
<p>is a probability distribution over the next state <span class="arithmatex">\(s'\)</span> given the current state <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>.<br />
• Because it is a probability distribution over <span class="arithmatex">\(s'\in\mathcal{S}\)</span>, it must sum to 1:  </p>
<div class="arithmatex">\[
\sum_{s' \in \mathcal{S}} \Pr\bigl(S_{t+1} = s' \,\big\vert\, S_t = s,\;A_t = a\bigr) \;=\; 1.
\]</div>
<p>2) Policy Function<br />
• A policy <span class="arithmatex">\(\pi\)</span> for a stationary MDP is given by  </p>
<div class="arithmatex">\[
\pi(a \mid s) \;=\; \Pr\bigl(A_t = a \,\big\vert\, S_t = s\bigr).
\]</div>
<p>• This is a probability distribution over actions <span class="arithmatex">\(a\in\mathcal{A}\)</span>, for each given state <span class="arithmatex">\(s\)</span>. Hence, for each <span class="arithmatex">\(s\)</span>,  </p>
<div class="arithmatex">\[
\sum_{a \,\in\,\mathcal{A}} \pi(a \mid s) \;=\; 1.
\]</div>
<p>3) Conclusion<br />
Yes, both conditions you stated are correct:<br />
• The transition probabilities over all possible next states sum to 1.<br />
• The policy probabilities over all possible actions in a given state also sum to 1. </p>
<h2 id="state-value-function">State value function<a class="headerlink" href="#state-value-function" title="Permanent link">&para;</a></h2>
<p>We have shown before that for a trajectory that starts at state <span class="arithmatex">\(s_t\)</span> under the policy <span class="arithmatex">\(\pi\)</span>,  the return <span class="arithmatex">\(G_t\)</span> from time <span class="arithmatex">\(t\)</span> is the total discounted reward obtained by following policy <span class="arithmatex">\(\pi\)</span> from time <span class="arithmatex">\(t\)</span> onward:  </p>
<div class="arithmatex">\[
G_t \;=\; R_{t+1} \;+\;\gamma\,R_{t+2} \;+\;\gamma^2\,R_{t+3} \;+\;\dots 
   \;=\; \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]</div>
<p>where <span class="arithmatex">\(0 \,\le\, \gamma \,\le\, 1\)</span> is the discount factor. Since <span class="arithmatex">\(S_t, A_t\)</span> are random variables, so is <span class="arithmatex">\(R_t\)</span> and therefore <span class="arithmatex">\(G_t\)</span>. Another way to say it that there are many trajectories that emanate from<span class="arithmatex">\(s_t\)</span> under <span class="arithmatex">\(\pi\)</span> and we can define an expected value of <span class="arithmatex">\(G_t\)</span> to be the state value function <span class="arithmatex">\(v_{\pi}(s)\)</span> where <span class="arithmatex">\(S_t = s\)</span>.</p>
<div class="arithmatex">\[
v_{\pi} = \mathbb{E} (G_t \vert S_t =s )
\]</div>
<p>Does this definition of the state value make sense and if so, the expected value implies a conditional probability distribution. How is this known?</p>
<p><strong>Answer</strong> Below is a step-by-step explanation:</p>
<p>1) The Definition of the State-Value Function<br />
The state-value function under a policy <span class="arithmatex">\(\pi\)</span>, denoted <span class="arithmatex">\(v_{\pi}(s)\)</span>, is defined as  </p>
<div class="arithmatex">\[
v_{\pi}(s) \;=\; \mathbb{E}\bigl[G_t \,\big\vert\, S_t = s \bigr],
\]</div>
<p>where  </p>
<div class="arithmatex">\[
G_t \;=\;\sum_{k=0}^{\infty} \gamma^k\,R_{t+k+1}.
\]</div>
<p>In words, <span class="arithmatex">\(v_{\pi}(s)\)</span> is the expected return (sum of discounted future rewards) when starting from state <span class="arithmatex">\(s\)</span> at time <span class="arithmatex">\(t\)</span> and subsequently following policy <span class="arithmatex">\(\pi\)</span>.</p>
<p>2) Why Is <span class="arithmatex">\(G_t\)</span> Random?<br />
• The trajectory that unfolds from time <span class="arithmatex">\(t\)</span> onward depends on:<br />
  – The stochastic policy <span class="arithmatex">\(\pi(a \mid s)\)</span>, which determines how likely each action <span class="arithmatex">\(a\)</span> is in state <span class="arithmatex">\(s\)</span>.<br />
  – The transition probabilities <span class="arithmatex">\(P(s' \mid s,a)\)</span>, which govern how you move to the next state <span class="arithmatex">\(s'\)</span>.<br />
  – The potentially stochastic reward <span class="arithmatex">\(R_{t+1}\)</span> itself.<br />
• Therefore, for a fixed <span class="arithmatex">\(s\)</span>, there are many possible future trajectories (sequences of states, actions, and rewards). Each trajectory has a different realization of <span class="arithmatex">\(G_t\)</span>. Hence <span class="arithmatex">\(G_t\)</span> is a random variable.</p>
<p>3) Taking the Expectation (Conditional Probability Distribution)<br />
Because there are multiple possible ways the future might evolve from <span class="arithmatex">\(s\)</span>, the definition  </p>
<div class="arithmatex">\[
\mathbb{E}\bigl[G_t \,\big\vert\, S_t = s\bigr]
\]</div>
<p>reflects an expectation over all those possible trajectories. Formally, this expectation is taken with respect to the conditional probability distribution of all future states, actions, and rewards given that <span class="arithmatex">\(S_t = s\)</span> and that the agent follows policy <span class="arithmatex">\(\pi\)</span>. Symbolically, we can write  </p>
<div class="arithmatex">\[
\mathbb{E}\bigl[G_t \,\big\vert\, S_t = s\bigr]
\;=\;
\sum_{\text{all possible future trajectories}}
\Pr\bigl(\text{trajectory}\,\big\vert\,S_t=s,\;\pi\bigr)
\;\times\;
G_t(\text{trajectory}),
\]</div>
<p>where <span class="arithmatex">\(\Pr(\text{trajectory}\,\vert\,S_t=s,\;\pi)\)</span> is constructed from the chain of probabilities defined by <span class="arithmatex">\(\pi(a\mid s)\)</span> and <span class="arithmatex">\(P(s'\mid s,a)\)</span>.</p>
<p>4) Where the Conditional Distribution Comes From<br />
• The agent’s policy <span class="arithmatex">\(\pi(a\mid s)\)</span> gives a distribution over actions <span class="arithmatex">\(a\)</span> for every state <span class="arithmatex">\(s\)</span>.<br />
• The environment’s transition function <span class="arithmatex">\(P(s'\mid s,a)\)</span> gives a distribution over next states <span class="arithmatex">\(s'\)</span> for each pair <span class="arithmatex">\((s,a)\)</span>.<br />
• Combining these, one obtains the probability tree of all future trajectories (including rewards).<br />
• In RL notation, we simply write <span class="arithmatex">\(\mathbb{E}[\,G_t\mid S_t=s]\)</span> as a shorthand. Under the hood, it involves the full conditional distribution derived from <span class="arithmatex">\(\pi\)</span> and <span class="arithmatex">\(P\)</span>.</p>
<p>5) Conclusion<br />
Yes, this definition of the state-value function  </p>
<div class="arithmatex">\[
v_{\pi}(s) \;=\; \mathbb{E}\bigl[G_t \,\big\vert\, S_t=s\bigr]
\]</div>
<p>makes sense: we take an expectation of the random return <span class="arithmatex">\(G_t\)</span> over all possible ways the future can evolve from state <span class="arithmatex">\(s\)</span>, given that the agent acts according to <span class="arithmatex">\(\pi\)</span>. That expectation is indeed a conditional probability distribution (conditioned on <span class="arithmatex">\(S_t = s\)</span>), implicitly governed by <span class="arithmatex">\(\pi\)</span> and the transition probabilities of the MDP.</p>
<h2 id="probability-tree-for-v_pis">Probability tree for <span class="arithmatex">\(v_{\pi}(s)\)</span><a class="headerlink" href="#probability-tree-for-v_pis" title="Permanent link">&para;</a></h2>
<p>Symbolically, <span class="arithmatex">\(v_{\pi}(s)\)</span> is given by</p>
<div class="arithmatex">\[
\mathbb{E}\bigl[G_t \,\big\vert\, S_t = s\bigr]
\;=\;
\sum_{\text{all possible future trajectories}}
\Pr\bigl(\text{trajectory}\,\big\vert\,S_t=s,\;\pi\bigr)
\;\times\;
G_t(\text{trajectory}),
\]</div>
<p>to illustrate how this conditional probability distribution is derived from <span class="arithmatex">\(\pi\)</span> and <span class="arithmatex">\(P\)</span>, use the time instants <span class="arithmatex">\(t, t+1, t+2\)</span> and work out the probability tree.</p>
<p><strong>Answer</strong> Below is a step-by-step illustration of how the probability of a particular future trajectory from time <span class="arithmatex">\(t\)</span> through <span class="arithmatex">\(t+2\)</span> (and onward) is derived from the policy <span class="arithmatex">\(\pi\)</span> and the transition probabilities <span class="arithmatex">\(P\)</span>. We will explicitly write out the probability of the first couple of steps in the “probability tree” before it continues further.</p>
<hr />
<p>1 Starting at Time t, State s</p>
<hr />
<p>We begin with:</p>
<div class="arithmatex">\[
S_t = s.
\]</div>
<p>Because we are conditioning on <span class="arithmatex">\(S_t = s\)</span>, the probability of being at <span class="arithmatex">\(s\)</span> at time <span class="arithmatex">\(t\)</span> is defined to be <span class="arithmatex">\(1\)</span> in that conditional scenario.</p>
<hr />
<p>2 Choosing Action Aₜ\</p>
<hr />
<p>Under policy <span class="arithmatex">\(\pi\)</span>, the agent chooses action <span class="arithmatex">\(A_t = a \in \mathcal{A}\)</span> with probability</p>
<div class="arithmatex">\[
\pi(a \,\vert\, s).
\]</div>
<p>Hence,</p>
<p>• Probability of picking action <span class="arithmatex">\(a\)</span> at time <span class="arithmatex">\(t\)</span>:</p>
<div class="arithmatex">\[
\Pr\bigl(A_t = a \,\big\vert\, S_t = s,\;\pi\bigr)
\;=\;
\pi(a \,\vert\, s).
\]</div>
<hr />
<p>3 Transition to Time t+1</p>
<hr />
<p>After choosing <span class="arithmatex">\(A_t = a\)</span>, the environment transitions to a new state <span class="arithmatex">\(S_{t+1} = s' \in \mathcal{S}\)</span> with probability</p>
<div class="arithmatex">\[
P(s' \,\vert\, s,\, a).
\]</div>
<p>Combined with the previous step, the joint probability of picking action <span class="arithmatex">\(a\)</span> and ending up in state <span class="arithmatex">\(s'\)</span> is:</p>
<div class="arithmatex">\[
\Pr\bigl(A_t = a,\,S_{t+1} = s' \,\big\vert\, S_t = s,\,\pi\bigr)
\;=\;
\pi(a \,\vert\, s)\;\times\;P(s' \,\vert\, s,\,a).
\]</div>
<hr />
<p>4 Choosing Action A₍t+1₎ in State s'</p>
<hr />
<p>Once in state <span class="arithmatex">\(s'\)</span> at time <span class="arithmatex">\(t+1\)</span>, the agent chooses action <span class="arithmatex">\(A_{t+1} = a'\)</span> with probability</p>
<div class="arithmatex">\[
\pi(a' \,\vert\, s').
\]</div>
<p>Hence,</p>
<p>• Probability of picking action <span class="arithmatex">\(a'\)</span> in state <span class="arithmatex">\(s'\)</span> at time <span class="arithmatex">\(t+1\)</span>:</p>
<div class="arithmatex">\[
\Pr\bigl(A_{t+1} = a' \,\big\vert\, S_{t+1} = s',\,\pi\bigr)
\;=\;
\pi(a' \,\vert\, s').
\]</div>
<hr />
<p>5 Transition to Time t+2</p>
<hr />
<p>After action <span class="arithmatex">\(A_{t+1} = a'\)</span>, the environment transitions to <span class="arithmatex">\(S_{t+2} = s''\)</span> with probability</p>
<div class="arithmatex">\[
P(s'' \,\vert\, s',\, a').
\]</div>
<p>Putting it together up to time <span class="arithmatex">\(t+2\)</span>:</p>
<div class="arithmatex">\[
\Pr\Bigl(A_t = a,\;S_{t+1} = s',\;A_{t+1} = a',\;S_{t+2} = s'' \,\Big\vert\, S_t = s,\;\pi\Bigr)
\;=\;
\pi(a \,\vert\, s)
\;\times\;
P\bigl(s' \,\vert\, s,\,a\bigr)
\;\times\;
\pi(a' \,\vert\, s')
\;\times\;
P\bigl(s'' \,\vert\, s',\,a'\bigr).
\]</div>
<hr />
<p>6 Extending Beyond t+2</p>
<hr />
<p>By continuing this process for <span class="arithmatex">\(t+3, t+4, \dots\)</span>, one builds the full probability tree of subsequent states and actions:</p>
<p>• At each step, the agent’s policy <span class="arithmatex">\(\pi(\cdot \,\vert\, s_j)\)</span> supplies the probabilities of each possible action given the current state <span class="arithmatex">\(s_j\)</span>.<br />
• The environment’s transition dynamics <span class="arithmatex">\(P(s_{j+1} \,\vert\, s_j,\, a_j)\)</span> supply the probabilities of next states.  </p>
<p>Therefore, any full future trajectory (from time <span class="arithmatex">\(t\)</span> to the end of the episode or indefinitely in an ongoing task) has a probability obtained by multiplying together all these policy and transition probabilities.</p>
<hr />
<p>7 Putting It All Into the Expectation for v₍π₎(s)</p>
<hr />
<p>When we write</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\mathbb{E}\bigl[\,G_t \;\big\vert\; S_t = s\bigr]
\;=\;
\sum_{\substack{\text{all possible future} \\ \text{trajectories}}}
\Pr\bigl(\text{trajectory}\,\big\vert\,S_t = s,\;\pi\bigr)\,\times\,G_t(\text{trajectory}),
\]</div>
<p>we are summing (or integrating) over all possible sequences of the form</p>
<div class="arithmatex">\[
\bigl(A_t,\,S_{t+1},\,A_{t+1},\,S_{t+2},\,A_{t+2},\,S_{t+3},\dots\bigr)
\]</div>
<p>and multiplying each trajectory’s return <span class="arithmatex">\(G_t(\text{trajectory})\)</span> by the probability of that trajectory, which is computed via the products of <span class="arithmatex">\(\pi(\cdot)\)</span> and <span class="arithmatex">\(P(\cdot)\)</span> factors illustrated above.</p>
<hr />
<p>8 Conclusion</p>
<hr />
<p>This small probability tree up to <span class="arithmatex">\(t+2\)</span> demonstrates how each branch’s probability is the product of:<br />
1) policy probabilities <span class="arithmatex">\(\pi(a \,\vert\, s)\)</span>, and<br />
2) transition probabilities <span class="arithmatex">\(P(s' \,\vert\, s,a)\)</span>.  </p>
<p>Continuing the same pattern through subsequent time steps yields the full conditional probability distribution of all possible future trajectories, thereby defining<br />
<span class="arithmatex">\(\Pr(\text{trajectory}\,\vert\,S_t=s,\pi)\)</span>.  </p>
<p>Thus, the expectation in<br />
<span class="arithmatex">\(\mathbb{E}[\,G_t \,\vert\,S_t=s]\)</span><br />
is taken over this entire (potentially infinite) probability tree.</p>
<h2 id="bellman-equation">Bellman Equation<a class="headerlink" href="#bellman-equation" title="Permanent link">&para;</a></h2>
<p>Given a policy <span class="arithmatex">\(\pi\)</span>, what is the Bellman equation and how is it used to calculate <span class="arithmatex">\(v_{\pi}(s)\)</span> for all <span class="arithmatex">\(s \in \mathcal{S}\)</span>?
How is it derived?</p>
<p><strong>Answer</strong> Below is a step-by-step explanation of the Bellman equation for a given policy <span class="arithmatex">\(\pi\)</span>, how it is derived, and how it is used to calculate the value function <span class="arithmatex">\(v_{\pi}(s)\)</span> for all states <span class="arithmatex">\(s\in\mathcal{S}\)</span>.</p>
<p>───────────────────────────────
1) Definition of the State-Value Function</p>
<p>─────────────────────────</p>
<p>Recall that under a policy <span class="arithmatex">\(\pi\)</span>, the state-value function <span class="arithmatex">\(v_{\pi}(s)\)</span> is  </p>
<div class="arithmatex">\[
v_{\pi}(s) 
\;=\;
\mathbb{E}\bigl[G_t \;\big\vert\; S_t = s\bigr]
\;=\;
\mathbb{E}\Bigl[\,
\sum_{k=0}^{\infty}
\gamma^k\,R_{t+k+1}
\;\Big\vert\;
S_t = s
\Bigr].
\]</div>
<p>Here:
• <span class="arithmatex">\(G_t\)</span> is the (random) return from time <span class="arithmatex">\(t\)</span> onward.<br />
• <span class="arithmatex">\(R_{t+k+1}\)</span> is the reward at time <span class="arithmatex">\(t+k+1\)</span>.<br />
• <span class="arithmatex">\(\gamma \in [0,\,1]\)</span> is the discount factor.<br />
• The expectation <span class="arithmatex">\(\mathbb{E}[\cdot]\)</span> is taken over all possible future trajectories following policy <span class="arithmatex">\(\pi\)</span>.  </p>
<p>────────────────────────────</p>
<p>2 One-Step Lookahead </p>
<p>────────────────────────────</p>
<p>Consider the very next step from state <span class="arithmatex">\(s\)</span> under policy <span class="arithmatex">\(\pi\)</span>. The agent will choose an action <span class="arithmatex">\(A_t = a\)</span> with probability <span class="arithmatex">\(\pi(a \,\vert\, s)\)</span>. The environment will then transition to a next state <span class="arithmatex">\(S_{t+1} = s'\)</span> with probability <span class="arithmatex">\(P(s' \,\vert\, s,\,a)\)</span>, and the agent will receive a reward <span class="arithmatex">\(R_{t+1} = R(s,a,s')\)</span> (or its expected form <span class="arithmatex">\(R(s,a)\)</span> depending on the problem).  </p>
<p>After that step, the agent continues from state <span class="arithmatex">\(s'\)</span>, and the expected value of the return from <span class="arithmatex">\(s'\)</span> onward is <span class="arithmatex">\(v_{\pi}(s')\)</span>.  </p>
<p>Thus the return <span class="arithmatex">\(G_t\)</span>, when starting in state <span class="arithmatex">\(s\)</span> and taking action <span class="arithmatex">\(a\)</span> that leads to state <span class="arithmatex">\(s'\)</span>, can be decomposed as:  </p>
<div class="arithmatex">\[
G_t
\;=\;
R_{t+1} 
\;+\;
\gamma\,G_{t+1}.
\]</div>
<p>But the expected value of <span class="arithmatex">\(G_{t+1}\)</span>, given <span class="arithmatex">\(S_{t+1} = s'\)</span> and following <span class="arithmatex">\(\pi\)</span> thereafter, is <span class="arithmatex">\(v_{\pi}(s')\)</span>.</p>
<p>─────────────────────────────</p>
<p>3 The Bellman Equation for v₍π₎(s)</p>
<p>────────────────────────────</p>
<p>Putting it all together, we take the expectation with respect to actions (as given by <span class="arithmatex">\(\pi\)</span>) and next states (as given by <span class="arithmatex">\(P\)</span>). We get the Bellman expectation equation:</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \in \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \in \mathcal{S}}
P(s' \,\vert\, s,\, a)
\bigl[
R(s,a,s')
\;+\;
\gamma\,v_{\pi}(s')
\bigr].
\]</div>
<p>Sometimes you will see it written in a shorter form (if the immediate reward does not explicitly depend on <span class="arithmatex">\(s'\)</span>):</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)
\bigl[
R(s,a)
\;+\;
\gamma\,v_{\pi}(s')
\bigr].
\]</div>
<p>This Bellman equation says:
1. From state <span class="arithmatex">\(s\)</span>, choose an action <span class="arithmatex">\(a\)</span> according to <span class="arithmatex">\(\pi\)</span>.<br />
2. Possibly transition to state <span class="arithmatex">\(s'\)</span> with probability <span class="arithmatex">\(P(s'\mid s,a)\)</span>.<br />
3. Accumulate the immediate reward <span class="arithmatex">\(R(s,a,s')\)</span>.<br />
4. Then continue and accumulate discounted future rewards, whose expected value is <span class="arithmatex">\(\gamma\,v_{\pi}(s')\)</span>.</p>
<p>───────────────────────</p>
<p>4 Using the Bellman Equation to Compute v₍π₎(s)</p>
<p>───────────────────────────</p>
<p>• The set of equations </p>
<div class="arithmatex">\[
v_{\pi}(s)
  \;=\;
  \sum_{a} \pi(a \,\vert\, s)
  \sum_{s'} P(s' \,\vert\, s,a)\,\bigl[R(s,a,s') + \gamma\,v_{\pi}(s')\bigr]
  \quad
  \text{for all } s\in\mathcal{S}
\]</div>
<p>can be viewed as a linear system (when <span class="arithmatex">\(\gamma &lt; 1\)</span>) if <span class="arithmatex">\(\mathcal{S}\)</span> is finite.<br />
• In principle, one can solve these equations exactly for <span class="arithmatex">\(v_{\pi}\)</span> by standard linear algebra if the state space <span class="arithmatex">\(\mathcal{S}\)</span> is not too large.  </p>
<p>More commonly, in large or unknown environments, we use iterative methods—such as iterative policy evaluation—to approximate or converge to the true <span class="arithmatex">\(v_{\pi}(s)\)</span>:</p>
<ol>
<li>Initialize <span class="arithmatex">\(v^{(0)}(s)\)</span> arbitrarily (e.g., zeros).  </li>
<li>For each iteration <span class="arithmatex">\(n=0,1,2,\dots\)</span>, update</li>
</ol>
<div class="arithmatex">\[
v^{(n+1)}(s)
   \;\leftarrow\;
   \sum_{a} \pi(a \,\vert\, s)
   \sum_{s'} 
   P(s' \,\vert\, s,a)\,
   \bigl[R(s,a,s') + \gamma\,v^{(n)}(s')\bigr].
\]</div>
<ol>
<li>Repeat until convergence.</li>
</ol>
<p>────────────────</p>
<p>5 Why It Is Called a “Consistency” Equation</p>
<p>──────────</p>
<p>The Bellman equation essentially enforces consistency: the value of a state <span class="arithmatex">\(s\)</span> must match the expected immediate reward plus the discounted value of the successor state under the given policy. It is a self-consistent recursive definition of <span class="arithmatex">\(v_{\pi}\)</span>.</p>
<p>─────────────────────────</p>
<p>6 Derivation Summary</p>
<p>───────────────────</p>
<p>In summary, the Bellman equation is derived by:<br />
1. Defining <span class="arithmatex">\(v_{\pi}(s) = \mathbb{E}[G_t \mid S_t=s]\)</span>.<br />
2. Decomposing the return <span class="arithmatex">\(G_t\)</span> into immediate reward plus discounted future return, <span class="arithmatex">\(R_{t+1} + \gamma\,G_{t+1}\)</span>.<br />
3. Taking expectations over the random action-choice and state-transition processes dictated by <span class="arithmatex">\(\pi\)</span> and <span class="arithmatex">\(P\)</span>.  </p>
<p>Hence, we arrive at the Bellman expectation equation, which provides a system of equations for <span class="arithmatex">\(v_{\pi}(s)\)</span>.</p>
<p>─────────</p>
<p>7 Conclusion</p>
<p>──────</p>
<p>• The Bellman equation for a policy <span class="arithmatex">\(\pi\)</span> is</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,\,a)\,
\bigl[
R(s,a,s')
\;+\;
\gamma\,v_{\pi}(s')
\bigr].
\]</div>
<p>• It is used to calculate <span class="arithmatex">\(v_{\pi}(s)\)</span> for all <span class="arithmatex">\(s\)</span> either by directly solving this system (in the finite state case) or by applying iterative policy evaluation methods.  </p>
<p>This completes the derivation and usage of the Bellman equation in the context of evaluating a given policy <span class="arithmatex">\(\pi\)</span>.</p>
<h2 id="bellman-equation-details">Bellman Equation - Details<a class="headerlink" href="#bellman-equation-details" title="Permanent link">&para;</a></h2>
<p>Back to the Bellman equation under a given policy <span class="arithmatex">\(\pi\)</span>. The state value function is given by</p>
<div class="arithmatex">\[ 
v_{\pi}(s) \;=\; \mathbb{E}\bigl[G_t \;\big\vert\; S_t = s\bigr] \;=\; \mathbb{E}\Bigl[\,\sum_{k=0}^{\infty}
\gamma^k\,R_{t+k+1}
\;\Big\vert\; S_t = s \Bigr].
\]</div>
<p>Using a one step look ahead, we have</p>
<div class="arithmatex">\[
G_t
\;=\;
R_{t+1} 
\;+\;
\gamma\,G_{t+1}.
\]</div>
<p>where <span class="arithmatex">\(R_{t+1}\)</span> is the immediate reward and the <span class="arithmatex">\(\gamma \, G_{t+1}\)</span> is future discounted reward. Taking the expected value on both sides,</p>
<div class="arithmatex">\[
\mathbb{E} \bigl[G_t \;\big\vert\; S_t = s \bigr] = \mathbb{E} \bigl[R_{t+1} \;\big\vert\; S_t = s \bigr] + \mathbb{E} \bigl[ \gamma G_{t+1} \;\big\vert\; S_t = s \bigr]
\]</div>
<p>Complete the derivation of the this equation leading to the Bellman equation for <span class="arithmatex">\(v_{\pi}\)</span>.</p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step derivation, starting from the one-step lookahead identity and arriving at the Bellman equation for the state-value function under a given policy <span class="arithmatex">\(\pi\)</span>. All mathematical expressions are shown in LaTeX.</p>
<p>1 <strong>Recall the One-Step Decomposition</strong></p>
<p>We have:</p>
<div class="arithmatex">\[
G_t 
\;=\; 
R_{t+1} \;+\;\gamma\,G_{t+1}.
\]</div>
<p>Taking the expectation of both sides, conditioned on <span class="arithmatex">\(S_t = s\)</span> and following policy <span class="arithmatex">\(\pi\)</span>, gives:</p>
<div class="arithmatex">\[
\mathbb{E}[\,G_t \mid S_t = s,\,\pi]
\;=\;
\mathbb{E}[\,R_{t+1} \mid S_t = s,\,\pi]
\;+\;
\gamma\,\mathbb{E}[\,G_{t+1} \mid S_t = s,\,\pi].
\]</div>
<p>Because<br />
<span class="arithmatex">\(\mathbb{E}[\,G_t \mid S_t = s,\,\pi] = v_{\pi}(s)\)</span>,<br />
we can rewrite this as </p>
<div class="arithmatex">\[
v_{\pi}(s) 
\;=\;
\mathbb{E}[\,R_{t+1} \mid S_t = s,\,\pi]
\;+\;
\gamma\,\mathbb{E}[\,G_{t+1} \mid S_t = s,\,\pi].
\]</div>
<p>2 <strong>Break Down the Expectations</strong></p>
<p>a) The term <span class="arithmatex">\(\mathbb{E}[\,R_{t+1} \mid S_t = s,\,\pi]\)</span></p>
<p>Under policy <span class="arithmatex">\(\pi\)</span>, the agent chooses action <span class="arithmatex">\(A_t = a\)</span> in state <span class="arithmatex">\(s\)</span> with probability <span class="arithmatex">\(\pi(a \mid s)\)</span>. Then, the environment transitions to a next state <span class="arithmatex">\(s' \in \mathcal{S}\)</span> with probability <span class="arithmatex">\(P(s' \mid s,a)\)</span>, supplying a reward <span class="arithmatex">\(R(s,a,s')\)</span>. Therefore,</p>
<div class="arithmatex">\[
\mathbb{E}[R_{t+1} \mid S_t = s,\,\pi]
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \mid s,a)\;R(s,a,s').
\]</div>
<p>b) The term <span class="arithmatex">\(\mathbb{E}[\,G_{t+1} \mid S_t = s,\,\pi]\)</span></p>
<p>Once the agent has taken action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> and arrived in <span class="arithmatex">\(s'\)</span>, it continues following policy <span class="arithmatex">\(\pi\)</span> from time <span class="arithmatex">\(t+1\)</span>. Hence, the expected future return from <span class="arithmatex">\(s'\)</span> onward is just <span class="arithmatex">\(v_{\pi}(s')\)</span>. Again summing over all possible next states <span class="arithmatex">\(s'\)</span> and actions <span class="arithmatex">\(a\)</span>:</p>
<div class="arithmatex">\[
\mathbb{E}[\,G_{t+1} \mid S_t = s,\,\pi]
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \mid s,a)\;\underbrace{
  \mathbb{E}\bigl[G_{t+1} \mid S_{t+1}=s',\,\pi\bigr]
}_{v_{\pi}(s')}.
\]</div>
<p>Thus:</p>
<div class="arithmatex">\[
\mathbb{E}[\,G_{t+1} \mid S_t = s,\,\pi]
\;=\;
\sum_{a}
\pi(a \,\vert\, s)
\sum_{s' }
P(s' \mid s,a)\;v_{\pi}(s').
\]</div>
<p>3 <strong>Combine the Two Components</strong></p>
<p>Putting them all together:</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\underbrace{
  \sum_{a}
  \pi(a \mid s)
  \sum_{s'}
  P(s' \mid s,a)\,R(s,a,s')
}_{\mathbb{E}[R_{t+1}\,\vert\,S_t=s,\;\pi]}
\;+\;
\gamma
\underbrace{
  \sum_{a}
  \pi(a \mid s)
  \sum_{s'}
  P(s' \mid s,a)\,v_{\pi}(s')
}_{\mathbb{E}[G_{t+1}\,\vert\,S_t=s,\;\pi]}.
\]</div>
<p>Factor out the sums more compactly:</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)\,
\Bigl[
  R(s,a,s') 
  \;+\;
  \gamma\,v_{\pi}(s')
\Bigr].
\]</div>
<p>This equation is referred to as the Bellman Expectation Equation (or simply the Bellman equation) for the state-value function <span class="arithmatex">\(v_{\pi}\)</span>.</p>
<p>4 <strong>Final Bellman Equation for <span class="arithmatex">\(v_{\pi}\)</span></strong></p>
<p>Hence, the derivation shows that:</p>
<div class="arithmatex">\[
\boxed{
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)\,
\bigl[
  R(s,a,s') \;+\; \gamma\,v_{\pi}(s')
\bigr].
}
\]</div>
<p>From the one-step lookahead identity 
<span class="arithmatex">\(
G_t = R_{t+1} + \gamma G_{t+1}
\)</span>
and taking conditional expectations w.r.t. <span class="arithmatex">\((S_t = s,\pi)\)</span>, we arrive at this recursive relation for <span class="arithmatex">\(v_{\pi}\)</span>. This is precisely the Bellman equation under a given policy <span class="arithmatex">\(\pi\)</span>.</p>
<h2 id="bellman-equation-to-system-of-linear-equations">Bellman Equation <span class="arithmatex">\(\to\)</span> System of Linear Equations<a class="headerlink" href="#bellman-equation-to-system-of-linear-equations" title="Permanent link">&para;</a></h2>
<p>It seems that for the finite state space <span class="arithmatex">\(\mathcal{S}\)</span>, the Bellman equation results in a system of linear equations in terms of the "variables" <span class="arithmatex">\(v_{\pi}(s_t)\)</span>. Is this correct?</p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step explanation showing that for a finite state space <span class="arithmatex">\(\mathcal{S}\)</span>, the Bellman equation corresponds to a system of linear equations in the unknowns <span class="arithmatex">\(v_{\pi}(s)\)</span>:</p>
<p>1) The Bellman Equation for a Fixed Policy<br />
   Recall the Bellman expectation equation for a policy <span class="arithmatex">\(\pi\)</span> and a finite MDP:  </p>
<div class="arithmatex">\[
v_{\pi}(s)
   \;=\;
   \sum_{a \,\in\, \mathcal{A}}
   \pi(a \,\vert\, s)
   \sum_{s' \,\in\, \mathcal{S}}
   P(s' \,\vert\, s,\,a)
   \,\Bigl[
     R(s,a,s') \;+\; \gamma\,v_{\pi}(s')
   \Bigr].
\]</div>
<p>Here,<br />
   •  <span class="arithmatex">\(\mathcal{S} = \{s_1, s_2, \dots, s_{|\mathcal{S}|}\}\)</span> is a finite set of states.<br />
   •  <span class="arithmatex">\(\mathcal{A}\)</span> is the finite set of possible actions.<br />
   •  <span class="arithmatex">\(\pi(a\mid s)\)</span> is the probability of action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>.<br />
   •  <span class="arithmatex">\(P(s' \mid s,a)\)</span> is the probability of transitioning to <span class="arithmatex">\(s'\)</span> from <span class="arithmatex">\(s\)</span> after action <span class="arithmatex">\(a\)</span>.<br />
   •  <span class="arithmatex">\(R(s,a,s')\)</span> is the immediate reward.</p>
<p>2) Expressing the Equation for Each State<br />
   For each <span class="arithmatex">\(s \in \mathcal{S}\)</span>, we can write a separate instance of the Bellman equation. Concretely, labeling the states <span class="arithmatex">\(s_1, s_2, \dots, s_{|\mathcal{S}|}\)</span>, we get <span class="arithmatex">\(|\mathcal{S}|\)</span> equations of the form:  </p>
<div class="arithmatex">\[
v_{\pi}(s_i)
   \;=\;
   \sum_{a \,\in\, \mathcal{A}}
   \pi(a \,\vert\, s_i)
   \sum_{s_j \,\in\, \mathcal{S}}
   P(s_j \,\vert\, s_i,\,a)\,
   \Bigl[
     R(s_i,a,s_j) \;+\; \gamma\,v_{\pi}(s_j)
   \Bigr]
   \quad\text{for}\quad i=1,2,\dots,|\mathcal{S}|.
\]</div>
<p>3) Rearranging into Linear System Form<br />
   Notice that <span class="arithmatex">\(v_{\pi}(s_i)\)</span> appears on the left-hand side, and <span class="arithmatex">\(v_{\pi}(s_j)\)</span> (with potentially different <span class="arithmatex">\(j\)</span>) appears inside the sum on the right-hand side. What we have is a set of linear equations in the unknowns <span class="arithmatex">\(v_{\pi}(s_1),\, v_{\pi}(s_2), \dots, v_{\pi}(s_{|\mathcal{S}|})\)</span>, because each equation can be rearranged to something like:  </p>
<div class="arithmatex">\[
v_{\pi}(s_i)
   \;-\;
   \gamma
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times v_{\pi}(s_j)
   \Bigr]
   \;=\;
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times R(s_i,a,s_j)
   \Bigr].
\]</div>
<p>All terms involving <span class="arithmatex">\(v_{\pi}(s_i)\)</span> or <span class="arithmatex">\(v_{\pi}(s_j)\)</span> are multiplied by constant coefficients (which come from <span class="arithmatex">\(\pi(a\mid s_i)\)</span>, <span class="arithmatex">\(P(s_j\mid s_i,a)\)</span>, and <span class="arithmatex">\(\gamma\)</span>).</p>
<p>4) Matrix Notation<br />
   In matrix form, one can write:  </p>
<div class="arithmatex">\[
\mathbf{v}_{\pi}
   \;=\;
   \mathbf{r}_{\pi} 
   \;+\;
   \gamma \,\mathbf{P}_{\pi}\,\mathbf{v}_{\pi},
\]</div>
<p>where:<br />
   • <span class="arithmatex">\(\mathbf{v}_{\pi}\)</span> is a column vector <span class="arithmatex">\(\bigl(v_{\pi}(s_1), v_{\pi}(s_2), \ldots, v_{\pi}(s_{|\mathcal{S}|})\bigr)^\top\)</span>.<br />
   • <span class="arithmatex">\(\mathbf{r}_{\pi}\)</span> is the vector of expected immediate rewards in each state (depending on <span class="arithmatex">\(\pi\)</span>).<br />
   • <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span> is the state transition probability matrix when actions are chosen according to <span class="arithmatex">\(\pi\)</span>.  </p>
<p>Rearranging:  </p>
<div class="arithmatex">\[
\mathbf{v}_{\pi} 
   \;-\;
   \gamma\,\mathbf{P}_{\pi}\,\mathbf{v}_{\pi}
   \;=\;
   \mathbf{r}_{\pi},
\]</div>
<p>or  </p>
<div class="arithmatex">\[
(\mathbf{I} - \gamma\,\mathbf{P}_{\pi})\,\mathbf{v}_{\pi}
   \;=\;
   \mathbf{r}_{\pi}.
\]</div>
<p>This is a linear system of dimension <span class="arithmatex">\(|\mathcal{S}|\times|\mathcal{S}|\)</span>.</p>
<p>5) Conclusion<br />
   Therefore, for each state in a finite MDP, the Bellman equation contributes one linear equation in the unknown <span class="arithmatex">\(v_{\pi}(s)\)</span>. Putting them all together yields a system of <span class="arithmatex">\(|\mathcal{S}|\)</span> linear equations. Solving this system (when <span class="arithmatex">\(\gamma&lt;1\)</span> and <span class="arithmatex">\(\mathbf{I} - \gamma\,\mathbf{P}_{\pi}\)</span> is invertible) yields the value function <span class="arithmatex">\(v_{\pi}(s)\)</span> for all states simultaneously.  </p>
<p>Indeed, your observation is correct: in a finite state space, the Bellman equation is precisely a system of linear equations in the variables <span class="arithmatex">\(\{\,v_{\pi}(s): s\in \mathcal{S}\}\)</span>.</p>
<h2 id="bellman-equation-matrix-vector-form">Bellman Equation matrix-vector form<a class="headerlink" href="#bellman-equation-matrix-vector-form" title="Permanent link">&para;</a></h2>
<p>For each state <span class="arithmatex">\(s \in \mathcal{S}\)</span>, and concretely labeling the states as <span class="arithmatex">\(s_1, s_2, \ldots, s_{|\mathcal{s}|}\)</span>, we can write a Bellman equation,</p>
<div class="arithmatex">\[
v_{\pi}(s_i)
   \;-\;
   \gamma
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times v_{\pi}(s_j)
   \Bigr]
   \;=\;
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times R(s_i,a,s_j)
   \Bigr]
\]</div>
<p>which can be written in the form:</p>
<div class="arithmatex">\[
v_{\pi}(s_i)
   \;-\;
   \gamma
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times v_{\pi}(s_j)
   \Bigr]
   \;=\;
   \sum_{a,\,s_j}
   \Bigl[
   \text{(coefficient)} \times R(s_i,a,s_j)
   \Bigr].
\]</div>
<p>which then leads to the matrix-vector form:</p>
<div class="arithmatex">\[
\mathbf{v}_{\pi}
   \;=\;
   \mathbf{r}_{\pi} 
   \;+\;
   \gamma \,\mathbf{P}_{\pi}\,\mathbf{v}_{\pi}
\]</div>
<p>Write out the term <span class="arithmatex">\(\text{(coefficient)}\)</span> that appears above, fully. The entries of the matrix <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span> must be related to <span class="arithmatex">\(\text{(coefficient)}\)</span>. Clarify this.</p>
<p><strong><em>Answer</em></strong></p>
<p>Below is a step-by-step explanation of the coefficient that appears in the Bellman equation for a policy <span class="arithmatex">\(\pi\)</span>, written in full LaTeX, and how it relates to the entries of the matrix <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span> in the matrix-vector equation:</p>
<p>1 <strong>Single-State Bellman Equation</strong></p>
<p>For a fixed state <span class="arithmatex">\(s_{i}\)</span> in a finite state space <span class="arithmatex">\(\mathcal{S} = \{s_{1},\ldots,s_{|\mathcal{S}|}\}\)</span>, the Bellman equation under a policy <span class="arithmatex">\(\pi\)</span> can be written as:</p>
<div class="arithmatex">\[
v_{\pi}(s_{i})
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi\bigl(a \,\vert\, s_{i}\bigr)
\sum_{s_{j} \,\in\, \mathcal{S}}
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr)
\Bigl[
  R\bigl(s_{i},\,a,\,s_{j}\bigr)
  \;+\;
  \gamma \, v_{\pi}(s_{j})
\Bigr].
\]</div>
<p>Rearranging terms to isolate <span class="arithmatex">\(v_{\pi}(s_{i})\)</span> on the left and the discounted values on the right gives a form like:</p>
<div class="arithmatex">\[
v_{\pi}(s_{i})
\;-\;
\gamma
\sum_{a\,\in\,\mathcal{A}}
\sum_{s_{j}\,\in\,\mathcal{S}}
\Bigl[\text{(coefficient)}\times v_{\pi}(s_{j})\Bigr]
\;=\;
\sum_{a\,\in\,\mathcal{A}}
\sum_{s_{j}\,\in\,\mathcal{S}}
\Bigl[\text{(coefficient)}\times R\bigl(s_{i},\,a,\,s_{j}\bigr)\Bigr].
\]</div>
<p>2 <strong>Identifying the Coefficient</strong></p>
<p>Inside the double sum over <span class="arithmatex">\(a \in \mathcal{A}\)</span> and <span class="arithmatex">\(s_{j} \in \mathcal{S}\)</span>, the coefficient that multiplies <span class="arithmatex">\(v_{\pi}(s_{j})\)</span> (before factoring out <span class="arithmatex">\(\gamma\)</span>) is</p>
<div class="arithmatex">\[
\pi\bigl(a \,\vert\, s_{i}\bigr)
\;\times\;
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>Thus, if we look at the term on the left that is multiplied by <span class="arithmatex">\(\gamma\)</span>, the <span class="arithmatex">\(\text{(coefficient)}\)</span> is explicitly:</p>
<div class="arithmatex">\[
\gamma
\;\times\;
\pi\bigl(a \,\vert\, s_{i}\bigr)
\;\times\;
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>Similarly, the coefficient that appears with the reward <span class="arithmatex">\(R(s_{i},\,a,\,s_{j})\)</span> (on the right-hand side) is</p>
<div class="arithmatex">\[
\pi\bigl(a \,\vert\, s_{i}\bigr)
\;\times\;
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>3 <strong>Matrix-Vector Equation</strong></p>
<p>Collecting all <span class="arithmatex">\(|\mathcal{S}|\)</span> such Bellman equations for <span class="arithmatex">\(s_{1}, \dots, s_{|\mathcal{S}|}\)</span> yields the matrix-vector form:</p>
<div class="arithmatex">\[
\mathbf{v}_{\pi}
\;=\;
\mathbf{r}_{\pi}
\;+\;
\gamma\,\mathbf{P}_{\pi}\,\mathbf{v}_{\pi}.
\]</div>
<p>Here:</p>
<ol>
<li><span class="arithmatex">\(\mathbf{v}_{\pi}\)</span> is the <span class="arithmatex">\(|\mathcal{S}|\times 1\)</span> column vector of values <span class="arithmatex">\(\bigl[v_{\pi}(s_{1}),\,\dots,\,v_{\pi}(s_{|\mathcal{S}|})\bigr]^{\!\top}\)</span>.  </li>
<li><span class="arithmatex">\(\mathbf{r}_{\pi}\)</span> is the <span class="arithmatex">\(|\mathcal{S}|\times 1\)</span> column vector of expected immediate rewards under policy <span class="arithmatex">\(\pi\)</span> for each state.  </li>
<li><span class="arithmatex">\(\mathbf{P}_{\pi}\)</span> is an <span class="arithmatex">\(|\mathcal{S}|\times |\mathcal{S}|\)</span> matrix whose entries <span class="arithmatex">\(\bigl[\mathbf{P}_{\pi}\bigr]_{i,j}\)</span> capture the probability of transitioning from <span class="arithmatex">\(s_{i}\)</span> to <span class="arithmatex">\(s_{j}\)</span> when actions are chosen according to <span class="arithmatex">\(\pi\)</span>.</li>
</ol>
<p>4 <strong>Relation of the Coefficient to <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span></strong></p>
<p>By definition,</p>
<div class="arithmatex">\[
\bigl[\mathbf{P}_{\pi}\bigr]_{i,j}
\;=\;
\sum_{a \,\in\,\mathcal{A}}
\pi\bigl(a \,\vert\, s_{i}\bigr)
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>In other words, each matrix entry <span class="arithmatex">\([\mathbf{P}_{\pi}]_{i,j}\)</span> is the total transition probability from state <span class="arithmatex">\(s_{i}\)</span> to <span class="arithmatex">\(s_{j}\)</span> under policy <span class="arithmatex">\(\pi\)</span>, which is the sum over all actions <span class="arithmatex">\(a\)</span> of the product <span class="arithmatex">\(\pi(a \mid s_{i})\,P(s_{j} \mid s_{i},a)\)</span>.</p>
<p>Hence the coefficient <span class="arithmatex">\(\pi(a \mid s_{i})\,P(s_{j}\mid s_{i},a)\)</span> that appears in the single-state Bellman equation is precisely the partial contribution to <span class="arithmatex">\([\mathbf{P}_{\pi}]_{i,j}\)</span> from action <span class="arithmatex">\(a\)</span>. Summing that coefficient over all <span class="arithmatex">\(a\)</span> in a given row <span class="arithmatex">\((i)\)</span> gives the full matrix entry <span class="arithmatex">\([\mathbf{P}_{\pi}]_{i,j}\)</span>.</p>
<p>5 <strong>Summary</strong></p>
<p>• The “<span class="arithmatex">\(\text{(coefficient)}\)</span>” that multiplies <span class="arithmatex">\(v_{\pi}(s_{j})\)</span> in the single-state Bellman equation is</p>
<div class="arithmatex">\[
\pi\bigl(a \,\vert\, s_{i}\bigr)
\;\times\;
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>• When writing the full system in matrix form, these coefficients for each <span class="arithmatex">\((i,j)\)</span> and each action <span class="arithmatex">\(a\)</span> combine into the entries of the matrix <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span>. Specifically:</p>
<div class="arithmatex">\[
[\mathbf{P}_{\pi}]_{i,j}
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s_{i})\;
P\bigl(s_{j} \,\vert\, s_{i},\,a\bigr).
\]</div>
<p>• Consequently, the compact Bellman equation</p>
<div class="arithmatex">\[
\mathbf{v}_{\pi}
\;=\;
\mathbf{r}_{\pi}
\;+\;
\gamma
\,\mathbf{P}_{\pi}\,\mathbf{v}_{\pi}
\]</div>
<p>follows naturally from collecting each row’s (i.e., state’s) single-state Bellman equation in vector/matrix form.</p>
<h2 id="comapring-policies">Comapring Policies<a class="headerlink" href="#comapring-policies" title="Permanent link">&para;</a></h2>
<p>Suppose you have two policies <span class="arithmatex">\(\pi_1\)</span> and <span class="arithmatex">\(\pi_2\)</span>. And also suppose that the Bellman equation is solved so that we get <span class="arithmatex">\(v_{\pi_1}\)</span> and <span class="arithmatex">\(v_{\pi_2}\)</span>.  How is this used to evaluate and compare the two policies?</p>
<p><strong>Answer</strong></p>
<p>Below is a step-by-step explanation of how having the solved value functions <span class="arithmatex">\(v_{\pi_1}\)</span> and <span class="arithmatex">\(v_{\pi_2}\)</span> (from two different policies <span class="arithmatex">\(\pi_1\)</span> and <span class="arithmatex">\(\pi_2\)</span>) enables one to compare and evaluate those policies:</p>
<p>1 <strong>The Meaning of <span class="arithmatex">\(v_{\pi}(s)\)</span></strong></p>
<p>• For a given policy <span class="arithmatex">\(\pi\)</span> in a Markov Decision Process (MDP), the value function  </p>
<div class="arithmatex">\[
v_{\pi}(s)
  \;=\;
  \mathbb{E}\!\Bigl[G_t \,\Big\vert\, S_t = s\Bigr]
\]</div>
<p>tells us the expected return (cumulative discounted reward) starting from state <span class="arithmatex">\(s\)</span> and following <span class="arithmatex">\(\pi\)</span> thereafter.<br />
• Thus, <span class="arithmatex">\(v_{\pi}(s)\)</span> captures how “good” it is to be in state <span class="arithmatex">\(s\)</span> when one plans to follow <span class="arithmatex">\(\pi\)</span>.</p>
<p>2 <strong>Comparing Two Policies State by State</strong></p>
<p>• Once you have computed <span class="arithmatex">\(v_{\pi_1}(s)\)</span> and <span class="arithmatex">\(v_{\pi_2}(s)\)</span> for all <span class="arithmatex">\(s \in \mathcal{S}\)</span>, you can compare the policies pointwise by looking at each state <span class="arithmatex">\(s\)</span>:<br />
  – If <span class="arithmatex">\(v_{\pi_1}(s) &gt; v_{\pi_2}(s)\)</span>, then <span class="arithmatex">\(\pi_1\)</span> yields a higher expected return than <span class="arithmatex">\(\pi_2\)</span> specifically from state <span class="arithmatex">\(s\)</span>.<br />
  – If <span class="arithmatex">\(v_{\pi_2}(s)\)</span> exceeds <span class="arithmatex">\(v_{\pi_1}(s)\)</span>, then <span class="arithmatex">\(\pi_2\)</span> is better from that state.<br />
• A strict ordering of the policies for all states might not hold (one can find states where <span class="arithmatex">\(\pi_1\)</span> is better and others where <span class="arithmatex">\(\pi_2\)</span> is better). That said, if  </p>
<div class="arithmatex">\[
v_{\pi_1}(s)\;\ge\;v_{\pi_2}(s)
  \quad\text{for all }s,
\]</div>
<p>then we can unambiguously say <span class="arithmatex">\(\pi_1\)</span> is at least as good as <span class="arithmatex">\(\pi_2\)</span> in every state.</p>
<p>3 <strong>Comparing Two Policies from an Initial State or Distribution</strong></p>
<p>• In many RL problems, there is a designated initial state <span class="arithmatex">\(s_0\)</span> or an initial state distribution <span class="arithmatex">\(\mu_0\)</span>. In that scenario, one often compares the policies by looking at  </p>
<div class="arithmatex">\[
v_{\pi_1}(s_0)
  \quad\text{versus}\quad
  v_{\pi_2}(s_0),
\]</div>
<p>or by evaluating the expected return under <span class="arithmatex">\(\mu_0\)</span>:  </p>
<div class="arithmatex">\[
\sum_{s\in \mathcal{S}} 
  \mu_0(s)\,
  v_{\pi_1}(s)
  \quad\text{versus}\quad
  \sum_{s\in \mathcal{S}} 
  \mu_0(s)\,
  v_{\pi_2}(s).
\]</div>
<p>• The policy that yields a higher overall expected return from the start (or on average w.r.t. <span class="arithmatex">\(\mu_0\)</span>) is deemed better.</p>
<p>4 <strong>Practical Significance</strong></p>
<p>• If you have solved the Bellman equation for both policies (i.e., found <span class="arithmatex">\(v_{\pi_1}\)</span> and <span class="arithmatex">\(v_{\pi_2}\)</span>), then you have a quantitative measure of performance for each state in <span class="arithmatex">\(\mathcal{S}\)</span>.<br />
• You can use this information to decide which policy performs better in specific states, or from the viewpoint of a particular start state.<br />
• Often, this comparison step is part of Policy Iteration or other methods: one checks if an updated policy is better (in terms of the value function) than a previous one—if it is, one replaces the old policy.</p>
<p>5 <strong>Conclusion</strong></p>
<p>Yes, once you solve the Bellman equations to get <span class="arithmatex">\(v_{\pi_1}\)</span> and <span class="arithmatex">\(v_{\pi_2}\)</span>, you can evaluate and compare <span class="arithmatex">\(\pi_1\)</span> and <span class="arithmatex">\(\pi_2\)</span> by:<br />
1. Checking their value functions state by state, if desired.<br />
2. Checking the expected return from a specific start state or an initial state distribution.  </p>
<p>The policy with the larger expected return (from the relevant initial condition) is considered better. If one policy has a higher value function in all states, it strictly dominates the other policy.</p>
<h2 id="return-value">Return value<a class="headerlink" href="#return-value" title="Permanent link">&para;</a></h2>
<p>Below is a re-presentation of the concept of the action-value function, also known as the “Q-value” function, denoted by <span class="arithmatex">\(q_{\pi}\)</span>:</p>
<p>1) Motivation for <span class="arithmatex">\(q_{\pi}(s,a)\)</span><br />
   • We already know that for a given policy <span class="arithmatex">\(\pi\)</span>, the state-value function is:  </p>
<div class="arithmatex">\[
v_{\pi}(s) \;=\; \mathbb{E}\bigl[G_t \;\big\vert\; S_t = s\bigr].
\]</div>
<p>• However, this quantity does not distinguish which action is taken in state <span class="arithmatex">\(s\)</span>; it only assumes that actions will be chosen according to <span class="arithmatex">\(\pi\)</span>.<br />
   • If we want to evaluate taking a specific action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>, we introduce the action-value function.</p>
<p>2) Definition of the Action-Value<br />
   • The action-value function <span class="arithmatex">\(q_{\pi}(s,a)\)</span> is defined as:  </p>
<div class="arithmatex">\[
q_{\pi}(s,a) \;=\; \mathbb{E}\bigl[G_t \;\big\vert\; S_t = s,\; A_t = a,\; \pi\bigr].
\]</div>
<p>• This is the expected return obtained when:<br />
     1. We start in state <span class="arithmatex">\(s\)</span>.<br />
     2. We immediately take action <span class="arithmatex">\(a\)</span>.<br />
     3. Thereafter, we continue following policy <span class="arithmatex">\(\pi\)</span>.  </p>
<p>3) Relationship to <span class="arithmatex">\(v_{\pi}(s)\)</span><br />
   • From the definition above, after we take action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>, we end up in some next state <span class="arithmatex">\(s'\)</span> and keep following <span class="arithmatex">\(\pi\)</span>.<br />
   • If <span class="arithmatex">\(\pi\)</span> is stochastic, then  </p>
<div class="arithmatex">\[
v_{\pi}(s) 
     \;=\; 
     \sum_{a \in \mathcal{A}} \pi(a \mid s)\; q_{\pi}(s,a).
\]</div>
<p>• Intuitively, <span class="arithmatex">\(v_{\pi}(s)\)</span> is the average of <span class="arithmatex">\(q_{\pi}(s,a)\)</span> over all possible actions <span class="arithmatex">\(a\)</span>, weighted by <span class="arithmatex">\(\pi(a \mid s)\)</span>.</p>
<p>4) Bellman Equation for <span class="arithmatex">\(q_{\pi}\)</span><br />
   • The Bellman expectation equation for <span class="arithmatex">\(q_{\pi}(s,a)\)</span> can be written as:  </p>
<div class="arithmatex">\[
q_{\pi}(s,a)
     \;=\; 
     \mathbb{E}\Bigl[
       R_{t+1} \;+\; \gamma \, v_{\pi}(S_{t+1})
       \;\Big\vert\;
       S_t = s,\; A_t = a
     \Bigr].
\]</div>
<p>• Equivalently, since  </p>
<div class="arithmatex">\[
v_{\pi}(S_{t+1})
     \;=\;
     \sum_{a'} \pi(a' \mid S_{t+1})\,q_{\pi}(S_{t+1},\,a'),
\]</div>
<p>we can expand the right-hand side to express <span class="arithmatex">\(q_{\pi}\)</span> entirely in terms of immediate rewards and future <span class="arithmatex">\(q_{\pi}\)</span>-values.</p>
<p>5) Why <span class="arithmatex">\(q_{\pi}\)</span> Matters<br />
   • Many RL algorithms, such as Q-learning and SARSA, center around learning an approximation of <span class="arithmatex">\(q_{\pi}(s,a)\)</span>.<br />
   • Once <span class="arithmatex">\(q_{\pi}\)</span> is known (or estimated), choosing actions becomes straightforward: one can compare the values of different actions in the same state to see which leads to the greatest expected return.</p>
<p>6) Summary<br />
   • The state-value function <span class="arithmatex">\(v_{\pi}(s)\)</span> follows a policy everywhere.<br />
   • The action-value function <span class="arithmatex">\(q_{\pi}(s,a)\)</span> is a finer-grained measure that tells us the expected return if we pick a specific action <span class="arithmatex">\(a\)</span> now, then continue with <span class="arithmatex">\(\pi\)</span>.<br />
   • Both functions are crucial to understanding and devising strategies in Reinforcement Learning.</p>
<h2 id="bellman-equations-for-q_pi-and-v_pi">Bellman Equations for <span class="arithmatex">\(q_{\pi}\)</span> and <span class="arithmatex">\(v_{\pi}\)</span><a class="headerlink" href="#bellman-equations-for-q_pi-and-v_pi" title="Permanent link">&para;</a></h2>
<p><strong><em>The step 4 above on the Bellman equation for <span class="arithmatex">\(q_{\pi}\)</span>. Derive this with details and show the connection to the Bellman equation for <span class="arithmatex">\(v_{\pi}\)</span>.</em></strong></p>
<p><strong><em>Answer</em></strong></p>
<p>Below is a detailed derivation of the Bellman equation for the action-value function, denoted by <span class="arithmatex">\(q_{\pi}(s,a)\)</span>, 
showing its relationship to the Bellman equation for the state-value function <span class="arithmatex">\(v_{\pi}(s)\)</span>.</p>
<p>────────────────────────────────</p>
<p>1) Recall the Definitions</p>
<p>───────────────────────────────</p>
<p>• State-Value Function (under policy <span class="arithmatex">\(\pi\)</span>):  </p>
<div class="arithmatex">\[
v_{\pi}(s)
  \;=\;
  \mathbb{E}\bigl[G_t 
    \;\big\vert\;
    S_t = s
  \bigr],
\]</div>
<p>which is the expected return when starting in state <span class="arithmatex">\(s\)</span> at time <span class="arithmatex">\(t\)</span> and thereafter following <span class="arithmatex">\(\pi\)</span>.</p>
<p>• Action-Value Function (under policy <span class="arithmatex">\(\pi\)</span>):  </p>
<div class="arithmatex">\[
q_{\pi}(s,a)
  \;=\;
  \mathbb{E}\bigl[G_t
    \;\big\vert\;
    S_t = s,\;
    A_t = a,\;
    \pi
  \bigr],
\]</div>
<p>which is the expected return when starting in state <span class="arithmatex">\(s\)</span> at time <span class="arithmatex">\(t\)</span>, taking action <span class="arithmatex">\(a\)</span> immediately, and then following <span class="arithmatex">\(\pi\)</span> thereafter.</p>
<p>• Return <span class="arithmatex">\(G_t\)</span>:  </p>
<div class="arithmatex">\[
G_t
  \;=\;
  R_{t+1} \;+\;\gamma R_{t+2} \;+\;\gamma^2 R_{t+3} \;+\;\dots
  \;=\;
  \sum_{k=0}^{\infty}
  \gamma^k \, R_{t+k+1}.
\]</div>
<p>Here, <span class="arithmatex">\(0 \le \gamma \le 1\)</span> is the discount factor.</p>
<p>────────────────────────────────</p>
<p>2 One-Step Decomposition of the Return</p>
<p>────────────────────────────────</p>
<p>We start with the relation  </p>
<div class="arithmatex">\[
G_t
\;=\;
R_{t+1} \;+\; \gamma\,G_{t+1}.
\]</div>
<p>Taking the conditional expectation on both sides given <span class="arithmatex">\(\bigl(S_t = s,\; A_t = a,\; \pi\bigr)\)</span>:</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\mathbb{E}\bigl[
  G_t
  \;\big\vert\;
  S_t = s,\;
  A_t = a,\;
  \pi
\bigr]
\;=\;
\mathbb{E}\Bigl[
  R_{t+1} \;+\; \gamma\,G_{t+1}
  \;\Big\vert\;
  S_t = s,\;
  A_t = a,\;
  \pi
\Bigr].
\]</div>
<p>─────────────────────────────────</p>
<p>3 Separate Immediate Reward and Future Return</p>
<p>────────────────────────────────</p>
<p>We can split the expectation:</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\underbrace{
  \mathbb{E}\bigl[
    R_{t+1}
    \,\big\vert\,
    S_t = s,\;
    A_t = a,\;
    \pi
  \bigr]
}_{\text{expected immediate reward}}
\;+\;
\gamma
\underbrace{
  \mathbb{E}\bigl[
    G_{t+1}
    \,\big\vert\,
    S_t = s,\;
    A_t = a,\;
    \pi
  \bigr]
}_{\text{expected future discounted return}}
.
\]</div>
<ol>
<li>The term <span class="arithmatex">\(\mathbb{E}[\,R_{t+1}\,\vert\, s,a,\pi]\)</span> is the expected reward for taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>.  </li>
<li>The term <span class="arithmatex">\(\mathbb{E}[\,G_{t+1}\,\vert\, s,a,\pi]\)</span> signals that once we have taken action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>, we transition to some state <span class="arithmatex">\(s'\)</span>, and from <span class="arithmatex">\(t+1\)</span> onward we follow policy <span class="arithmatex">\(\pi\)</span>. The expected future return from <span class="arithmatex">\(s'\)</span> under <span class="arithmatex">\(\pi\)</span> is <span class="arithmatex">\(v_{\pi}(s')\)</span>.</li>
</ol>
<p>────────────────────────────────</p>
<p>4 Transition to the Next State</p>
<p>────────────────────────────────</p>
<p>The environment’s transition function <span class="arithmatex">\(P(s' \mid s,a)\)</span> gives the probability of ending up in state <span class="arithmatex">\(s'\)</span> after taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. Hence,</p>
<div class="arithmatex">\[
\mathbb{E}\bigl[
  R_{t+1}
  \;\big\vert\;
  S_t = s,\;
  A_t = a
\bigr]
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \;\vert\; s,a)\,\;R(s,a,s'),
\]</div>
<p>assuming <span class="arithmatex">\(R(s,a,s')\)</span> is the associated immediate reward for the transition <span class="arithmatex">\((s,a)\to s'\)</span>.<br />
Similarly,</p>
<div class="arithmatex">\[
\mathbb{E}\bigl[
  G_{t+1}
  \;\big\vert\;
  S_t = s,\;
  A_t = a,\;
  \pi
\bigr]
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \;\vert\; s,a)
\,\underbrace{
  \mathbb{E}[\,G_{t+1}\,\vert\,S_{t+1} = s',\,\pi]
}_{v_{\pi}(s')}
,
\]</div>
<p>since once we arrive in <span class="arithmatex">\(s'\)</span>, we follow <span class="arithmatex">\(\pi\)</span> and the expected return is <span class="arithmatex">\(v_{\pi}(s')\)</span>.</p>
<p>─────────────────────────────────</p>
<p>5 The Bellman Equation for <span class="arithmatex">\(q_{\pi}(s,a)\)</span></p>
<p>─────────────────────────────────</p>
<p>Putting these pieces together:</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma \, v_{\pi}(s')
\Bigr].
\]</div>
<p>This is the Bellman expectation equation for <span class="arithmatex">\(q_{\pi}(s,a)\)</span>. It shows that the action-value under <span class="arithmatex">\(\pi\)</span> is the expected immediate reward plus the discounted value of the next state, averaged over all possible next states <span class="arithmatex">\(s'\)</span>.</p>
<p>─────────────────────────────────</p>
<p>6 Connection to the Bellman Equation for <span class="arithmatex">\(v_{\pi}(s)\)</span></p>
<p>──────────────────────</p>
<p>The state-value function <span class="arithmatex">\(v_{\pi}(s)\)</span> can be expressed in terms of <span class="arithmatex">\(q_{\pi}(s,a)\)</span>:</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \,\vert\, s)\;q_{\pi}(s,a).
\]</div>
<p>In other words, <span class="arithmatex">\(v_{\pi}(s)\)</span> is the expectation of <span class="arithmatex">\(q_{\pi}(s,a)\)</span> over the policy <span class="arithmatex">\(\pi(a \mid s)\)</span>. Conversely, when we write <span class="arithmatex">\(q_{\pi}(s,a)\)</span> in its Bellman form, we use <span class="arithmatex">\(v_{\pi}(s')\)</span> to handle the future return once the agent transitions to the next state <span class="arithmatex">\(s'\)</span>:</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma \,\underbrace{
    \sum_{a' \,\in\, \mathcal{A}}
    \pi(a' \,\vert\, s')\,q_{\pi}(s',a')
  }_{v_{\pi}(s')}
\Bigr].
\]</div>
<p>Thus, <span class="arithmatex">\(q_{\pi}\)</span> and <span class="arithmatex">\(v_{\pi}\)</span> are tied together through these two Bellman equations.</p>
<p>─────────────────────────────────</p>
<p>7 Summary</p>
<p>─────────────────────────────────</p>
<p>• We started from the relation <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>, conditioning on <span class="arithmatex">\(\{S_t = s, A_t = a\}\)</span> and policy <span class="arithmatex">\(\pi\)</span>.<br />
• We split the expectation into the immediate reward plus the discounted future return.<br />
• By summing over next states <span class="arithmatex">\(s'\)</span> and using the definition of <span class="arithmatex">\(v_{\pi}(s')\)</span>, we arrived at</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \,\vert\, s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_{\pi}(s')
\Bigr].
\]</div>
<p>• Since <span class="arithmatex">\(v_{\pi}(s') = \sum_{a'} \pi(a' \mid s')\,q_{\pi}(s',a')\)</span>, we see the close relationship between these two value functions. This completes the derivation of the Bellman equation for the action-value function <span class="arithmatex">\(q_{\pi}(s,a)\)</span> and clarifies its connection to the Bellman equation for <span class="arithmatex">\(v_{\pi}(s)\)</span>.</p>
<h2 id="optimal-policy">Optimal Policy<a class="headerlink" href="#optimal-policy" title="Permanent link">&para;</a></h2>
<p><strong><em>Now that we have the concepts of the state value <span class="arithmatex">\(v_{\pi}\)</span> and the return value <span class="arithmatex">\(q_{\pi}\)</span> for a given policy <span class="arithmatex">\(\pi\)</span>, and the associated Bellman equations, there is the concept of the optimal policy <span class="arithmatex">\(\pi^*\)</span>. There is an associated concept of the Bellman Optimality Equation. Explain these concepts.</em></strong></p>
<p><strong><em>Answer</em></strong></p>
<p>Below is a step-by-step explanation of the concept of an optimal policy in Reinforcement Learning, and how it leads to the Bellman Optimality Equations.</p>
<p>──────────────────────────</p>
<p>1 Policy and Value Functions</p>
<hr />
<p>• A policy <span class="arithmatex">\(\pi\)</span> is any mapping from states to action probabilities (or actions, if deterministic).<br />
• The state-value function for a given policy <span class="arithmatex">\(\pi\)</span> is  </p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\mathbb{E}\bigl[G_t \;\big\vert\; S_t = s,\;\pi \bigr],
\]</div>
<p>where <span class="arithmatex">\(G_t\)</span> is the return from time <span class="arithmatex">\(t\)</span> onward.<br />
• The action-value function for <span class="arithmatex">\(\pi\)</span> is  </p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\mathbb{E}\bigl[G_t \;\big\vert\; S_t = s,\; A_t = a,\;\pi \bigr].
\]</div>
<p>These value functions describe how “good” it is to be in a particular state <span class="arithmatex">\(s\)</span>—or to take a particular action <span class="arithmatex">\(a\)</span> in <span class="arithmatex">\(s\)</span>—under the policy <span class="arithmatex">\(\pi\)</span>.</p>
<hr />
<p>2 Optimal Policy <span class="arithmatex">\(\pi^*\)</span></p>
<p>─────────────────────────</p>
<p>• The goal in Reinforcement Learning is often to find a policy <span class="arithmatex">\(\pi^*\)</span> that maximizes the expected return from every state.<br />
• Formally, <span class="arithmatex">\(\pi^*\)</span> is said to be optimal if, for all states <span class="arithmatex">\(s\in \mathcal{S}\)</span>,</p>
<div class="arithmatex">\[
v_{\pi^*}(s)
\;=\;
\max_{\pi} \;
v_{\pi}(s).
\]</div>
<p>Equivalently, <span class="arithmatex">\(\pi^*\)</span> achieves or exceeds the performance of every other policy <span class="arithmatex">\(\pi\)</span>.<br />
• There can be multiple optimal policies, but they share the same value functions, denoted <span class="arithmatex">\(v_*\)</span> and <span class="arithmatex">\(q_*\)</span>, defined below.</p>
<p>──────────────────────────────</p>
<p>3 Optimal Value Functions</p>
<p>────────────────────────────</p>
<p>We define  </p>
<div class="arithmatex">\[
v_*(s)
\;=\;
\max_{\pi} \;v_{\pi}(s),
\]</div>
<p>and  </p>
<div class="arithmatex">\[
q_*(s,a)
\;=\;
\max_{\pi} \;q_{\pi}(s,a).
\]</div>
<p>• Thus, <span class="arithmatex">\(v_*(s)\)</span> is the maximum possible expected return achievable from state <span class="arithmatex">\(s\)</span>, over all policies.<br />
• Similarly, <span class="arithmatex">\(q_*(s,a)\)</span> is the maximum possible expected return when starting from <span class="arithmatex">\(s\)</span>, taking action <span class="arithmatex">\(a\)</span>, and thereafter following the best possible policy.</p>
<p>───────────────────────────────</p>
<p>4 Bellman Optimality Equations</p>
<p>────────────────────────</p>
<p>• Just as there are Bellman equations for <span class="arithmatex">\(v_{\pi}\)</span> and <span class="arithmatex">\(q_{\pi}\)</span> under a fixed policy <span class="arithmatex">\(\pi\)</span>, there are corresponding Bellman Optimality Equations for <span class="arithmatex">\(v_*\)</span> and <span class="arithmatex">\(q_*\)</span>.<br />
• For the optimal state-value function <span class="arithmatex">\(v_*(s)\)</span>, the Bellman Optimality Equation is:</p>
<div class="arithmatex">\[
v_*(s)
\;=\;
\max_{a \,\in\,\mathcal{A}}
\;\biggl[
  \sum_{s' \,\in\,\mathcal{S}}
  P(s' \mid s,a)\,
  \Bigl(
    R(s,a,s') \;+\;\gamma\,v_*(s')
  \Bigr)
\biggr].
\]</div>
<p>• In words: from state <span class="arithmatex">\(s\)</span>, pick the action <span class="arithmatex">\(a\)</span> that gives the highest expected immediate reward plus the discounted optimal value of the next state.</p>
<p>• For the optimal action-value function <span class="arithmatex">\(q_*(s,a)\)</span>, the Bellman Optimality Equation is:</p>
<div class="arithmatex">\[
q_*(s,a)
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \mid s,a)
\Bigl[
  R(s,a,s') \;+\;
  \gamma \,\max_{a'\,\in\,\mathcal{A}} \,q_*(s',a')
\Bigr].
\]</div>
<p>• Here, after taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> to reach <span class="arithmatex">\(s'\)</span>, we assume the agent chooses the (optimal) action <span class="arithmatex">\(a' = \arg\max_{\,a'} q_*(s',a')\)</span> thereafter.</p>
<p>───────────────────────────────</p>
<p>5 Connection to the Concept of Optimality</p>
<p>────────────────────────────</p>
<p>• If the agent somehow knows <span class="arithmatex">\(q_*\)</span>, it can act optimally by choosing, in each state <span class="arithmatex">\(s\)</span>, the action <span class="arithmatex">\(a\)</span> that maximizes <span class="arithmatex">\(q_*(s,a)\)</span>.<br />
• Equivalently, if the agent knows <span class="arithmatex">\(v_*\)</span> and the environment’s transition probabilities, then to select the optimal action from a state <span class="arithmatex">\(s\)</span>, one can compute the action that attains the maximum in</p>
<div class="arithmatex">\[
\max_{a}\;\bigl[
  \sum_{s'}
    P(s' \mid s,a)\,
    (\,R(s,a,s') + \gamma\,v_*(s')\,)
\bigr].
\]</div>
<p>──────────────────────────────</p>
<p>6 Summary</p>
<p>────────────────────────────</p>
<ol>
<li>An optimal policy <span class="arithmatex">\(\pi^*\)</span> is one that achieves the highest possible value in every state (or equivalently, for every state-action pair).  </li>
<li>The optimal value functions <span class="arithmatex">\(v_*\)</span> and <span class="arithmatex">\(q_*\)</span> are defined as the supremum over all policies of <span class="arithmatex">\(v_{\pi}\)</span> and <span class="arithmatex">\(q_{\pi}\)</span>, respectively.  </li>
<li>They satisfy the Bellman Optimality Equations, which replace the explicit dependence on <span class="arithmatex">\(\pi\)</span> with a maximization over actions.  </li>
<li>Solving these equations in principle yields <span class="arithmatex">\(v_*\)</span> (or <span class="arithmatex">\(q_*\)</span>) and hence identifies which actions are optimal in each state, leading to an optimal policy.</li>
</ol>
<h2 id="bellman-optimality-equation">Bellman Optimality Equation<a class="headerlink" href="#bellman-optimality-equation" title="Permanent link">&para;</a></h2>
<p><strong><em>The selection of an optimal policy implies choosing the maximum or supremum over all policies. However, in step 4 describing the Bellman Optimality Equation, there does not seem to be an explicit selection across the set of policies. Perhaps, this is an outcome. If so, derive in detail, how the Bellman Optimality Equation follows from the notion of optimal value functions.</em></strong></p>
<p><strong><em>Answer</em></strong></p>
<p>Below is a detailed derivation showing how the notion of “maximizing over all policies” leads to the Bellman Optimality Equations, using LaTeX for all mathematical symbols and expressions.</p>
<p>─────────────────────────────</p>
<p>1 Optimal Value Functions</p>
<p>─────────────────────</p>
<p>We define the optimal state-value function and action-value function as follows:</p>
<p>• Optimal state-value function:  </p>
<div class="arithmatex">\[
v_*(s) 
\;=\; 
\max_{\pi}\;v_{\pi}(s),
\]</div>
<p>where $v_{\pi}(s) = \mathbb{E}!\bigl[G_t \,\vert\,S_t = s,\;\pi\bigr] $</p>
<p>• Optimal action-value function:</p>
<div class="arithmatex">\[
q_*(s,a) 
\;=\;
\max_{\pi}\;q_{\pi}(s,a),
\]</div>
<p>where <span class="arithmatex">\(q_{\pi}(s,a) = \mathbb{E}\!\bigl[G_t \,\vert\,S_t = s,\;A_t = a,\;\pi\bigr]\)</span></p>
<p>The notation “<span class="arithmatex">\(\max_{\pi}\)</span>” means we take the supremum (maximum) over all possible policies <span class="arithmatex">\(\pi\)</span>. Under standard conditions (e.g., finite state space, bounded rewards, or <span class="arithmatex">\(\gamma&lt;1\)</span>), these maxima exist and define <span class="arithmatex">\(v_*\)</span> and <span class="arithmatex">\(q_*\)</span>.</p>
<p>───────────────────────────────</p>
<p>2 Bellman Equation for a Given Policy <span class="arithmatex">\(\pi\)</span></p>
<p>─────────────────────</p>
<p>For reference, recall that for any fixed policy <span class="arithmatex">\(\pi\)</span>, the Bellman expectation equations are:</p>
<p>• State-value:</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\sum_{a \,\in\, \mathcal{A}}
\pi(a \mid s)
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \mid s,a\bigr)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_{\pi}(s')
\Bigr].
\]</div>
<p>• Action-value:</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \mid s,a\bigr)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma \,v_{\pi}(s')
\Bigr].
\]</div>
<p>These follow directly from the fact that <span class="arithmatex">\(G_t = R_{t+1} + \gamma\,G_{t+1}\)</span> and taking expectations conditioned on <span class="arithmatex">\((S_t=s, A_t=a)\)</span> or <span class="arithmatex">\((S_t=s)\)</span> respectively.</p>
<p>──────────────────────</p>
<p>3 Maximizing Over Policies for <span class="arithmatex">\(v_*\)</span></p>
<p>──────────────────────────────</p>
<p>By definition,</p>
<div class="arithmatex">\[
v_*(s)
\;=\;
\max_{\pi}\,v_{\pi}(s).
\]</div>
<p>Substitute the Bellman equation for <span class="arithmatex">\(v_{\pi}(s)\)</span>:</p>
<div class="arithmatex">\[
v_*(s)
\;=\;
\max_{\pi}
\sum_{a}
\pi(a \mid s)
\sum_{s'}
P(s' \mid s,a)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_{\pi}(s')
\Bigr].
\]</div>
<p>Notice that for each fixed state <span class="arithmatex">\(s\)</span>, a policy <span class="arithmatex">\(\pi\)</span> chooses a distribution <span class="arithmatex">\(\{\pi(a \mid s)\}\)</span> over actions <span class="arithmatex">\(a\)</span>. But taking a probability-weighted sum of values cannot exceed simply picking the single best action. Formally,</p>
<div class="arithmatex">\[
\max_{\pi}\,
\sum_{a}
\pi(a \mid s)\,f(a)
\;=\;
\max_{a}\,f(a),
\]</div>
<p>for any function <span class="arithmatex">\(f\)</span>. Let</p>
<div class="arithmatex">\[
f(a)
\;=\;
\sum_{s'}
P(s' \mid s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma \,\underbrace{v_{\pi}(s')}_{\le v_*(s')}
\Bigr].
\]</div>
<p>Because <span class="arithmatex">\(v_{\pi}(s') \le v_*(s')\)</span> for all <span class="arithmatex">\(\pi\)</span> and <span class="arithmatex">\(s'\)</span>, the best we can do is to replace <span class="arithmatex">\(v_{\pi}(s')\)</span> by <span class="arithmatex">\(v_*(s')\)</span>. Thus,</p>
<div class="arithmatex">\[
v_*(s)
\;=\;
\max_{a}
\sum_{s'}
P(s' \mid s,a)\,
\bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_*(s')
\bigr].
\]</div>
<p>This yields the Bellman Optimality Equation for <span class="arithmatex">\(v_*(s)\)</span>:</p>
<div class="arithmatex">\[
\boxed{
v_*(s)
\;=\;
\max_{a \,\in\, \mathcal{A}}\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \mid s,a)\;
\bigl[
  R(s,a,s')
  \;+\;\gamma\,v_*(s')
\bigr].
}
\]</div>
<p>────────────────────────────</p>
<p>4 Bellman Optimality Equation for <span class="arithmatex">\(q_*\)</span></p>
<p>───────────────────────</p>
<p>Similarly, from</p>
<div class="arithmatex">\[
q_{\pi}(s,a)
\;=\;
\sum_{s'}
P(s' \mid s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_{\pi}(s')
\Bigr],
\]</div>
<p>we define</p>
<div class="arithmatex">\[
q_*(s,a)
\;=\;
\max_{\pi}\;
q_{\pi}(s,a).
\]</div>
<p>But we already have</p>
<div class="arithmatex">\[
v_{\pi}(s')
\;\le\;
v_*(s'),
\]</div>
<p>and</p>
<div class="arithmatex">\[
v_*(s')
\;=\;
\max_{a'}\;
q_*(s',a').
\]</div>
<p>Hence, by substituting <span class="arithmatex">\(v_*(s') = \max_{a'} q_*(s',a')\)</span>, we arrive at</p>
<div class="arithmatex">\[
q_*(s,a)
\;=\;
\sum_{s'}
P(s' \mid s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\;\max_{a'}\;q_*(s',a')
\Bigr].
\]</div>
<p>This is the Bellman Optimality Equation for the action-value function:</p>
<div class="arithmatex">\[
\boxed{
q_*(s,a)
\;=\;
\sum_{s'\,\in\,\mathcal{S}}
P\bigl(s' \mid s,a\bigr)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma
  \,\max_{a'\,\in\,\mathcal{A}}
  q_*(s',a')
\Bigr].
}
\]</div>
<p>──────────────────────────────</p>
<p>5 Interpretation and Summary</p>
<p>──────────────────────</p>
<p>• By defining <span class="arithmatex">\(v_*\)</span> and <span class="arithmatex">\(q_*\)</span> as the maximum over all policies of the corresponding value functions, we effectively pick the best possible action choices at each state.<br />
• The Markov Decision Process structure means that “maximizing over policies” can be carried out state by state, leading to a pointwise maximization over actions in the Bellman Optimality Equations.<br />
• Once one computes or approximates <span class="arithmatex">\(q_*\)</span>, one obtains an optimal policy simply by greedily choosing the action that maximizes <span class="arithmatex">\(q_*(s,a)\)</span> in each state <span class="arithmatex">\(s\)</span>. Equivalently, from <span class="arithmatex">\(v_*\)</span> and knowledge of the environment’s transitions and rewards, one can also identify the best actions.</p>
<p>Thus, the Bellman Optimality Equations naturally arise from the desire to choose the best action in each state—an outcome of maximizing over all possible policies in an MDP.</p>
<h2 id="existence-of-optimal-policies">Existence of optimal policies<a class="headerlink" href="#existence-of-optimal-policies" title="Permanent link">&para;</a></h2>
<p><strong><em>You claim that under standard conditions such as finite state space and bounded rewards, the optimal state value <span class="arithmatex">\(v_*(s) \;=\; \max_{\pi}\;v_{\pi}(s)\)</span> and the action value <span class="arithmatex">\(q_*(s,a) \;=\;\max_{\pi}\;q_{\pi}(s,a)\)</span> exist. Formalize the argument to support this claim.</em></strong></p>
<p>Below is a step-by-step argument showing why, under standard conditions such as a finite Markov Decision Process (MDP) with discounted and bounded rewards, the following optimal value functions exist and are well-defined:</p>
<p>• The optimal state-value function:  </p>
<div class="arithmatex">\[
v_{*}(s) \;=\; \max_{\pi}\,v_{\pi}(s),
\]</div>
<p>• The optimal action-value function:  </p>
<div class="arithmatex">\[
q_{*}(s,a) \;=\; \max_{\pi}\,q_{\pi}(s,a).
\]</div>
<p>1 <strong>Finite MDP and Bounded Reward Assumptions</strong></p>
<p>Assume:</p>
<ol>
<li>A finite state space <span class="arithmatex">\(\mathcal{S} = \{\,s_{1},\dots, s_{|\mathcal{S}|}\}\)</span>.  </li>
<li>A finite action space <span class="arithmatex">\(\mathcal{A} = \{\,a_{1}, \dots, a_{|\mathcal{A}|}\}\)</span>.  </li>
<li>A discount factor <span class="arithmatex">\(0 \,\le\, \gamma &lt; 1.\)</span>  </li>
<li>Bounded rewards: there exists <span class="arithmatex">\(R_{\max} \ge 0\)</span> such that  </li>
</ol>
<div class="arithmatex">\[
\bigl|\,R(s,a,s')\bigr| \;\le\; R_{\max}
\]</div>
<p>for all feasible triples <span class="arithmatex">\((s,a,s')\)</span>.</p>
<p>Under these conditions, each state-action-next-state combination appears in only finitely many possibilities, and any infinite sum of discounted, bounded rewards converges well.</p>
<p>2 <strong>Value Functions for a Fixed Policy <span class="arithmatex">\(\pi\)</span></strong></p>
<p>Consider a fixed policy <span class="arithmatex">\(\pi\)</span>. Define the return from time <span class="arithmatex">\(t\)</span> as</p>
<div class="arithmatex">\[
G_{t} \;=\; \sum_{k=0}^{\infty} \gamma^{k}\,R_{t+k+1},
\]</div>
<p>where <span class="arithmatex">\(R_{t+k+1}\)</span> is the reward at time <span class="arithmatex">\(t+k+1\)</span>. Because <span class="arithmatex">\(\gamma \in [0,1)\)</span> and <span class="arithmatex">\(\bigl|R_{t+k+1}\bigr|\le R_{\max}\)</span>, the infinite series converges almost surely and satisfies</p>
<div class="arithmatex">\[
\bigl|\!G_{t}\bigr|\;\le\;\sum_{k=0}^{\infty} \gamma^{k}\,R_{\max}
\;=\;\frac{R_{\max}}{1-\gamma}.
\]</div>
<p>Hence the state-value function</p>
<div class="arithmatex">\[
v_{\pi}(s) \;=\; \mathbb{E}\bigl[G_{t}\,\big\vert\,S_{t}=s,\;\pi\bigr]
\]</div>
<p>is well-defined and bounded. Likewise, the action-value function</p>
<div class="arithmatex">\[
q_{\pi}(s,a) \;=\;\mathbb{E}\bigl[G_{t}\,\big\vert\,S_{t}=s,\,A_{t}=a,\,\pi\bigr]
\]</div>
<p>is also bounded by <span class="arithmatex">\(\pm\,R_{\max}/(1-\gamma)\)</span>.</p>
<p>3 <strong>Supremum Over All Policies</strong></p>
<p>For each state <span class="arithmatex">\(s\)</span>, define</p>
<div class="arithmatex">\[
v_{*}(s)\;=\;\sup_{\pi}\,v_{\pi}(s).
\]</div>
<p>Similarly, for each state-action pair <span class="arithmatex">\((s,a)\)</span>, define</p>
<div class="arithmatex">\[
q_{*}(s,a)\;=\;\sup_{\pi}\,q_{\pi}(s,a).
\]</div>
<p>Since <span class="arithmatex">\(v_{\pi}(s)\)</span> and <span class="arithmatex">\(q_{\pi}(s,a)\)</span> are bounded above by <span class="arithmatex">\(R_{\max}/(1-\gamma)\)</span> and below by <span class="arithmatex">\(-\,R_{\max}/(1-\gamma)\)</span>, the suprema <span class="arithmatex">\(v_{*}(s)\)</span> and <span class="arithmatex">\(q_{*}(s,a)\)</span> are finite.</p>
<p>4 <strong>Existence of an Optimal Policy</strong></p>
<p>Because <span class="arithmatex">\(\mathcal{S}\)</span> and <span class="arithmatex">\(\mathcal{A}\)</span> are finite, there are only finitely many deterministic policies: specifically <span class="arithmatex">\(\lvert\mathcal{A}\rvert^{\lvert\mathcal{S}\rvert}\)</span>. Consider the set</p>
<div class="arithmatex">\[
\Bigl\{\;v_{\pi}\,\bigm|\,\pi \text{ is deterministic}\Bigr\}.
\]</div>
<p>Each <span class="arithmatex">\(v_{\pi}\)</span> is a vector in <span class="arithmatex">\(\mathbb{R}^{\lvert\mathcal{S}\rvert}\)</span>. Hence the supremum over a finite set is actually a maximum, implying there exists at least one deterministic policy <span class="arithmatex">\(\pi^{*}\)</span> such that</p>
<div class="arithmatex">\[
v_{\pi^{*}}(s)\;=\;v_{*}(s)\quad \text{for all }s.
\]</div>
<p>Thus the supremum is in fact attained by at least one policy <span class="arithmatex">\(\pi^{*}\)</span>. An analogous argument holds for <span class="arithmatex">\(q_{*}(s,a)\)</span>.</p>
<p>5 <strong>Conclusion</strong></p>
<p>Under these standard conditions:</p>
<ol>
<li>For each policy <span class="arithmatex">\(\pi\)</span>, <span class="arithmatex">\(v_{\pi}\)</span> and <span class="arithmatex">\(q_{\pi}\)</span> exist and lie in <span class="arithmatex">\(\bigl[-\,R_{\max}/(1-\gamma),\;R_{\max}/(1-\gamma)\bigr]\)</span>.  </li>
<li>Taking the supremum over all policies yields finite quantities <span class="arithmatex">\(v_{*}(s)\)</span> and <span class="arithmatex">\(q_{*}(s,a)\)</span>.  </li>
<li>Because only finitely many deterministic policies exist, the supremum is actually a maximum, and an optimal policy <span class="arithmatex">\(\pi^{*}\)</span> attains those maximal values.  </li>
</ol>
<p>Hence the definitions</p>
<div class="arithmatex">\[
v_{*}(s)\;=\;\max_{\pi} v_{\pi}(s)
\quad\text{and}\quad
q_{*}(s,a)\;=\;\max_{\pi} q_{\pi}(s,a)
\]</div>
<p>are well-defined in such finite, discounted MDPs.</p>
<h2 id="uniqueness-of-optimal-policies">Uniqueness of Optimal policies<a class="headerlink" href="#uniqueness-of-optimal-policies" title="Permanent link">&para;</a></h2>
<p><strong><em>What about uniqueness of these optimal policies? Can there be two different optimal policies?</em></strong></p>
<p>Below is a step-by-step explanation of whether the optimal policy in a finite Markov Decision Process (MDP) with discounted rewards and bounded reward magnitudes must be unique. All mathematical expressions are written in standard LaTeX:</p>
<p>1) Existence of an Optimal Policy</p>
<p>In a finite discounted MDP with  <span class="arithmatex">\(\ 0 \le \gamma &lt; 1 \)</span><br />
and bounded rewards, there exists at least one policy <span class="arithmatex">\(\ \pi^*\)</span> that achieves the highest possible value in every state <span class="arithmatex">\(s\)</span>. Formally,  </p>
<div class="arithmatex">\[
v_{*}(s) \;=\; v_{\pi^*}(s)
\]</div>
<p>for all <span class="arithmatex">\(s\)</span>. Here, the state-value function for any policy <span class="arithmatex">\(\ \pi\)</span> is  </p>
<div class="arithmatex">\[
v_{\pi}(s) \;=\; \mathbb{E}\!\Bigl[\;\sum_{k=0}^{\infty} \gamma^k \; R_{t+k+1}\;\Bigm|\; S_t = s,\; \pi\Bigr],
\]</div>
<p>and the optimal state-value function is defined by  </p>
<div class="arithmatex">\[
v_{*}(s) \;=\; \max_{\pi}\; v_{\pi}(s).
\]</div>
<p>2) Multiple Optimal Policies</p>
<p>Even though an optimal policy <span class="arithmatex">\(\ \pi^*\)</span> exists, it need not be unique. Multiple distinct policies can yield the same optimal value function <span class="arithmatex">\(v_{*}(s)\)</span>. Common reasons for this include:</p>
<p>• Two or more actions in a given state <span class="arithmatex">\(s\)</span> might produce the exact same expected immediate reward plus the same transitions leading to the same future discounted returns.<br />
• In more complex scenarios, there may be different ways to behave optimally across one or more states, all yielding the same overall returns.</p>
<p>3) Ties in the Optimal Action-Value Function</p>
<p>Consider the optimal action-value function</p>
<div class="arithmatex">\[
q_{*}(s,a) \;=\; \max_{\pi}\; q_{\pi}(s,a).
\]</div>
<p>If, for some state <span class="arithmatex">\(s\)</span> and two distinct actions <span class="arithmatex">\(a_1\neq a_2\)</span>, it holds that </p>
<div class="arithmatex">\[
q_{*}(s,a_1) \;=\; q_{*}(s,a_2),
\]</div>
<p>then both actions <span class="arithmatex">\(a_1\)</span> and <span class="arithmatex">\(a_2\)</span> are equally good in state <span class="arithmatex">\(s\)</span>. Hence, any policy choosing either action in that state is still optimal. If multiple such ties occur across different states, this can lead to many distinct policies <span class="arithmatex">\(\ \pi^*\)</span> that all achieve the same maximum returns.</p>
<p>4) Summary</p>
<p>• A finite discounted MDP with <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span> and bounded rewards always has at least one optimal policy.<br />
• There can be more than one optimal policy; uniqueness is not guaranteed.<br />
• All optimal policies share the same optimal state-value function <span class="arithmatex">\(v_{*}(s)\)</span> (and the same optimal action-value function <span class="arithmatex">\(q_{*}(s,a)\)</span>).</p>
<h2 id="bellman-optimality-operator-i">Bellman Optimality Operator I<a class="headerlink" href="#bellman-optimality-operator-i" title="Permanent link">&para;</a></h2>
<p>The value function <span class="arithmatex">\(v_{\pi}(s)\)</span> is a function, <span class="arithmatex">\(v_{\pi}(\cdot): \mathcal{S} \to \mathbb{R}\)</span>. In our MDP setting, this space is a space of bounded functions. Previously, we derived the Bellman Optimality Equation as:</p>
<div class="arithmatex">\[
\boxed{
v_*(s)
\;=\;
\max_{a \,\in\, \mathcal{A}}\;
\sum_{s' \,\in\, \mathcal{S}}
P(s' \mid s,a)\;
\bigl[
  R(s,a,s')
  \;+\;\gamma\,v_*(s')
\bigr].
}
\]</div>
<p>Clarify how this results in the Bellman optimality operator <span class="arithmatex">\([\mathcal{T} v](\cdot)\)</span>. This operator acts on the space of bounded state value functions to what space? Explain this in a rigorous manner.</p>
<p><strong><em>Answer</em></strong></p>
<p>Below is a step-by-step explanation of how the Bellman Optimality Equation induces an operator (the Bellman Optimality Operator) on the space of bounded value functions. All mathematical symbols and expressions appear in standard LaTeX:</p>
<p>1) <strong>Finite MDP and Space of Bounded Value Functions</strong></p>
<p>• Let the state space be finite: <span class="arithmatex">\(\mathcal{S} = \{\,s_{1},s_{2},\dots,s_{|\mathcal{S}|}\}\)</span>.<br />
• Consider the space of bounded real-valued functions on <span class="arithmatex">\(\mathcal{S}\)</span>, denoted by <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span>.<br />
Each element <span class="arithmatex">\(v \in \mathcal{B}(\mathcal{S})\)</span> is a function </p>
<div class="arithmatex">\[
v: \;\mathcal{S}\;\to\;\mathbb{R} \quad  \text{with}  \quad
  \|\,v\,\|_{\infty}   \;=\;
  \sup_{s \in \mathcal{S}}\;\bigl|\,v(s)\bigr|
  \;&lt;\;\infty.
\]</div>
<p>• Because <span class="arithmatex">\(\mathcal{S}\)</span> is finite, any real-valued function on <span class="arithmatex">\(\mathcal{S}\)</span> is automatically bounded if its values are finite.</p>
<p>2) <strong>Bellman Optimality Equation</strong></p>
<p>The Bellman Optimality Equation for the optimal value function <span class="arithmatex">\(v_{*}\)</span> is:</p>
<div class="arithmatex">\[
v_{*}(s)
\;=\;
\max_{a \,\in\, \mathcal{A}}
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \,\vert\, s,a\bigr)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v_{*}(s')
\Bigr],
\]</div>
<p>where<br />
• <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span> is the discount factor,<br />
• <span class="arithmatex">\(R(s,a,s')\)</span> is the reward for transitioning <span class="arithmatex">\(s \to s'\)</span> via action <span class="arithmatex">\(a\)</span>, and<br />
• <span class="arithmatex">\(P(s' \mid s,a)\)</span> is the transition probability.</p>
<p>3) <strong>Defining the Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}\)</span></strong></p>
<p>We now define an operator <span class="arithmatex">\(\mathcal{T}\)</span> that acts on any bounded value function <span class="arithmatex">\(v \in \mathcal{B}(\mathcal{S})\)</span>. For each state <span class="arithmatex">\(s\in \mathcal{S}\)</span>, set</p>
<div class="arithmatex">\[
[\,\mathcal{T}v\,](s)
\;=\;
\max_{a \,\in\, \mathcal{A}}
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \,\vert\, s,a\bigr)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v(s')
\Bigr].
\]</div>
<p>Thus <span class="arithmatex">\(\mathcal{T}\)</span> takes the function <span class="arithmatex">\(v\)</span> and produces a new function <span class="arithmatex">\(\mathcal{T}v\)</span>. Concretely,</p>
<div class="arithmatex">\[
\mathcal{T}:\;\mathcal{B}(\mathcal{S})\;\to\;\mathcal{B}(\mathcal{S}), 
\quad
v \;\mapsto\;\mathcal{T}v.
\]</div>
<p>4) <strong><span class="arithmatex">\(\mathcal{T}v\)</span> Is Bounded</strong></p>
<p>• Suppose <span class="arithmatex">\(v \in \mathcal{B}(\mathcal{S})\)</span>, meaning that <span class="arithmatex">\(\|\,v\,\|_{\infty} = \sup_{s\in\mathcal{S}}|v(s)| &lt; \infty\)</span>.<br />
• Assume rewards are bounded: <span class="arithmatex">\(\bigl|\,R(s,a,s')\bigr|\le R_{\max}\)</span> for all <span class="arithmatex">\((s,a,s')\)</span>.<br />
• For each <span class="arithmatex">\(s\in \mathcal{S}\)</span>, the quantity <span class="arithmatex">\([\mathcal{T}v](s)\)</span> is a finite maximum (since <span class="arithmatex">\(\mathcal{A}\)</span> is finite) of finite sums of probabilities times <span class="arithmatex">\(\bigl(R(s,a,s') + \gamma\,v(s')\bigr)\)</span>.<br />
• Hence <span class="arithmatex">\([\mathcal{T}v](s)\)</span> is finite, and collectively <span class="arithmatex">\(\|\mathcal{T}v\|_{\infty} &lt; \infty\)</span>.  </p>
<p>Therefore, <span class="arithmatex">\(\mathcal{T}v\)</span> lies in the same space <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span>. </p>
<p>5) <strong>How <span class="arithmatex">\(\mathcal{T}\)</span> Relates to the Bellman Optimality Equation</strong></p>
<p>The Bellman Optimality Equation says that the optimal value function <span class="arithmatex">\(v_{*}\)</span> must satisfy</p>
<div class="arithmatex">\[
v_{*}(s)
\;=\;
[\mathcal{T}v_{*}](s)
\quad
\text{for each }s.
\]</div>
<p>In other words, <span class="arithmatex">\(v_{*}\)</span> is a fixed point of the operator <span class="arithmatex">\(\mathcal{T}\)</span>. Symbolically,</p>
<div class="arithmatex">\[
\mathcal{T}v_{*} = v_{*}.
\]</div>
<p>Moreover, in a discounted, finite MDP with bounded rewards, one can show (via a contraction mapping argument in the supremum norm) that <span class="arithmatex">\(\mathcal{T}\)</span> has a unique fixed point in <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span>, namely <span class="arithmatex">\(v_{*}\)</span>.</p>
<p>6) <strong>Conclusion</strong></p>
<p>• The space of bounded value functions on a finite state space is <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span>.<br />
• The Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}\)</span> transforms any <span class="arithmatex">\(v\in\mathcal{B}(\mathcal{S})\)</span> into another bounded function <span class="arithmatex">\(\mathcal{T}v\in\mathcal{B}(\mathcal{S})\)</span> via</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) =
\max_{a}
\sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\,\Bigl[
  R(s,a,s') + \gamma\,v(s')
\Bigr].
\]</div>
<p>• The Bellman Optimality Equation asserts that the unique optimal value function <span class="arithmatex">\(v_{*}\)</span> is a fixed point of <span class="arithmatex">\(\mathcal{T}\)</span>.  </p>
<p>Hence, in rigorous terms, <span class="arithmatex">\(\mathcal{T}\)</span> is well-defined as an operator (<span class="arithmatex">\(\mathcal{T}:\mathcal{B}(\mathcal{S})\to \mathcal{B}(\mathcal{S})\)</span>) that captures “take the maximum over actions of expected reward plus discounted value,” exactly as described by the Bellman Optimality Equation.</p>
<h2 id="bellman-optimality-operator-ii">Bellman Optimality Operator II<a class="headerlink" href="#bellman-optimality-operator-ii" title="Permanent link">&para;</a></h2>
<p>This makes sense. I need clarification on one point in your answer. The Bellman Optimality operator is defined as</p>
<div class="arithmatex">\[
[\,\mathcal{T}v\,](s)
\;=\;
\max_{a \,\in\, \mathcal{A}}
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \,\vert\, s,a\bigr)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v(s')
\Bigr].
\]</div>
<p>I need a more detailed explanation to see why the following is true.</p>
<div class="arithmatex">\[
\mathcal{T}:\;\mathcal{B}(\mathcal{S})\;\to\;\mathcal{B}(\mathcal{S}), 
\quad
v \;\mapsto\;\mathcal{T}v.
\]</div>
<p><strong><em>Answer</em></strong></p>
<p>Below is a step-by-step clarification of why the Bellman Optimality Operator, often denoted by <span class="arithmatex">\(\mathcal{T}\)</span>, is defined as:</p>
<div class="arithmatex">\[
[\mathcal{T}v](s)
\;=\;
\max_{a \,\in\, \mathcal{A}}
\;\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s' \,\vert\, s,a\bigr)\,
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v(s')
\Bigr],
\]</div>
<p>and how it acts from the space of bounded value functions <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span> to itself.</p>
<p>1) <strong>Context: Value Functions and Optimality</strong></p>
<p>• In a Markov Decision Process (MDP), the state-value function for a policy <span class="arithmatex">\(\pi\)</span>, denoted <span class="arithmatex">\(v_{\pi}(s)\)</span>, embodies the expected return (sum of discounted rewards) starting from state <span class="arithmatex">\(s\)</span>.<br />
• The “optimal” value function <span class="arithmatex">\(v_{*}\)</span> solves the Bellman Optimality Equation:</p>
<div class="arithmatex">\[
  v_{*}(s)  \;=\;   \max_{a \in \mathcal{A}}   \sum_{s' \in \mathcal{S}}
  P(s' \mid s,a)\,\Bigl[R(s,a,s') + \gamma\,v_{*}(s')\Bigr].
\]</div>
<p>• This essentially says: from state <span class="arithmatex">\(s\)</span>, pick the action <span class="arithmatex">\(a\)</span> that maximizes the immediate expected reward plus the discounted future.  </p>
<p>2) <strong>One-Step Lookahead + Max</strong></p>
<p>• Observe that the right-hand side of the above equation is a function of <span class="arithmatex">\(v_{*}\)</span>.<br />
• If we do not yet know <span class="arithmatex">\(v_{*}\)</span>, but have some candidate bounded value function <span class="arithmatex">\(v\)</span>, we can \emph{approximate} what it means to “pick the best action based on <span class="arithmatex">\(v\)</span>,” by replacing <span class="arithmatex">\(v_{*}(s')\)</span> with <span class="arithmatex">\(v(s')\)</span>.<br />
• Consequently, to get a new function <span class="arithmatex">\(\bigl[\mathcal{T}v\bigr]\)</span>(s), we do exactly</p>
<div class="arithmatex">\[
\max_{a \in \mathcal{A}}
\sum_{s' \in \mathcal{S}}
P(s' \mid s,a)\,
\Bigl[
  R(s,a,s') + \gamma\,v(s')
\Bigr].
\]</div>
<p>This procedure is “look one step ahead, then add discounted future as given by <span class="arithmatex">\(v\)</span>, and choose the action yielding the maximal value.”</p>
<p>3) <strong>Definition of <span class="arithmatex">\(\mathcal{T}\)</span>: Acting on Bounded Value Functions</strong></p>
<p>Let <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span> be the vector space of bounded real-valued functions over a finite state set <span class="arithmatex">\(\mathcal{S}\)</span>. Formally,</p>
<div class="arithmatex">\[
v \in \mathcal{B}(\mathcal{S}) \quad\Longleftrightarrow\quad v: \mathcal{S} \to \mathbb{R},
\;\text{with}\;
\|v\|_{\infty} = \sup_{s \in \mathcal{S}}
\bigl|v(s)\bigr|
&lt;
\infty.
\]</div>
<p>We define the Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}:\mathcal{B}(\mathcal{S})\to \mathcal{B}(\mathcal{S})\)</span> by specifying its action on each <span class="arithmatex">\(v\in\mathcal{B}(\mathcal{S})\)</span> and each state <span class="arithmatex">\(s\in\mathcal{S}\)</span>:</p>
<div class="arithmatex">\[
\bigl[\mathcal{T}v\bigr](s)
\;=\;
\max_{a \,\in\, \mathcal{A}}
\sum_{s' \,\in\, \mathcal{S}}
P\bigl(s'\mid s,a\bigr)\;
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v(s')
\Bigr].
\]</div>
<p>4) <strong><span class="arithmatex">\(\mathcal{T}v\)</span> Remains Bounded</strong></p>
<p>• Because <span class="arithmatex">\(\|v\|_{\infty}\)</span> is finite and the immediate rewards <span class="arithmatex">\(R(s,a,s')\)</span> are assumed bounded, each summation over <span class="arithmatex">\(s'\)</span> is finite.<br />
• There are finitely many actions <span class="arithmatex">\(a\)</span>, so the <span class="arithmatex">\(\max_{a \in \mathcal{A}}\)</span> expression is finite as well.<br />
• Hence <span class="arithmatex">\(\|\,\mathcal{T}v\,\|_{\infty} &lt; \infty\)</span>.<br />
• In other words, <span class="arithmatex">\(\mathcal{T}v\)</span> is itself in <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span>. Thus <span class="arithmatex">\(\mathcal{T}\)</span> indeed maps bounded functions to bounded functions:</p>
<div class="arithmatex">\[
\mathcal{T}:\;\mathcal{B}(\mathcal{S})
\;\to\;
\mathcal{B}(\mathcal{S}).
\]</div>
<p>5) <strong>Connection to the Bellman Optimality Equation</strong></p>
<p>• The Bellman Optimality Equation states that <span class="arithmatex">\(v_{*}\)</span> is a fixed point of <span class="arithmatex">\(\mathcal{T}\)</span>, i.e.,</p>
<div class="arithmatex">\[
v_{*}(s)
\;=\;
[\mathcal{T}v_{*}](s)
\quad\text{for all }s.
\]</div>
<p>• In short, if you plug <span class="arithmatex">\(v_{*}\)</span> into <span class="arithmatex">\(\mathcal{T}\)</span>, you get the same function <span class="arithmatex">\(v_{*}\)</span> back.<br />
• Under standard assumptions (finite <span class="arithmatex">\(\mathcal{S}, \mathcal{A}\)</span>, <span class="arithmatex">\(\gamma &lt; 1\)</span>, bounded rewards), this fixed point is unique, and repeated application of <span class="arithmatex">\(\mathcal{T}\)</span> (starting from \emph{any} bounded function <span class="arithmatex">\(v^{(0)}\)</span>) converges to <span class="arithmatex">\(v_{*}\)</span> in the supremum norm.</p>
<p>6) <strong>Summary</strong></p>
<p>Given a bounded function <span class="arithmatex">\(v\)</span>, <span class="arithmatex">\(\bigl[\mathcal{T}v\bigr](s)\)</span> is obtained by taking the one-step expected reward (averaging over next states <span class="arithmatex">\(s'\)</span> with probabilities <span class="arithmatex">\(P(s'\mid s,a)\)</span>), adding the discounted future value <span class="arithmatex">\(\gamma\,v(s')\)</span>, and finally choosing the action <span class="arithmatex">\(a\)</span> that maximizes this sum. Mathematically,</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) = \max_{a} \sum_{s'}
P(s'\mid s,a)
\bigl[
  R(s,a,s')
  +
  \gamma\,v(s')
\bigr].
\]</div>
<p>Hence <span class="arithmatex">\(\mathcal{T}\)</span> is exactly the <span class="arithmatex">\(\textit{optimality operator}\)</span> that, when iterated, drives a value function <span class="arithmatex">\(v\)</span> closer and closer to the unique optimal value function <span class="arithmatex">\(v_{*}\)</span>.</p>
<h2 id="uniqueness-revisited">Uniqueness revisited<a class="headerlink" href="#uniqueness-revisited" title="Permanent link">&para;</a></h2>
<p><strong><em>Provide a more mathematically rigorous explanation for the uniqueness question. It appears that optimal policies are not unique but the state value functions are.</em></strong></p>
<p>Below is a mathematically more rigorous explanation regarding the uniqueness or non‐uniqueness of optimal policies versus the uniqueness of the optimal value function in a finite, discounted Markov Decision Process (MDP) with bounded rewards.</p>
<p>1) <strong>Setting: Discounted Finite MDPs</strong></p>
<p>Consider an MDP with:</p>
<p>• A finite state space:    <span class="arithmatex">\(\mathcal{S} = \{\,s_1,\dots,s_{|\mathcal{S}|}\}\)</span>.<br />
• A finite action space:    <span class="arithmatex">\(\mathcal{A} = \{\,a_1,\dots,a_{|\mathcal{A}|}\}\)</span>.<br />
• A transition kernel:    <span class="arithmatex">\(P(s' \mid s,a)\)</span>.<br />
• A bounded reward function:    <span class="arithmatex">\(\lvert R(s,a,s')\rvert \le R_{\max}\)</span>.<br />
• A discount factor:    <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span>.</p>
<p>Define the return from time <span class="arithmatex">\(t\)</span> as</p>
<div class="arithmatex">\[
G_t \;=\;\sum_{k=0}^{\infty}\,\gamma^k\,R_{t+k+1}.
\]</div>
<p>A policy <span class="arithmatex">\(\pi\)</span> determines the probabilities of selecting actions in each state. Then</p>
<div class="arithmatex">\[
v_{\pi}(s)
\;=\;
\mathbb{E}\Bigl[G_t \,\big\vert\, S_t=s,\;\pi\Bigr]
\quad\text{and}\quad
q_{\pi}(s,a)
\;=\;
\mathbb{E}\Bigl[G_t \,\big\vert\, S_t=s,\;A_t=a,\;\pi\Bigr].
\]</div>
<p>We define the optimal state- and action-value functions:</p>
<div class="arithmatex">\[
v_{*}(s) 
\;=\;
\max_{\pi} \,v_{\pi}(s),
\quad
q_{*}(s,a)
\;=\;
\max_{\pi} \,q_{\pi}(s,a).
\]</div>
<p>2) <strong>Uniqueness of the Optimal Value Function</strong></p>
<p>1 Banach Fixed Point / Contraction Mapping Argument:</p>
<p>The Bellman optimality operator <span class="arithmatex">\(\mathcal{T}\)</span>, acting on a value function <span class="arithmatex">\(v\)</span>, is defined by</p>
<div class="arithmatex">\[
\mathcal{T}v(s)    \;=\;   \max_{a\in\mathcal{A}}   \sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\,\Bigl[R(s,a,s') + \gamma \,v(s')\Bigr].
\]</div>
<p>One can show <span class="arithmatex">\(\mathcal{T}\)</span> is a <span class="arithmatex">\(\gamma\)</span>-contraction in the supremum norm <span class="arithmatex">\(\|\cdot\|_{\infty}\)</span> over the space of bounded functions from <span class="arithmatex">\(\mathcal{S}\)</span> to <span class="arithmatex">\(\mathbb{R}\)</span>. By the Banach Fixed Point Theorem, there exists a unique fixed point <span class="arithmatex">\(v_{*}\)</span> of <span class="arithmatex">\(\mathcal{T}\)</span> such that</p>
<div class="arithmatex">\[
   \mathcal{T}v_{*} = v_{*}.
\]</div>
<p>Moreover, for any initial bounded value function <span class="arithmatex">\(v^{(0)}\)</span>, iterating</p>
<div class="arithmatex">\[
   v^{(k+1)} = \mathcal{T}v^{(k)}
\]</div>
<p>converges to <span class="arithmatex">\(v_{*}\)</span> in <span class="arithmatex">\(\|\cdot\|_{\infty}\)</span>.  </p>
<p>2 Equivalence to <span class="arithmatex">\(v_{*}(s)\)</span>:</p>
<p>Because</p>
<div class="arithmatex">\[
   v_{*}(s)
   \;=\;
   \max_{\pi}\,v_{\pi}(s),
\]</div>
<p>and each <span class="arithmatex">\(v_{\pi}\)</span> is a fixed point of the policy-evaluation operator with respect to <span class="arithmatex">\(\pi\)</span>, it follows that <span class="arithmatex">\(v_{*}\)</span> is exactly the unique fixed point of <span class="arithmatex">\(\mathcal{T}\)</span>. Conclusively, the \textit{function} <span class="arithmatex">\(v_{*}\)</span> is unique among all bounded functions.</p>
<p>3) <strong>Why Multiple Optimal Policies Can Exist</strong></p>
<p>1 Ties in the Maximization:</p>
<p>Even though the operator <span class="arithmatex">\(\mathcal{T}v\)</span>(s) uses a “<span class="arithmatex">\(\max_{a}\)</span>” over actions, it is possible for multiple actions <span class="arithmatex">\(a\)</span> to \textit{achieve} that maximum in a given state <span class="arithmatex">\(s\)</span>. Concretely, if</p>
<div class="arithmatex">\[
   \sum_{s'}P(s'\mid s,a_{1})
   \bigl[R(s,a_{1},s') + \gamma\,v_{*}(s')\bigr]
   \;=\;
   \sum_{s'}P(s'\mid s,a_{2})
   \bigl[R(s,a_{2},s') + \gamma\,v_{*}(s')\bigr],
\]</div>
<p>then both <span class="arithmatex">\(a_{1}\)</span> and <span class="arithmatex">\(a_{2}\)</span> give the same (optimal) value in state <span class="arithmatex">\(s\)</span>. This means, at state <span class="arithmatex">\(s\)</span>, you may choose either <span class="arithmatex">\(a_{1}\)</span> or <span class="arithmatex">\(a_{2}\)</span> (or any distribution over them) and still achieve the same optimal outcome.</p>
<p>2 Constructing Distinct Optimal Policies:</p>
<p>If there is at least one state <span class="arithmatex">\(s\)</span> where two or more actions tie for the maximum in the Bellman optimality equation, then we can define multiple deterministic policies by picking different “best” actions in that state—and possibly more states have such ties. Each of these policies achieves</p>
<div class="arithmatex">\[
   v_{\pi^{*}}(s) \;=\; v_{*}(s)
   \quad
   \forall\,s.
\]</div>
<p>Consequently, the <span class="arithmatex">\(\textit{policy}\)</span> is not unique.</p>
<p>4) <strong>Same <span class="arithmatex">\(v_{*}\)</span>, Different <span class="arithmatex">\(\pi^{*}\)</span>:</strong></p>
<p>Because all such policies pick (in each state <span class="arithmatex">\(s\)</span>) <span class="arithmatex">\(\textit{some}\)</span> action that maximizes the quantity <span class="arithmatex">\(\sum_{s'}P(s'\mid s,a)[\,R + \gamma\,v_{*}\,]\)</span>, the resulting value function cannot exceed <span class="arithmatex">\(v_{*}(s)\)</span>. On the other hand, it cannot be less either, since that is the maximum. Hence each of these “tie-breaking” policies yields the same <span class="arithmatex">\(v_{*}\)</span>. This is exactly how multiple optimal policies arise, while the optimal value function remains unique.</p>
<p>5) <strong>Conclusion</strong></p>
<p>• In a finite discounted MDP, the Bellman optimality operator is a strict contraction (given <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span>), ensuring the \textbf{optimal value function} <span class="arithmatex">\(v_{*}\)</span> is \textbf{unique}.</p>
<p>• However, <span class="arithmatex">\(\textit{optimal policies}\)</span>—those that achieve <span class="arithmatex">\(v_{*}(s)\)</span> for all <span class="arithmatex">\(s\)</span>—can be <span class="arithmatex">\(\textbf{non‐unique}\)</span>, precisely when multiple actions tie in the state-wise maximization. All such policies still yield the same unique <span class="arithmatex">\(v_{*}\)</span>.</p>
<p>Hence, the uniqueness conferred by the contraction mapping theorem refers to the <span class="arithmatex">\(\textbf{value function}\)</span>, not necessarily the <span class="arithmatex">\(\textbf{policy}\)</span>, explaining why there can be multiple distinct optimal policies yet only a single optimal value function.</p>
<h2 id="mathcalt-is-a-contraction-mapping"><span class="arithmatex">\(\mathcal{T}\)</span> is a contraction mapping<a class="headerlink" href="#mathcalt-is-a-contraction-mapping" title="Permanent link">&para;</a></h2>
<p><strong><em>Prove in a mathematically rigorous manner that <span class="arithmatex">\(\mathcal{T}\)</span> is a <span class="arithmatex">\(\gamma\)</span>-contraction in the supremum norm <span class="arithmatex">\(\|\cdot\|_{\infty}\)</span>.</em></strong></p>
<p>Below is a step-by-step proof that the Bellman Optimality Operator,  </p>
<div class="arithmatex">\[
[\mathcal{T}v](s)
\;=\;
\max_{a\,\in\,\mathcal{A}}
\sum_{s'\,\in\,\mathcal{S}}
P(s'\mid s,a)\,
\Bigl[
  R(s,a,s')
  \;+\;\gamma\,v(s')
\Bigr],
\]</div>
<p>is a <span class="arithmatex">\(\gamma\)</span>-contraction mapping in the supremum (∞-)norm. That is, we prove:</p>
<div class="arithmatex">\[
\|\mathcal{T}v \;-\;\mathcal{T}w\|_{\infty}
\;\le\;
\gamma\;\|v \;-\; w\|_{\infty}
\quad\text{for all}\;\;v,w\in\mathcal{B}(\mathcal{S}).
\]</div>
<p>Here, <span class="arithmatex">\(\|\,f\,\|_{\infty} = \sup_{s\in\mathcal{S}} |f(s)|\)</span>, and <span class="arithmatex">\(\mathcal{B}(\mathcal{S})\)</span> is the space of all bounded functions on the finite state space <span class="arithmatex">\(\mathcal{S}\)</span>.</p>
<p>1 <strong>The Operator <span class="arithmatex">\(\mathcal{T}\)</span> in Detail</strong></p>
<p>Given any bounded function <span class="arithmatex">\(v:\mathcal{S}\to\mathbb{R}\)</span>, we define</p>
<div class="arithmatex">\[
[\mathcal{T}v](s)
\;=\;
\max_{a\,\in\,\mathcal{A}}
\sum_{s'\,\in\,\mathcal{S}}
P(s'\,\mid\,s,a)
\Bigl[
  R(s,a,s')
  \;+\;
  \gamma\,v(s')
\Bigr].
\]</div>
<p>Such an operator <span class="arithmatex">\(\mathcal{T}\)</span> is called the Bellman Optimality Operator for a discounted MDP with discount factor <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span>. Our goal is to show</p>
<div class="arithmatex">\[
\|\mathcal{T}v - \mathcal{T}w\|_{\infty}
\;\le\;
\gamma\;\|\,v-w\,\|_{\infty}
\quad
\forall\,
v,w \in \mathcal{B}(\mathcal{S}).
\]</div>
<p>2 <strong>Examine <span class="arithmatex">\([\mathcal{T}v](s)\)</span> Minus <span class="arithmatex">\([\mathcal{T}w](s)\)</span></strong></p>
<p>Fix any state <span class="arithmatex">\(s\in\mathcal{S}\)</span>. We have:</p>
<div class="arithmatex">\[
[\mathcal{T}v](s)
\;=\;
\max_{a\in\mathcal{A}}
\sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\;
\Bigl[
  R(s,a,s') + \gamma\,v(s')
\Bigr],
\]</div>
<div class="arithmatex">\[
[\mathcal{T}w](s)
\;=\;
\max_{a\in\mathcal{A}}
\sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\;
\Bigl[
  R(s,a,s') + \gamma\,w(s')
\Bigr].
\]</div>
<p>Denote</p>
<div class="arithmatex">\[
F_{v}(s,a)
\;=\;
\sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\,[\,R(s,a,s') + \gamma\,v(s')\,],
\]</div>
<p>so that <span class="arithmatex">\([\mathcal{T}v](s) = \max_{a\in\mathcal{A}} F_{v}(s,a)\)</span>. Similarly, <span class="arithmatex">\([\mathcal{T}w](s) = \max_{a\in\mathcal{A}} F_{w}(s,a)\)</span>. Then</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) - [\mathcal{T}w](s)
\;=\;
\max_{a\in\mathcal{A}}\,F_{v}(s,a)
\;-\;
\max_{a\in\mathcal{A}}\,F_{w}(s,a).
\]</div>
<p>3 <strong>Bounding the Difference Using “max - max ≤ max of differences”</strong></p>
<p>Recall a known inequality for real numbers: <span class="arithmatex">\(\max_{x} f(x) \;-\;\max_{x} g(x) \;\le\;\max_{x}\,[f(x) - g(x)]\)</span>.  </p>
<p>Hence,</p>
<div class="arithmatex">\[
\max_{a}\,F_{v}(s,a)
\;-\;
\max_{a}\,F_{w}(s,a)
\;\le\;
\max_{a}\,\bigl[F_{v}(s,a) - F_{w}(s,a)\bigr].
\]</div>
<p>Therefore,</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) - [\mathcal{T}w](s)
\;\le\;
\max_{a\in\mathcal{A}}
\Bigl\{
  \sum_{s'\in\mathcal{S}}
  P(s'\mid s,a)\,
  \bigl[\,\gamma\,v(s') - \gamma\,w(s')\,\bigr]
\Bigr\},
\]</div>
<p>since the reward part <span class="arithmatex">\(R(s,a,s')\)</span> cancels out (it appears in both <span class="arithmatex">\(F_{v}\)</span> and <span class="arithmatex">\(F_{w}\)</span>):</p>
<div class="arithmatex">\[
F_{v}(s,a) - F_{w}(s,a) =
\sum_{s'}P(s'\mid s,a)\,
\Bigl[
  \gamma\,v(s') - \gamma\,w(s')
\Bigr].
\]</div>
<p>4 <strong>Factor Out <span class="arithmatex">\(\gamma\)</span> and Sum of Probabilities</strong></p>
<p>Thus</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) - [\mathcal{T}w](s)
\;\le\;
\max_{a}\Bigl\{
  \gamma
  \sum_{s'}
  P(s'\mid s,a)\,\bigl[v(s') - w(s')\bigr]
\Bigr\}.
\]</div>
<p>Observing that <span class="arithmatex">\(\sum_{s'} P(s'\mid s,a) = 1\)</span>, we get</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) - [\mathcal{T}w](s)
\;\le\;
\gamma
\max_{a}
\sum_{s'\in\mathcal{S}}
P(s'\mid s,a)\,\bigl[v(s') - w(s')\bigr].
\]</div>
<p>Now, for each s′, the difference <span class="arithmatex">\(v(s') - w(s')\)</span> satisfies:</p>
<div class="arithmatex">\[
v(s') - w(s')
\;\le\;
\|v-w\|_{\infty}.
\]</div>
<p>Hence,</p>
<div class="arithmatex">\[
\sum_{s'}
P(s'\mid s,a)\,\bigl[v(s') - w(s')\bigr]
\;\le\;
\sum_{s'} P(s'\mid s,a)\,\|v-w\|_{\infty}
\;=\;
\|\,v-w\,\|_{\infty}
\]</div>
<p>(because <span class="arithmatex">\(\sum_{s'}P(s'\mid s,a) = 1\)</span>). Therefore,</p>
<div class="arithmatex">\[
\max_{a}
\sum_{s'}
P(s'\mid s,a)\,\bigl[v(s') - w(s')\bigr]
\;\le\;
\|v-w\|_{\infty}.
\]</div>
<p>Hence</p>
<div class="arithmatex">\[
[\mathcal{T}v](s) - [\mathcal{T}w](s)
\;\le\;
\gamma\;\|v-w\|_{\infty}.
\]</div>
<p>5 <strong>Symmetry: The Same Bound with (v,w) Swapped</strong></p>
<p>We also need a “lower bound” so we can handle absolute values. By a symmetric argument, swapping the roles of v and w,</p>
<div class="arithmatex">\[
[\mathcal{T}w](s) - [\mathcal{T}v](s)
\;\le\;
\gamma\;\|v-w\|_{\infty}.
\]</div>
<p>6 <strong>Conclude the Contraction Property</strong></p>
<p>Combining both, we see that</p>
<div class="arithmatex">\[
|[\mathcal{T}v](s) \;-\; [\mathcal{T}w](s)|
\;\le\;
\gamma\;\|v-w\|_{\infty}
\quad
\text{for each state }s.
\]</div>
<p>Hence taking the supremum over <span class="arithmatex">\(s \in \mathcal{S}\)</span> yields:</p>
<div class="arithmatex">\[
\|\mathcal{T}v \;-\; \mathcal{T}w\|_{\infty} =
\sup_{s\in\mathcal{S}}
\,\bigl|
  [\mathcal{T}v](s)
  \;-\;
  [\mathcal{T}w](s)
\bigr|
\;\le\;
\gamma\;\|v-w\|_{\infty}.
\]</div>
<p>This is precisely the definition of a <span class="arithmatex">\(\gamma\)</span>-contraction in the ∞-norm.</p>
<p>7 <strong>Summary</strong></p>
<p>Thus, the Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}\)</span> satisfies</p>
<div class="arithmatex">\[
\|\mathcal{T}v \;-\; \mathcal{T}w\|_{\infty}
\;\le\;
\gamma\;\|v-w\|_{\infty},
\]</div>
<p>showing it is a <span class="arithmatex">\(\gamma\)</span>-contraction mapping in the supremum norm. This property underpins why <span class="arithmatex">\(\mathcal{T}\)</span> has a unique fixed point (the optimal value function <span class="arithmatex">\(v_{*}\)</span>) in the Banach space <span class="arithmatex">\(\bigl(\mathcal{B}(\mathcal{S}),\|\cdot\|_{\infty}\bigr)\)</span> and why repeated iteration of <span class="arithmatex">\(\mathcal{T}\)</span> converges to <span class="arithmatex">\(v_{*}\)</span>.</p>
<h2 id="bellman-optimality-operator-matrix-vector-form">Bellman Optimality Operator - matrix vector form<a class="headerlink" href="#bellman-optimality-operator-matrix-vector-form" title="Permanent link">&para;</a></h2>
<p><strong><em>Just as a Bellman equation for a policy <span class="arithmatex">\(\pi\)</span> can be written in matrix-vector form, similarly, the Bellman optimality equation can also be written in matrix-vector form. Starting with the scalar form for the Bellman optimality equation for each state <span class="arithmatex">\(s_i \in \mathcal{S}\)</span>, write down the matrix-vector form while clarifying the entries of the associated vectors and matrix. Show the connection of the Bellman Optimality operator <span class="arithmatex">\(\mathcal{T}\)</span> to the matrix-vector form.  Write down all mathematical symbols and expressions in full Latex.</em></strong></p>
<p>Below is a step‐by‐step derivation showing how the Bellman Optimality Equation, which is initially given in a scalar “per‐state” form, can also be written in a matrix‐vector form. All mathematical symbols and expressions are in LaTeX.</p>
<p>1 <strong>Scalar (Per‐State) Form of the Bellman Optimality Equation</strong></p>
<p>Consider a finite Markov Decision Process (MDP) with:</p>
<p>• A finite state space <span class="arithmatex">\(\mathcal{S} = \{s_{1},\,s_{2},\dots,s_{|\mathcal{S}|}\}\)</span>.<br />
• A finite action space <span class="arithmatex">\(\mathcal{A} = \{a_{1},\,a_{2},\dots,a_{|\mathcal{A}|}\}\)</span>.<br />
• Transition probabilities <span class="arithmatex">\(P(s_{j}\mid s_{i},a)\)</span>.<br />
• A discount factor <span class="arithmatex">\(0 \le \gamma &lt; 1\)</span>.<br />
• A bounded reward function <span class="arithmatex">\(R(s_{i},\,a,\,s_{j})\)</span>.</p>
<p>The Bellman Optimality Equation states that the optimal value function <span class="arithmatex">\(v_{*}\)</span> satisfies, for each state <span class="arithmatex">\(s_{i}\in \mathcal{S}\)</span>,</p>
<div class="arithmatex">\[
v_{*}(s_{i})
\;=\;
\max_{a\,\in\,\mathcal{A}}
\sum_{s_{j}\,\in\, \mathcal{S}}
P\bigl(s_{j}\,\vert\,s_{i},\,a\bigr)
\Bigl[
  R\bigl(s_{i},\,a,\,s_{j}\bigr)
  \;+\;
  \gamma\,v_{*}(s_{j})
\Bigr].
\]</div>
<p>2 <strong>Defining Matrices and Vectors for Each Action</strong></p>
<p>To move toward a matrix‐vector representation, we introduce:</p>
<ol>
<li>A transition matrix <span class="arithmatex">\(\mathbf{P}_{a} \in \mathbb{R}^{|\mathcal{S}|\times |\mathcal{S}|}\)</span> associated with each action <span class="arithmatex">\(a\)</span>. Its entries are</li>
</ol>
<div class="arithmatex">\[
   [\mathbf{P}_{a}]_{i,j}  \;=\;
   P\bigl(s_{j}\,\vert\,s_{i},\,a\bigr).
\]</div>
<ol>
<li>A reward vector <span class="arithmatex">\(\mathbf{r}_{a} \in \mathbb{R}^{|\mathcal{S}|}\)</span> associated with each action <span class="arithmatex">\(a\)</span>. Its <span class="arithmatex">\(i\)</span>-th entry captures the expected immediate reward upon taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s_{i}\)</span>:</li>
</ol>
<div class="arithmatex">\[
   [\mathbf{r}_{a}]_{\,i}    \;=\;
   \sum_{s_{j}\,\in\,\mathcal{S}}
   P\bigl(s_{j}\,\vert\,s_{i},\,a\bigr)\;
   R\bigl(s_{i},\,a,\,s_{j}\bigr).
\]</div>
<p>Equivalently, one can keep the form <span class="arithmatex">\(\sum_{s_{j}}P(s_{j}\mid s_{i},a)\,R(\dots)\)</span>, or define a suitable immediate‐reward function that depends on <span class="arithmatex">\(\bigl(s_{i},a,s_{j}\bigr)\)</span>. Either way the end result is a vector of length <span class="arithmatex">\(|\mathcal{S}|\)</span>.</p>
<p>Hence for each <span class="arithmatex">\(a \in \mathcal{A}\)</span>, we have:</p>
<p>• <span class="arithmatex">\(\mathbf{P}_{a}\,v\)</span> is a vector where the <span class="arithmatex">\(i\)</span>-th component is <span class="arithmatex">\(\sum_{s_{j}} P(s_{j}\mid s_{i},a)\,v(s_{j})\)</span>.<br />
• <span class="arithmatex">\(\mathbf{r}_{a}\)</span> is a vector whose <span class="arithmatex">\(i\)</span>-th entry accounts for <span class="arithmatex">\(\sum_{s_{j}} P(s_{j}\mid s_{i},a)\,R(s_{i},a,s_{j})\)</span>.</p>
<p>3 <strong>The Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}\)</span> in Matrix‐Vector Notation</strong></p>
<p>Define an operator <span class="arithmatex">\(\mathcal{T}\)</span> that takes any vector <span class="arithmatex">\(v \in \mathbb{R}^{|\mathcal{S}|}\)</span> (viewed as a function <span class="arithmatex">\(v:\mathcal{S}\to \mathbb{R}\)</span>) and returns another vector <span class="arithmatex">\(\mathcal{T}v\in \mathbb{R}^{|\mathcal{S}|}\)</span>. The <span class="arithmatex">\(i\)</span>-th component of <span class="arithmatex">\(\mathcal{T}v\)</span> is:</p>
<div class="arithmatex">\[
[\mathcal{T}v](s_{i})
\;=\;
\max_{a\,\in\,\mathcal{A}}
\Bigl(
  [\mathbf{r}_{a}]_{\,i}
  \;+\;
  \gamma\,[\mathbf{P}_{a}\,v]_{\,i}
\Bigr).
\]</div>
<p>Concretely, each action <span class="arithmatex">\(a\)</span> suggests the vector</p>
<div class="arithmatex">\[
\mathbf{r}_{a}
\;+\;
\gamma\,\mathbf{P}_{a}\,v
\quad\in\quad
\mathbb{R}^{|\mathcal{S}|}.
\]</div>
<p>We then take a coordinate‐wise maximum over <span class="arithmatex">\(a\)</span>. That is, for each <span class="arithmatex">\(i\)</span>,</p>
<div class="arithmatex">\[
[\mathcal{T}v]_{i}
\;=\;
\max_{a\in\mathcal{A}}
\Bigl(
  [\mathbf{r}_{a}]_{\,i}
  \;+\;
  \gamma\,[\mathbf{P}_{a}\,v]_{\,i}
\Bigr).
\]</div>
<p>4 <strong>Writing <span class="arithmatex">\(v_{*}\)</span> = <span class="arithmatex">\(\mathcal{T}v_{*}\)</span> in a Matrix‐Vector Way</strong></p>
<p>The Bellman Optimality Equation for the optimal value function <span class="arithmatex">\(v_{*}\)</span> is equivalently the statement that</p>
<div class="arithmatex">\[
v_{*}
\;=\;
\mathcal{T}\,v_{*}.
\]</div>
<p>Component‐wise:</p>
<div class="arithmatex">\[
[v_{*}](s_{i})
\;=\;
[\mathcal{T}v_{*}](s_{i})
\;=\;
\max_{a\in\mathcal{A}}
\Bigl(
  [\mathbf{r}_{a}]_{i}
  \;+\;
  \gamma\,[\mathbf{P}_{a}\,v_{*}]_{i}
\Bigr).
\]</div>
<p>This matches exactly the scalar form</p>
<div class="arithmatex">\[
v_{*}(s_{i})
\;=\;
\max_{a\,\in\,\mathcal{A}}
\sum_{s_{j}}
P(s_{j}\,\vert\,s_{i},a)
\,\Bigl(R(s_{i},a,s_{j}) + \gamma\,v_{*}(s_{j})\Bigr).
\]</div>
<p>5 <strong>Observations</strong></p>
<ol>
<li>Unlike the policy‐evaluation case (a single <span class="arithmatex">\(\mathbf{P}_{\pi}\)</span> and <span class="arithmatex">\(\mathbf{r}_{\pi}\)</span>), here we have one transition matrix <span class="arithmatex">\(\mathbf{P}_{a}\)</span> and one reward vector <span class="arithmatex">\(\mathbf{r}_{a}\)</span> for each action <span class="arithmatex">\(a\)</span>.  </li>
<li>The operator <span class="arithmatex">\(\mathcal{T}\)</span> is nonlinear because of the coordinate‐wise maximum over actions. Consequently, one cannot write a single “matrix equation” like <span class="arithmatex">\(\mathbf{v} = \mathbf{r} + \gamma\,\mathbf{P}\,\mathbf{v}\)</span> for the optimal scenario. Instead, the “matrix‐vector” expression is:</li>
</ol>
<div class="arithmatex">\[
\mathcal{T}\,v \;=\;
\max_{a \,\in\, \mathcal{A}}
\bigl(
  \mathbf{r}_{a}
  \;+\;
  \gamma\,\mathbf{P}_{a}\,v
\bigr)
\]</div>
<p>(where “<span class="arithmatex">\(\max\)</span>” is applied coordinate‐wise).</p>
<ol>
<li>The Bellman Optimality Equation then says <span class="arithmatex">\(v_{*}\)</span> is the unique (under standard conditions) fixed point of <span class="arithmatex">\(\mathcal{T}\)</span>, i.e.,</li>
</ol>
<div class="arithmatex">\[
v_{*} =
\mathcal{T}\,v_{*}.
\]</div>
<p>6 <strong>Summary</strong></p>
<p>• The scalar equation</p>
<div class="arithmatex">\[
v_{*}(s_{i}) =
\max_{a\,\in\,\mathcal{A}}
\sum_{s_{j}\,\in\,\mathcal{S}}
P(s_{j}\,\vert\,s_{i},a)\,\Bigl[
  R(s_{i},\,a,\,s_{j})
  +
  \gamma\,v_{*}(s_{j})
\Bigr]
\]</div>
<p>can be interpreted in matrix‐vector notation by defining one transition matrix <span class="arithmatex">\(\mathbf{P}_{a}\)</span> and one expected‐reward vector <span class="arithmatex">\(\mathbf{r}_{a}\)</span> per action <span class="arithmatex">\(a\)</span>.  </p>
<p>• The Bellman Optimality Operator <span class="arithmatex">\(\mathcal{T}\)</span> then “picks” for each state the best action’s vector</p>
<div class="arithmatex">\[
\mathbf{r}_{a}
+
\gamma\,\mathbf{P}_{a}\,v,
\]</div>
<p>yielding</p>
<div class="arithmatex">\[
[\mathcal{T}v](s_{i}) =
\max_{a\in \mathcal{A}}
\Bigl(
  [\mathbf{r}_{a}]_{\,i}
  \;+\;
  \gamma\,[\mathbf{P}_{a}\,v]_{\,i}
\Bigr).
\]</div>
<p>• The Bellman optimality equation succinctly becomes <span class="arithmatex">\(v_{*} = \mathcal{T}\,v_{*}\)</span>, capturing the same idea as the scalar form but in a higher‐level matrix‐vector operator sense.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "toc.integrate", "navigation.top", "content.code.copy", "content.code.annotate", "palette.toggle"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/eruda"></script>
      
        <script src="../javascripts/eruda_init.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>