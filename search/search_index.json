{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Notes on Reinforcement Learning","text":""},{"location":"#about-the-book","title":"About the Book","text":"<p>This book covers fundamental concepts of RL. We follow the excellent book: Shiyu Zhao, \"Mathematical Foundations of Reinforcement Learning\".</p> <p>We use the GPT4-o1 to augment the book. These notes are the outcome of a chat session with o1 using prompts to clarify concepts as we follow the book.</p>"},{"location":"#chapters-overview-for-sidebar-on-home-page","title":"Chapters Overview (for sidebar on Home page)","text":"<ul> <li>Chapter 1: Basics</li> <li>Chapter 2: Temp</li> </ul>"},{"location":"chapter1/","title":"Chapter 1: Reinforcement Learning basics","text":""},{"location":"chapter1/#explain-the-concept-of-reinforcement-learning-in-a-formal-way","title":"Explain the concept of Reinforcement Learning in a formal way.","text":"<p>Below is a step-by-step formal explanation of the basic concepts in Reinforcement Learning (RL). We will frame RL in terms of a Markov Decision Process (MDP), which is the standard mathematical model used.</p> <ol> <li>Markov Decision Process (MDP)    A Markov Decision Process is characterized by the tuple  </li> </ol> \\[ (\\mathcal{S},\\,\\mathcal{A},\\,P,\\,R,\\,\\gamma), \\] <p>where:    \u2022 \\(\\mathcal{S}\\) is the state space. Each element \\(s \\in \\mathcal{S}\\) represents a possible configuration of the environment.    \u2022 \\(\\mathcal{A}\\) is the action space. Each element \\(a \\in \\mathcal{A}\\) is a possible action the agent can take.    \u2022 \\(P(s' \\mid s,a)\\) is the transition probability function, which gives the probability of transitioning from state \\(s\\) to state \\(s'\\) when action \\(a\\) is taken. Formally,  </p> \\[ P(s' \\mid s,a) \\;=\\;\\Pr(S_{t+1} = s' \\,\\big\\vert\\, S_t = s,\\, A_t = a). \\] <p>\u2022 \\(R(s,a)\\) is the reward function, which specifies the reward the agent receives for taking action \\(a\\) in state \\(s\\). Sometimes it is also written as the expected immediate reward \\(R: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\).    \u2022 \\(\\gamma \\in [0,1]\\) is the discount factor, which balances the importance of immediate rewards versus future rewards. A smaller \\(\\gamma\\) emphasizes present rewards, while a value near \\(1\\) gives more weight to future rewards.</p> <ol> <li> <p>Agent-Environment Interaction    In Reinforcement Learning, an agent interacts with an environment (modeled as an MDP) over a sequence of discrete time steps \\(t = 0,1,2,\\dots\\). The agent observes the state \\(S_t\\), takes an action \\(A_t\\), receives a reward \\(R_{t+1}\\), and the environment transitions to a new state \\(S_{t+1}\\). The agent\u2019s goal is to learn a strategy (policy) to choose actions that maximize cumulative reward.</p> </li> <li> <p>Policy (\\(\\pi\\))    A policy \\(\\pi\\) is a mapping from states to probabilities of selecting each available action. Formally,  </p> </li> </ol> \\[ \\pi(a \\mid s) \\;=\\;\\Pr(A_t = a \\,\\big\\vert\\,S_t = s). \\] <p>A (deterministic) policy can also be written as a function $ \\pi : \\mathcal{S} \\to \\mathcal{A}$ that picks a specific action in each state.</p> <ol> <li>Return (Cumulative Discounted Reward)    The return \\(G_t\\) from a time step \\(t\\) is defined as the total discounted reward from time \\(t\\) onward. That is,  </li> </ol> \\[ G_t \\;=\\; R_{t+1} \\;+\\;\\gamma\\,R_{t+2} \\;+\\;\\gamma^2\\,R_{t+3} \\;+\\;\\ldots     \\;=\\; \\sum_{k=0}^{\\infty} \\gamma^{\\,k}\\, R_{t+k+1}. \\] <ol> <li>Value Functions    Value functions quantify how \u201cgood\u201d it is to be in a certain state or to take a certain action in a given state. Two common value functions are:  </li> </ol> <p>a) State-Value Function    The value of a state \\(s\\) under a policy \\(\\pi\\), denoted \\(v_\\pi(s)\\), is the expected return when starting in state \\(s\\) and following \\(\\pi\\) thereafter. Formally,  </p> \\[ v_\\pi(s) \\;=\\; \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s,\\;\\pi \\bigr]. \\] <p>b) Action-Value Function    The action-value function \\(q_\\pi(s,a)\\) is the expected return when starting from state \\(s\\), taking action \\(a\\), and thereafter following policy \\(\\pi\\). Formally,  </p> \\[ q_\\pi(s,a) \\;=\\;     \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s,\\;A_t = a,\\;\\pi \\bigr]. \\] <ol> <li>Optimal Value Functions and Optimal Policy    The goal in reinforcement learning is typically to find an optimal policy \\(\\pi^*\\) that maximizes the expected return from every state. We define the optimal state-value function \\(v_*(s)\\) and the optimal action-value function \\(q_*(s,a)\\) by  </li> </ol> \\[ v_*(s) \\;=\\; \\max_{\\pi} \\;v_\\pi(s),    \\quad    q_*(s,a) \\;=\\; \\max_{\\pi} \\;q_\\pi(s,a). \\] <p>An optimal policy \\(\\pi^*\\) is any policy that achieves these optimal values.</p> <ol> <li>Bellman Optimality Equations    The optimal value functions satisfy the Bellman optimality equations:  </li> </ol> \\[ v_*(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Bigl[\\,R(s,a)\\;+\\;\\gamma\\sum_{s'}P(s' \\mid s,a)\\,v_*(s')\\Bigr], \\] \\[ q_*(s,a) \\;=\\; R(s,a)\\;+\\;\\gamma\\sum_{s'} P(s' \\mid s,a)\\,\\max_{a'} q_*(s',a'). \\] <p>These equations express a recursive relationship between the value of a state (or state-action pair) and the values of subsequent states (state-action pairs).</p> <ol> <li>Learning in Reinforcement Learning    The crux of Reinforcement Learning is that the agent does not have full knowledge of the MDP (i.e., it does not initially know \\(P\\) or \\(R\\)). The agent must estimate the optimal policy by interacting with the environment:    \u2022 It observes states and rewards.    \u2022 It updates estimates (e.g., of \\(v_*(s)\\) or \\(q_*(s,a)\\)).    \u2022 It adjusts its policy to choose better actions in the future.  </li> </ol> <p>Popular algorithms include:    \u2022 Temporal-Difference Methods (e.g., Q-Learning, SARSA).    \u2022 Monte Carlo Methods.    \u2022 Policy Gradient Methods.  </p> <p>In summary, Reinforcement Learning is about an agent learning to map states of an environment to actions in order to maximize the cumulative reward. Mathematically, RL methods are designed to solve or approximate solutions to the MDP\u2019s optimal value functions\u2014and thus find an optimal or near-optimal policy\u2014when the agent must learn about the environment solely through experience.</p>"},{"location":"chapter1/#reward-function-rsa-dependence","title":"Reward function \\(R(s,a)\\) dependence","text":"<p>It seems to me that the reward function \\(R(s,a)\\) should also depend upon \\(s\u2019\\)? For a given \\((s,a)\\), the agent could transition to multiple \\(s\u2019\\) and therefore it suggests that \\(R\\) should also depend on \\(s\u2019\\). Clarify this.</p> <p>Answer</p> <p>Below is a step-by-step clarification of why, in many MDP or Reinforcement Learning formulations, one sometimes sees the reward function written as \\(R(s,a)\\) (i.e., depending only on \\((s,a)\\)) and other times as \\(R(s,a,s')\\) (i.e., also including the next state):</p> <ol> <li>Different but Equivalent MDP Definitions</li> </ol> <p>In the most general Markov Decision Process (MDP) definition, the one-step reward after taking action \\(a\\) in state \\(s\\) and ending up in state \\(s'\\) can indeed depend on all three: </p> \\[ R(s,a,s') : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\;\\rightarrow\\; \\mathbb{R}. \\] <p>This is sometimes written as \\(R_t = R(S_t, A_t, S_{t+1})\\).</p> <p>However, many treatments simplify notation by writing</p> \\[ R(s,a) \\quad\\text{or}\\quad R(s) \\quad\\text{or}\\quad R(a) \\] <p>depending on which elements really matter for the reward in the domain under consideration.</p> <ol> <li>Why Using R(s,a) Can Be Enough</li> </ol> <p>In some problems, the one-step reward is essentially fixed once you pick \\((s,a)\\), regardless of the next state \\(s'\\). More precisely, one might define:</p> \\[ R(s,a) \\;=\\; \\mathbb{E}\\bigl[\\,R_t \\,\\big\\vert\\,S_t=s,\\,A_t=a\\bigr], \\] <p>meaning that this function gives the expected immediate reward upon taking action \\(a\\) in state \\(s\\). If the environment is stochastic (i.e., multiple possible \\(s'\\)), then</p> \\[ R(s,a) \\;=\\;\\sum_{s'} P(s' \\mid s,a)\\,\\tilde{r}(s,a,s'), \\] <p>where \\(\\tilde{r}\\) is some underlying reward for transitioning to \\(s'\\). Thus, even though the actual reward might depend on \\(s'\\), one can bundle the expectation over \\(s'\\) into \\(R(s,a)\\). This keeps the notation more compact while still retaining the necessary information.</p> <ol> <li>When R(s,a,s') is Used Explicitly</li> </ol> <p>If you want to specify exactly how the reward depends on the next state \\(s'\\), or if your problem inherently requires that detail (for instance, certain transitions might yield different bonuses or penalties), you can define</p> \\[ R(s,a,s'). \\] <p>Then, for each transition \\((s,a)\\to s'\\), you would get a specific reward \\(R(s,a,s')\\) rather than an expected value. This form is more explicit but can be more cumbersome in some derivations.  </p> <ol> <li>Equivalence in Theory</li> </ol> <p>Mathematically, the two formulations (a) \\(R(s,a)\\,=\\,\\mathbb{E}[R(s,a,s')\\,\\vert\\,s,a]\\) and  (b) \\(R(s,a,s')\\) are equivalent if you interpret \\(R(s,a)\\) as that conditional expectation over the randomized transition to \\(s'\\). </p> <p>Therefore, one formulation or the other does not fundamentally change the problem; it just changes how explicitly you write the dependence on \\(s'\\).</p> <ol> <li>Example: Chess</li> </ol> <p>Taking chess as an example: \u2022 One might write \\(R(s,a) = 0\\) for all intermediate moves (since no immediate reward is given until checkmate). At the end of the game, you get +1, -1, or 0. \u2022 If you wanted an incremental reward for capturing certain pieces, you might define \\(R(s,a,s')\\) that encodes a bonus whenever a capture move transitions you to a state \\(s'\\) with fewer opponent pieces. Alternatively, you could incorporate that into an expected sense, but it becomes somewhat clearer to define it as a function of the specific transition.</p> <ol> <li>Summary</li> </ol> <p>\u2022 It is correct that there can be multiple next states \\(s'\\) for a given \\((s,a)\\). \u2022 Hence, in the most general MDP definition, we often see \\(R(s,a,s')\\). \u2022 Many RL texts and simpler MDP formulations use \\(R(s,a)\\) to represent the expected immediate reward (averaging over the distribution of next states \\(s'\\)), making notation a bit simpler for analysis.  </p> <p>In short: yes, the underlying reward can depend on \\(s'\\). One either writes \\(R(s,a,s')\\) explicitly or uses the shorter \\(R(s,a)\\) to denote the expectation over \\(s'\\). Both are valid formulations of the same concept.</p>"},{"location":"chapter1/#the-policy-picdot","title":"The policy \\(\\pi(\\cdot)\\)","text":"<p>The policy \\(\\pi(a \\vert s) = \\text{Pr}(A_t = a \\vert S_t = s)\\) seems to be defined for each state \\(a\\). Once a policy is executed at \\(a\\), then we are in a new state, say \\(s\u2019\\). Is the same policy \\(\\pi\\) defined for \\(s\u2019\\) as well? In other words, is \\(\\pi(\\cdot)\\) defined for the entire state space?</p> <p>Answer</p> <p>Below is a step-by-step explanation clarifying whether the same policy \\(\\pi\\) is defined for every state in the state space:</p> <ol> <li>What a Policy Is    \u2022 In Reinforcement Learning, a policy \\(\\pi\\) is defined as a mapping from states to a probability distribution over actions. Formally:  </li> </ol> \\[ \\pi:\\;\\mathcal{S}\\;\\to\\;\\Delta(\\mathcal{A}), \\] <p>where \\(\\Delta(\\mathcal{A})\\) is the set of all probability distributions over the action set \\(\\mathcal{A}\\).  </p> <p>\u2022 Equivalently, we often write \\(\\pi(a \\mid s)\\) to denote the probability of choosing action \\(a\\) in state \\(s\\).</p> <ol> <li> <p>Applying \\(\\pi\\) in an MDP Scenario    \u2022 Consider the Markov Decision Process (MDP) with state space \\(\\mathcal{S}\\). At any step \\(t\\), the agent observes the current state \\(S_t\\in \\mathcal{S}\\).    \u2022 The policy \\(\\pi\\) is used to select an action \\(A_t\\). Even if \\(\\pi\\) is stochastic, \\(\\pi(a \\mid s)\\) tells us how likely each action \\(a\\) is, given \\(s\\).</p> </li> <li> <p>Transition to the Next State    \u2022 After executing the chosen action \\(A_t\\) in state \\(S_t\\), the environment transitions to a new state \\(S_{t+1}=s'\\).    \u2022 Now, at state \\(s'\\), the same policy \\(\\pi\\) applies; namely, to choose the next action \\(A_{t+1}\\), we again use \\(\\pi(a \\mid s')\\).    \u2022 Thus, one and the same \\(\\pi\\) determines actions for every state the agent might encounter.</p> </li> <li> <p>\\(\\pi\\) Is Defined for the Entire State Space    \u2022 By definition, a (stationary) reinforcement-learning policy gives a decision rule for every possible state in \\(\\mathcal{S}\\).    \u2022 That is, for each state \\(s\\in\\mathcal{S}\\), \\(\\pi\\) prescribes how the agent should act.    \u2022 So, yes\u2014\\(\\pi\\) is indeed \u201cthe same policy\u201d for all states, in the sense that there is a single function \\(\\pi(\\cdot)\\) that applies throughout the state space.</p> </li> <li> <p>Illustration with Deterministic Policies    \u2022 If \\(\\pi\\) is deterministic, then for every state \\(s\\), \\(\\pi(s)\\) is exactly one action (rather than a distribution).    \u2022 In either case\u2014deterministic or stochastic\u2014\\(\\pi\\) unequivocally answers: \u201cWhat do we do if we end up in state \\(s\\)?\u201d</p> </li> <li> <p>Policy Improvement or Changes Over Time    \u2022 In some learning algorithms, the policy \\(\\pi\\) may be updated as the agent gains experience (for instance, by gradient steps or by policy iteration).    \u2022 However, at any particular time during training (or after training), the agent\u2019s \u201ccurrent\u201d policy is still a single function defined over all states.</p> </li> <li> <p>Conclusion    \u2022 Yes, once the agent transitions to the new state \\(s'\\), the same policy \\(\\pi\\) is again used to pick the next action.    \u2022 Formally, a policy is defined over the entire state space \\(\\mathcal{S}\\), ensuring that no matter which state is encountered, the agent knows how it should proceed.</p> </li> </ol>"},{"location":"chapter1/#trajectory","title":"Trajectory","text":"<p>Suppose, the agent is at state \\(s_t\\) at time \\(t\\) under a policy \\(\\pi\\). Per this policy the action \\(a_t\\) is applied and the agent moves to state \\(s_{t+1}\\) getting the reward \\(R_{t+1}\\). In this way, the agent traces out a trajectory in state space from which the Return \\(G_t\\) for this trajectory can be calculated. Is my understanding correct?</p> <p>Answer</p> <p>Below is a step-by-step confirmation and elaboration on your understanding, with all mathematical symbols in LaTeX:</p> <ol> <li> <p>Agent in State \\({s_t}\\) at Time \\(t\\)    At any discrete time step \\(t\\), the agent observes the current state \\(s_t\\).</p> </li> <li> <p>Action \\(a_t\\) per Policy \\(\\pi\\)    According to the agent\u2019s policy \\(\\pi\\), the agent selects and executes an action \\(a_t\\).    \u2022 If \\(\\pi\\) is stochastic, then  </p> </li> </ol> \\[ a_t \\sim \\pi(\\cdot \\,\\vert\\, s_t), \\] <p>meaning \\(a_t\\) is sampled from the probability distribution \\(\\pi(a\\,\\vert\\,s_t)\\).    \u2022 If \\(\\pi\\) is deterministic, then     </p> \\[ a_t \\;=\\;\\pi(s_t). \\] <ol> <li>Transition and Reward    After the agent executes action \\(a_t\\) in state \\(s_t\\), the environment transitions to a new state \\(s_{t+1}\\) and provides a (possibly stochastic) reward \\(R_{t+1}\\). Formally,  </li> </ol> \\[ s_{t+1} \\;\\sim\\; P(\\,\\cdot \\,\\vert\\, s_t,\\, a_t), \\] \\[ R_{t+1} \\;=\\; R\\bigl(s_t,\\,a_t,\\,s_{t+1}\\bigr)    \\quad    (\\text{or an expected form }R(s,a)\\text{, depending on the formulation}). \\] <ol> <li>Trajectory (Episode)    By repeating this process over time, one obtains a trajectory (or sequence) of states, actions, and rewards:  </li> </ol> \\[ \\bigl(s_0,\\,a_0,\\,R_1,\\,s_1,\\,a_1,\\,R_2,\\,s_2,\\,a_2,\\,R_3,\\dots\\bigr). \\] <p>Such a sequence typically continues until a terminal condition (end of episode) is reached or indefinitely in an ongoing task.</p> <ol> <li>Return \\(G_t\\)    The return \\(G_t\\) from time \\(t\\) is the total discounted reward obtained by following policy \\(\\pi\\) from time \\(t\\) onward:  </li> </ol> \\[ G_t \\;=\\; R_{t+1} \\;+\\;\\gamma\\,R_{t+2} \\;+\\;\\gamma^2\\,R_{t+3} \\;+\\;\\dots     \\;=\\; \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}, \\] <p>where \\(0 \\,\\le\\, \\gamma \\,\\le\\, 1\\) is the discount factor that balances immediate and future rewards.</p> <ol> <li>Conclusion    Hence, your understanding is correct: by following the policy \\(\\pi\\), the agent traces out a trajectory $ (s_t, a_t, R_{t+1}, s_{t+1}, \\dots) $ in the state space. From this trajectory, one can calculate the return \\(G_t\\). This process\u2014collecting states, actions, and rewards\u2014is at the heart of Reinforcement Learning, where the agent\u2019s goal is to maximize the expected return.  </li> </ol>"},{"location":"chapter1/#transition-probability-and-policy-probability","title":"Transition probability and Policy Probability","text":"<p>In the MDP formulation, we can regard \\(S_t \\in \\mathcal{S}\\) as a random variable. Similarly, \\(A_t \\in \\mathcal{A}\\) at time \\(t\\). We have 2 probabilistic definitions:</p> <p>1) State Transition:  \\(\\text{Pr}(S_{t+1} = s ^{\\prime} \\vert S_t = s, A_t = a)\\). This gives the probability of a particular value of the state \\(s^{\\prime}\\) at time \\(t+1\\) given specific values of the state and action at time \\(t\\). Therefore we have:</p> \\[ \\sum_{s^{\\prime} \\in \\mathcal{S}} \\text{Pr}(S_{t+1} = s ^{\\prime} \\vert S_t = s, A_t = a) = 1 \\] <p>2) Policy function: \\(\\pi(a \\vert s) = \\text{Pr}(A_t = a \\vert S_t = s)\\). Therefore we have:</p> \\[ \\sum_{a \\in \\mathcal{A}}  \\text{Pr}(A_t = a \\vert S_t = s) =1 \\] <p>Is this correct?</p> <p>Below is a step-by-step confirmation and explanation:</p> <p>1) State Transition Probability \u2022 In a Markov Decision Process (MDP), the transition function  </p> \\[ \\Pr\\bigl(S_{t+1} = s' \\,\\big\\vert\\, S_t = s,\\;A_t = a\\bigr) \\] <p>is a probability distribution over the next state \\(s'\\) given the current state \\(s\\) and action \\(a\\). \u2022 Because it is a probability distribution over \\(s'\\in\\mathcal{S}\\), it must sum to 1:  </p> \\[ \\sum_{s' \\in \\mathcal{S}} \\Pr\\bigl(S_{t+1} = s' \\,\\big\\vert\\, S_t = s,\\;A_t = a\\bigr) \\;=\\; 1. \\] <p>2) Policy Function \u2022 A policy \\(\\pi\\) for a stationary MDP is given by  </p> \\[ \\pi(a \\mid s) \\;=\\; \\Pr\\bigl(A_t = a \\,\\big\\vert\\, S_t = s\\bigr). \\] <p>\u2022 This is a probability distribution over actions \\(a\\in\\mathcal{A}\\), for each given state \\(s\\). Hence, for each \\(s\\),  </p> \\[ \\sum_{a \\,\\in\\,\\mathcal{A}} \\pi(a \\mid s) \\;=\\; 1. \\] <p>3) Conclusion Yes, both conditions you stated are correct: \u2022 The transition probabilities over all possible next states sum to 1. \u2022 The policy probabilities over all possible actions in a given state also sum to 1. </p>"},{"location":"chapter1/#state-value-function","title":"State value function","text":"<p>We have shown before that for a trajectory that starts at state \\(s_t\\) under the policy \\(\\pi\\),  the return \\(G_t\\) from time \\(t\\) is the total discounted reward obtained by following policy \\(\\pi\\) from time \\(t\\) onward:  </p> \\[ G_t \\;=\\; R_{t+1} \\;+\\;\\gamma\\,R_{t+2} \\;+\\;\\gamma^2\\,R_{t+3} \\;+\\;\\dots     \\;=\\; \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}, \\] <p>where \\(0 \\,\\le\\, \\gamma \\,\\le\\, 1\\) is the discount factor. Since \\(S_t, A_t\\) are random variables, so is \\(R_t\\) and therefore \\(G_t\\). Another way to say it that there are many trajectories that emanate from\\(s_t\\) under \\(\\pi\\) and we can define an expected value of \\(G_t\\) to be the state value function \\(v_{\\pi}(s)\\) where \\(S_t = s\\).</p> \\[ v_{\\pi} = \\mathbb{E} (G_t \\vert S_t =s ) \\] <p>Does this definition of the state value make sense and if so, the expected value implies a conditional probability distribution. How is this known?</p> <p>Answer Below is a step-by-step explanation:</p> <p>1) The Definition of the State-Value Function The state-value function under a policy \\(\\pi\\), denoted \\(v_{\\pi}(s)\\), is defined as  </p> \\[ v_{\\pi}(s) \\;=\\; \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s \\bigr], \\] <p>where  </p> \\[ G_t \\;=\\;\\sum_{k=0}^{\\infty} \\gamma^k\\,R_{t+k+1}. \\] <p>In words, \\(v_{\\pi}(s)\\) is the expected return (sum of discounted future rewards) when starting from state \\(s\\) at time \\(t\\) and subsequently following policy \\(\\pi\\).</p> <p>2) Why Is \\(G_t\\) Random? \u2022 The trajectory that unfolds from time \\(t\\) onward depends on:   \u2013 The stochastic policy \\(\\pi(a \\mid s)\\), which determines how likely each action \\(a\\) is in state \\(s\\).   \u2013 The transition probabilities \\(P(s' \\mid s,a)\\), which govern how you move to the next state \\(s'\\).   \u2013 The potentially stochastic reward \\(R_{t+1}\\) itself. \u2022 Therefore, for a fixed \\(s\\), there are many possible future trajectories (sequences of states, actions, and rewards). Each trajectory has a different realization of \\(G_t\\). Hence \\(G_t\\) is a random variable.</p> <p>3) Taking the Expectation (Conditional Probability Distribution) Because there are multiple possible ways the future might evolve from \\(s\\), the definition  </p> \\[ \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s\\bigr] \\] <p>reflects an expectation over all those possible trajectories. Formally, this expectation is taken with respect to the conditional probability distribution of all future states, actions, and rewards given that \\(S_t = s\\) and that the agent follows policy \\(\\pi\\). Symbolically, we can write  </p> \\[ \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s\\bigr] \\;=\\; \\sum_{\\text{all possible future trajectories}} \\Pr\\bigl(\\text{trajectory}\\,\\big\\vert\\,S_t=s,\\;\\pi\\bigr) \\;\\times\\; G_t(\\text{trajectory}), \\] <p>where \\(\\Pr(\\text{trajectory}\\,\\vert\\,S_t=s,\\;\\pi)\\) is constructed from the chain of probabilities defined by \\(\\pi(a\\mid s)\\) and \\(P(s'\\mid s,a)\\).</p> <p>4) Where the Conditional Distribution Comes From \u2022 The agent\u2019s policy \\(\\pi(a\\mid s)\\) gives a distribution over actions \\(a\\) for every state \\(s\\). \u2022 The environment\u2019s transition function \\(P(s'\\mid s,a)\\) gives a distribution over next states \\(s'\\) for each pair \\((s,a)\\). \u2022 Combining these, one obtains the probability tree of all future trajectories (including rewards). \u2022 In RL notation, we simply write \\(\\mathbb{E}[\\,G_t\\mid S_t=s]\\) as a shorthand. Under the hood, it involves the full conditional distribution derived from \\(\\pi\\) and \\(P\\).</p> <p>5) Conclusion Yes, this definition of the state-value function  </p> \\[ v_{\\pi}(s) \\;=\\; \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t=s\\bigr] \\] <p>makes sense: we take an expectation of the random return \\(G_t\\) over all possible ways the future can evolve from state \\(s\\), given that the agent acts according to \\(\\pi\\). That expectation is indeed a conditional probability distribution (conditioned on \\(S_t = s\\)), implicitly governed by \\(\\pi\\) and the transition probabilities of the MDP.</p>"},{"location":"chapter1/#probability-tree-for-v_pis","title":"Probability tree for \\(v_{\\pi}(s)\\)","text":"<p>Symbolically, \\(v_{\\pi}(s)\\) is given by</p> \\[ \\mathbb{E}\\bigl[G_t \\,\\big\\vert\\, S_t = s\\bigr] \\;=\\; \\sum_{\\text{all possible future trajectories}} \\Pr\\bigl(\\text{trajectory}\\,\\big\\vert\\,S_t=s,\\;\\pi\\bigr) \\;\\times\\; G_t(\\text{trajectory}), \\] <p>to illustrate how this conditional probability distribution is derived from \\(\\pi\\) and \\(P\\), use the time instants \\(t, t+1, t+2\\) and work out the probability tree.</p> <p>Answer Below is a step-by-step illustration of how the probability of a particular future trajectory from time \\(t\\) through \\(t+2\\) (and onward) is derived from the policy \\(\\pi\\) and the transition probabilities \\(P\\). We will explicitly write out the probability of the first couple of steps in the \u201cprobability tree\u201d before it continues further.</p> <p>1 Starting at Time t, State s</p> <p>We begin with:</p> \\[ S_t = s. \\] <p>Because we are conditioning on \\(S_t = s\\), the probability of being at \\(s\\) at time \\(t\\) is defined to be \\(1\\) in that conditional scenario.</p> <p>2 Choosing Action A\u209c\\</p> <p>Under policy \\(\\pi\\), the agent chooses action \\(A_t = a \\in \\mathcal{A}\\) with probability</p> \\[ \\pi(a \\,\\vert\\, s). \\] <p>Hence,</p> <p>\u2022 Probability of picking action \\(a\\) at time \\(t\\):</p> \\[ \\Pr\\bigl(A_t = a \\,\\big\\vert\\, S_t = s,\\;\\pi\\bigr) \\;=\\; \\pi(a \\,\\vert\\, s). \\] <p>3 Transition to Time t+1</p> <p>After choosing \\(A_t = a\\), the environment transitions to a new state \\(S_{t+1} = s' \\in \\mathcal{S}\\) with probability</p> \\[ P(s' \\,\\vert\\, s,\\, a). \\] <p>Combined with the previous step, the joint probability of picking action \\(a\\) and ending up in state \\(s'\\) is:</p> \\[ \\Pr\\bigl(A_t = a,\\,S_{t+1} = s' \\,\\big\\vert\\, S_t = s,\\,\\pi\\bigr) \\;=\\; \\pi(a \\,\\vert\\, s)\\;\\times\\;P(s' \\,\\vert\\, s,\\,a). \\] <p>4 Choosing Action A\u208dt+1\u208e in State s'</p> <p>Once in state \\(s'\\) at time \\(t+1\\), the agent chooses action \\(A_{t+1} = a'\\) with probability</p> \\[ \\pi(a' \\,\\vert\\, s'). \\] <p>Hence,</p> <p>\u2022 Probability of picking action \\(a'\\) in state \\(s'\\) at time \\(t+1\\):</p> \\[ \\Pr\\bigl(A_{t+1} = a' \\,\\big\\vert\\, S_{t+1} = s',\\,\\pi\\bigr) \\;=\\; \\pi(a' \\,\\vert\\, s'). \\] <p>5 Transition to Time t+2</p> <p>After action \\(A_{t+1} = a'\\), the environment transitions to \\(S_{t+2} = s''\\) with probability</p> \\[ P(s'' \\,\\vert\\, s',\\, a'). \\] <p>Putting it together up to time \\(t+2\\):</p> \\[ \\Pr\\Bigl(A_t = a,\\;S_{t+1} = s',\\;A_{t+1} = a',\\;S_{t+2} = s'' \\,\\Big\\vert\\, S_t = s,\\;\\pi\\Bigr) \\;=\\; \\pi(a \\,\\vert\\, s) \\;\\times\\; P\\bigl(s' \\,\\vert\\, s,\\,a\\bigr) \\;\\times\\; \\pi(a' \\,\\vert\\, s') \\;\\times\\; P\\bigl(s'' \\,\\vert\\, s',\\,a'\\bigr). \\] <p>6 Extending Beyond t+2</p> <p>By continuing this process for \\(t+3, t+4, \\dots\\), one builds the full probability tree of subsequent states and actions:</p> <p>\u2022 At each step, the agent\u2019s policy \\(\\pi(\\cdot \\,\\vert\\, s_j)\\) supplies the probabilities of each possible action given the current state \\(s_j\\). \u2022 The environment\u2019s transition dynamics \\(P(s_{j+1} \\,\\vert\\, s_j,\\, a_j)\\) supply the probabilities of next states.  </p> <p>Therefore, any full future trajectory (from time \\(t\\) to the end of the episode or indefinitely in an ongoing task) has a probability obtained by multiplying together all these policy and transition probabilities.</p> <p>7 Putting It All Into the Expectation for v\u208d\u03c0\u208e(s)</p> <p>When we write</p> \\[ v_{\\pi}(s) \\;=\\; \\mathbb{E}\\bigl[\\,G_t \\;\\big\\vert\\; S_t = s\\bigr] \\;=\\; \\sum_{\\substack{\\text{all possible future} \\\\ \\text{trajectories}}} \\Pr\\bigl(\\text{trajectory}\\,\\big\\vert\\,S_t = s,\\;\\pi\\bigr)\\,\\times\\,G_t(\\text{trajectory}), \\] <p>we are summing (or integrating) over all possible sequences of the form</p> \\[ \\bigl(A_t,\\,S_{t+1},\\,A_{t+1},\\,S_{t+2},\\,A_{t+2},\\,S_{t+3},\\dots\\bigr) \\] <p>and multiplying each trajectory\u2019s return \\(G_t(\\text{trajectory})\\) by the probability of that trajectory, which is computed via the products of \\(\\pi(\\cdot)\\) and \\(P(\\cdot)\\) factors illustrated above.</p> <p>8 Conclusion</p> <p>This small probability tree up to \\(t+2\\) demonstrates how each branch\u2019s probability is the product of: 1) policy probabilities \\(\\pi(a \\,\\vert\\, s)\\), and 2) transition probabilities \\(P(s' \\,\\vert\\, s,a)\\).  </p> <p>Continuing the same pattern through subsequent time steps yields the full conditional probability distribution of all possible future trajectories, thereby defining \\(\\Pr(\\text{trajectory}\\,\\vert\\,S_t=s,\\pi)\\).  </p> <p>Thus, the expectation in \\(\\mathbb{E}[\\,G_t \\,\\vert\\,S_t=s]\\) is taken over this entire (potentially infinite) probability tree.</p>"},{"location":"chapter1/#bellman-equation","title":"Bellman Equation","text":"<p>Given a policy \\(\\pi\\), what is the Bellman equation and how is it used to calculate \\(v_{\\pi}(s)\\) for all \\(s \\in \\mathcal{S}\\)? How is it derived?</p> <p>Answer Below is a step-by-step explanation of the Bellman equation for a given policy \\(\\pi\\), how it is derived, and how it is used to calculate the value function \\(v_{\\pi}(s)\\) for all states \\(s\\in\\mathcal{S}\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 1) Definition of the State-Value Function</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Recall that under a policy \\(\\pi\\), the state-value function \\(v_{\\pi}(s)\\) is  </p> \\[ v_{\\pi}(s)  \\;=\\; \\mathbb{E}\\bigl[G_t \\;\\big\\vert\\; S_t = s\\bigr] \\;=\\; \\mathbb{E}\\Bigl[\\, \\sum_{k=0}^{\\infty} \\gamma^k\\,R_{t+k+1} \\;\\Big\\vert\\; S_t = s \\Bigr]. \\] <p>Here: \u2022 \\(G_t\\) is the (random) return from time \\(t\\) onward. \u2022 \\(R_{t+k+1}\\) is the reward at time \\(t+k+1\\). \u2022 \\(\\gamma \\in [0,\\,1]\\) is the discount factor. \u2022 The expectation \\(\\mathbb{E}[\\cdot]\\) is taken over all possible future trajectories following policy \\(\\pi\\).  </p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>2 One-Step Lookahead </p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Consider the very next step from state \\(s\\) under policy \\(\\pi\\). The agent will choose an action \\(A_t = a\\) with probability \\(\\pi(a \\,\\vert\\, s)\\). The environment will then transition to a next state \\(S_{t+1} = s'\\) with probability \\(P(s' \\,\\vert\\, s,\\,a)\\), and the agent will receive a reward \\(R_{t+1} = R(s,a,s')\\) (or its expected form \\(R(s,a)\\) depending on the problem).  </p> <p>After that step, the agent continues from state \\(s'\\), and the expected value of the return from \\(s'\\) onward is \\(v_{\\pi}(s')\\).  </p> <p>Thus the return \\(G_t\\), when starting in state \\(s\\) and taking action \\(a\\) that leads to state \\(s'\\), can be decomposed as:  </p> \\[ G_t \\;=\\; R_{t+1}  \\;+\\; \\gamma\\,G_{t+1}. \\] <p>But the expected value of \\(G_{t+1}\\), given \\(S_{t+1} = s'\\) and following \\(\\pi\\) thereafter, is \\(v_{\\pi}(s')\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>3 The Bellman Equation for v\u208d\u03c0\u208e(s)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Putting it all together, we take the expectation with respect to actions (as given by \\(\\pi\\)) and next states (as given by \\(P\\)). We get the Bellman expectation equation:</p> \\[ v_{\\pi}(s) \\;=\\; \\sum_{a \\in \\mathcal{A}} \\pi(a \\,\\vert\\, s) \\sum_{s' \\in \\mathcal{S}} P(s' \\,\\vert\\, s,\\, a) \\bigl[ R(s,a,s') \\;+\\; \\gamma\\,v_{\\pi}(s') \\bigr]. \\] <p>Sometimes you will see it written in a shorter form (if the immediate reward does not explicitly depend on \\(s'\\)):</p> \\[ v_{\\pi}(s) \\;=\\; \\sum_{a \\,\\in\\, \\mathcal{A}} \\pi(a \\,\\vert\\, s) \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\,\\vert\\, s,a) \\bigl[ R(s,a) \\;+\\; \\gamma\\,v_{\\pi}(s') \\bigr]. \\] <p>This Bellman equation says: 1. From state \\(s\\), choose an action \\(a\\) according to \\(\\pi\\). 2. Possibly transition to state \\(s'\\) with probability \\(P(s'\\mid s,a)\\). 3. Accumulate the immediate reward \\(R(s,a,s')\\). 4. Then continue and accumulate discounted future rewards, whose expected value is \\(\\gamma\\,v_{\\pi}(s')\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>4 Using the Bellman Equation to Compute v\u208d\u03c0\u208e(s)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 The set of equations </p> \\[ v_{\\pi}(s)   \\;=\\;   \\sum_{a} \\pi(a \\,\\vert\\, s)   \\sum_{s'} P(s' \\,\\vert\\, s,a)\\,\\bigl[R(s,a,s') + \\gamma\\,v_{\\pi}(s')\\bigr]   \\quad   \\text{for all } s\\in\\mathcal{S} \\] <p>can be viewed as a linear system (when \\(\\gamma &lt; 1\\)) if \\(\\mathcal{S}\\) is finite. \u2022 In principle, one can solve these equations exactly for \\(v_{\\pi}\\) by standard linear algebra if the state space \\(\\mathcal{S}\\) is not too large.  </p> <p>More commonly, in large or unknown environments, we use iterative methods\u2014such as iterative policy evaluation\u2014to approximate or converge to the true \\(v_{\\pi}(s)\\):</p> <ol> <li>Initialize \\(v^{(0)}(s)\\) arbitrarily (e.g., zeros).  </li> <li>For each iteration \\(n=0,1,2,\\dots\\), update</li> </ol> \\[ v^{(n+1)}(s)    \\;\\leftarrow\\;    \\sum_{a} \\pi(a \\,\\vert\\, s)    \\sum_{s'}     P(s' \\,\\vert\\, s,a)\\,    \\bigl[R(s,a,s') + \\gamma\\,v^{(n)}(s')\\bigr]. \\] <ol> <li>Repeat until convergence.</li> </ol> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>5 Why It Is Called a \u201cConsistency\u201d Equation</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>The Bellman equation essentially enforces consistency: the value of a state \\(s\\) must match the expected immediate reward plus the discounted value of the successor state under the given policy. It is a self-consistent recursive definition of \\(v_{\\pi}\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>6 Derivation Summary</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>In summary, the Bellman equation is derived by: 1. Defining \\(v_{\\pi}(s) = \\mathbb{E}[G_t \\mid S_t=s]\\). 2. Decomposing the return \\(G_t\\) into immediate reward plus discounted future return, \\(R_{t+1} + \\gamma\\,G_{t+1}\\). 3. Taking expectations over the random action-choice and state-transition processes dictated by \\(\\pi\\) and \\(P\\).  </p> <p>Hence, we arrive at the Bellman expectation equation, which provides a system of equations for \\(v_{\\pi}(s)\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>7 Conclusion</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 The Bellman equation for a policy \\(\\pi\\) is</p> \\[ v_{\\pi}(s) \\;=\\; \\sum_{a \\,\\in\\, \\mathcal{A}} \\pi(a \\,\\vert\\, s) \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\,\\vert\\, s,\\,a)\\, \\bigl[ R(s,a,s') \\;+\\; \\gamma\\,v_{\\pi}(s') \\bigr]. \\] <p>\u2022 It is used to calculate \\(v_{\\pi}(s)\\) for all \\(s\\) either by directly solving this system (in the finite state case) or by applying iterative policy evaluation methods.  </p> <p>This completes the derivation and usage of the Bellman equation in the context of evaluating a given policy \\(\\pi\\).</p>"},{"location":"chapter1/#bellman-equation-to-system-of-linear-equations","title":"Bellman Equation \\(\\to\\) System of Linear Equations","text":"<p>It seems that for the finite state space \\(\\mathcal{S}\\), the Bellman equation results in a system of linear equations in terms of the \"variables\" \\(v_{\\pi}(s_t)\\). Is this correct?</p> <p>Answer</p> <p>Below is a step-by-step explanation showing that for a finite state space \\(\\mathcal{S}\\), the Bellman equation corresponds to a system of linear equations in the unknowns \\(v_{\\pi}(s)\\):</p> <p>1) The Bellman Equation for a Fixed Policy    Recall the Bellman expectation equation for a policy \\(\\pi\\) and a finite MDP:  </p> \\[ v_{\\pi}(s)    \\;=\\;    \\sum_{a \\,\\in\\, \\mathcal{A}}    \\pi(a \\,\\vert\\, s)    \\sum_{s' \\,\\in\\, \\mathcal{S}}    P(s' \\,\\vert\\, s,\\,a)    \\,\\Bigl[      R(s,a,s') \\;+\\; \\gamma\\,v_{\\pi}(s')    \\Bigr]. \\] <p>Here,    \u2022  \\(\\mathcal{S} = \\{s_1, s_2, \\dots, s_{|\\mathcal{S}|}\\}\\) is a finite set of states.    \u2022  \\(\\mathcal{A}\\) is the finite set of possible actions.    \u2022  \\(\\pi(a\\mid s)\\) is the probability of action \\(a\\) in state \\(s\\).    \u2022  \\(P(s' \\mid s,a)\\) is the probability of transitioning to \\(s'\\) from \\(s\\) after action \\(a\\).    \u2022  \\(R(s,a,s')\\) is the immediate reward.</p> <p>2) Expressing the Equation for Each State    For each \\(s \\in \\mathcal{S}\\), we can write a separate instance of the Bellman equation. Concretely, labeling the states \\(s_1, s_2, \\dots, s_{|\\mathcal{S}|}\\), we get \\(|\\mathcal{S}|\\) equations of the form:  </p> \\[ v_{\\pi}(s_i)    \\;=\\;    \\sum_{a \\,\\in\\, \\mathcal{A}}    \\pi(a \\,\\vert\\, s_i)    \\sum_{s_j \\,\\in\\, \\mathcal{S}}    P(s_j \\,\\vert\\, s_i,\\,a)\\,    \\Bigl[      R(s_i,a,s_j) \\;+\\; \\gamma\\,v_{\\pi}(s_j)    \\Bigr]    \\quad\\text{for}\\quad i=1,2,\\dots,|\\mathcal{S}|. \\] <p>3) Rearranging into Linear System Form    Notice that \\(v_{\\pi}(s_i)\\) appears on the left-hand side, and \\(v_{\\pi}(s_j)\\) (with potentially different \\(j\\)) appears inside the sum on the right-hand side. What we have is a set of linear equations in the unknowns \\(v_{\\pi}(s_1),\\, v_{\\pi}(s_2), \\dots, v_{\\pi}(s_{|\\mathcal{S}|})\\), because each equation can be rearranged to something like:  </p> \\[ v_{\\pi}(s_i)    \\;-\\;    \\gamma    \\sum_{a,\\,s_j}    \\Bigl[    \\text{(coefficient)} \\times v_{\\pi}(s_j)    \\Bigr]    \\;=\\;    \\sum_{a,\\,s_j}    \\Bigl[    \\text{(coefficient)} \\times R(s_i,a,s_j)    \\Bigr]. \\] <p>All terms involving \\(v_{\\pi}(s_i)\\) or \\(v_{\\pi}(s_j)\\) are multiplied by constant coefficients (which come from \\(\\pi(a\\mid s_i)\\), \\(P(s_j\\mid s_i,a)\\), and \\(\\gamma\\)).</p> <p>4) Matrix Notation    In matrix form, one can write:  </p> \\[ \\mathbf{v}_{\\pi}    \\;=\\;    \\mathbf{r}_{\\pi}     \\;+\\;    \\gamma \\,\\mathbf{P}_{\\pi}\\,\\mathbf{v}_{\\pi}, \\] <p>where:    \u2022 \\(\\mathbf{v}_{\\pi}\\) is a column vector \\(\\bigl(v_{\\pi}(s_1), v_{\\pi}(s_2), \\ldots, v_{\\pi}(s_{|\\mathcal{S}|})\\bigr)^\\top\\).    \u2022 \\(\\mathbf{r}_{\\pi}\\) is the vector of expected immediate rewards in each state (depending on \\(\\pi\\)).    \u2022 \\(\\mathbf{P}_{\\pi}\\) is the state transition probability matrix when actions are chosen according to \\(\\pi\\).  </p> <p>Rearranging:  </p> \\[ \\mathbf{v}_{\\pi}     \\;-\\;    \\gamma\\,\\mathbf{P}_{\\pi}\\,\\mathbf{v}_{\\pi}    \\;=\\;    \\mathbf{r}_{\\pi}, \\] <p>or  </p> \\[ (\\mathbf{I} - \\gamma\\,\\mathbf{P}_{\\pi})\\,\\mathbf{v}_{\\pi}    \\;=\\;    \\mathbf{r}_{\\pi}. \\] <p>This is a linear system of dimension \\(|\\mathcal{S}|\\times|\\mathcal{S}|\\).</p> <p>5) Conclusion    Therefore, for each state in a finite MDP, the Bellman equation contributes one linear equation in the unknown \\(v_{\\pi}(s)\\). Putting them all together yields a system of \\(|\\mathcal{S}|\\) linear equations. Solving this system (when \\(\\gamma&lt;1\\) and \\(\\mathbf{I} - \\gamma\\,\\mathbf{P}_{\\pi}\\) is invertible) yields the value function \\(v_{\\pi}(s)\\) for all states simultaneously.  </p> <p>Indeed, your observation is correct: in a finite state space, the Bellman equation is precisely a system of linear equations in the variables \\(\\{\\,v_{\\pi}(s): s\\in \\mathcal{S}\\}\\).</p>"},{"location":"chapter1/#comapring-policies","title":"Comapring Policies","text":"<p>Suppose you have two policies \\(\\pi_1\\) and \\(\\pi_2\\). And also suppose that the Bellman equation is solved so that we get \\(v_{\\pi_1}\\) and \\(v_{\\pi_2}\\).  How is this used to evaluate and compare the two policies?</p> <p>Answer</p> <p>Below is a step-by-step explanation of how having the solved value functions \\(v_{\\pi_1}\\) and \\(v_{\\pi_2}\\) (from two different policies \\(\\pi_1\\) and \\(\\pi_2\\)) enables one to compare and evaluate those policies:</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 1) The Meaning of v\u208d\u03c0\u208e(s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 For a given policy \\(\\pi\\) in a Markov Decision Process (MDP), the value function  </p> \\[ v_{\\pi}(s)   \\;=\\;   \\mathbb{E}\\!\\Bigl[G_t \\,\\Big\\vert\\, S_t = s\\Bigr] \\] <p>tells us the expected return (cumulative discounted reward) starting from state \\(s\\) and following \\(\\pi\\) thereafter. \u2022 Thus, \\(v_{\\pi}(s)\\) captures how \u201cgood\u201d it is to be in state \\(s\\) when one plans to follow \\(\\pi\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 2) Comparing Two Policies State by State \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 Once you have computed \\(v_{\\pi_1}(s)\\) and \\(v_{\\pi_2}(s)\\) for all \\(s \\in \\mathcal{S}\\), you can compare the policies pointwise by looking at each state \\(s\\):   \u2013 If \\(v_{\\pi_1}(s) &gt; v_{\\pi_2}(s)\\), then \\(\\pi_1\\) yields a higher expected return than \\(\\pi_2\\) specifically from state \\(s\\).   \u2013 If \\(v_{\\pi_2}(s)\\) exceeds \\(v_{\\pi_1}(s)\\), then \\(\\pi_2\\) is better from that state. \u2022 A strict ordering of the policies for all states might not hold (one can find states where \\(\\pi_1\\) is better and others where \\(\\pi_2\\) is better). That said, if  </p> \\[ v_{\\pi_1}(s)\\;\\ge\\;v_{\\pi_2}(s)   \\quad\\text{for all }s, \\] <p>then we can unambiguously say \\(\\pi_1\\) is at least as good as \\(\\pi_2\\) in every state.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 3) Comparing Two Policies from an Initial State or Distribution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 In many RL problems, there is a designated initial state \\(s_0\\) or an initial state distribution \\(\\mu_0\\). In that scenario, one often compares the policies by looking at  </p> \\[ v_{\\pi_1}(s_0)   \\quad\\text{versus}\\quad   v_{\\pi_2}(s_0), \\] <p>or by evaluating the expected return under \\(\\mu_0\\):  </p> \\[ \\sum_{s\\in \\mathcal{S}}    \\mu_0(s)\\,   v_{\\pi_1}(s)   \\quad\\text{versus}\\quad   \\sum_{s\\in \\mathcal{S}}    \\mu_0(s)\\,   v_{\\pi_2}(s). \\] <p>\u2022 The policy that yields a higher overall expected return from the start (or on average w.r.t. \\(\\mu_0\\)) is deemed better.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 4) Practical Significance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 If you have solved the Bellman equation for both policies (i.e., found \\(v_{\\pi_1}\\) and \\(v_{\\pi_2}\\)), then you have a quantitative measure of performance for each state in \\(\\mathcal{S}\\). \u2022 You can use this information to decide which policy performs better in specific states, or from the viewpoint of a particular start state. \u2022 Often, this comparison step is part of Policy Iteration or other methods: one checks if an updated policy is better (in terms of the value function) than a previous one\u2014if it is, one replaces the old policy.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 5) Conclusion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Yes, once you solve the Bellman equations to get \\(v_{\\pi_1}\\) and \\(v_{\\pi_2}\\), you can evaluate and compare \\(\\pi_1\\) and \\(\\pi_2\\) by: 1. Checking their value functions state by state, if desired. 2. Checking the expected return from a specific start state or an initial state distribution.  </p> <p>The policy with the larger expected return (from the relevant initial condition) is considered better. If one policy has a higher value function in all states, it strictly dominates the other policy.</p>"},{"location":"chapter1/#return-value","title":"Return value","text":"<p>Below is a re-presentation of the concept of the action-value function, also known as the \u201cQ-value\u201d function, denoted by \\(q_{\\pi}\\):</p> <p>1) Motivation for \\(q_{\\pi}(s,a)\\)    \u2022 We already know that for a given policy \\(\\pi\\), the state-value function is:  </p> \\[ v_{\\pi}(s) \\;=\\; \\mathbb{E}\\bigl[G_t \\;\\big\\vert\\; S_t = s\\bigr]. \\] <p>\u2022 However, this quantity does not distinguish which action is taken in state \\(s\\); it only assumes that actions will be chosen according to \\(\\pi\\).    \u2022 If we want to evaluate taking a specific action \\(a\\) in state \\(s\\), we introduce the action-value function.</p> <p>2) Definition of the Action-Value    \u2022 The action-value function \\(q_{\\pi}(s,a)\\) is defined as:  </p> \\[ q_{\\pi}(s,a) \\;=\\; \\mathbb{E}\\bigl[G_t \\;\\big\\vert\\; S_t = s,\\; A_t = a,\\; \\pi\\bigr]. \\] <p>\u2022 This is the expected return obtained when:      1. We start in state \\(s\\).      2. We immediately take action \\(a\\).      3. Thereafter, we continue following policy \\(\\pi\\).  </p> <p>3) Relationship to \\(v_{\\pi}(s)\\)    \u2022 From the definition above, after we take action \\(a\\) in state \\(s\\), we end up in some next state \\(s'\\) and keep following \\(\\pi\\).    \u2022 If \\(\\pi\\) is stochastic, then  </p> \\[ v_{\\pi}(s)       \\;=\\;       \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\; q_{\\pi}(s,a). \\] <p>\u2022 Intuitively, \\(v_{\\pi}(s)\\) is the average of \\(q_{\\pi}(s,a)\\) over all possible actions \\(a\\), weighted by \\(\\pi(a \\mid s)\\).</p> <p>4) Bellman Equation for \\(q_{\\pi}\\)    \u2022 The Bellman expectation equation for \\(q_{\\pi}(s,a)\\) can be written as:  </p> \\[ q_{\\pi}(s,a)      \\;=\\;       \\mathbb{E}\\Bigl[        R_{t+1} \\;+\\; \\gamma \\, v_{\\pi}(S_{t+1})        \\;\\Big\\vert\\;        S_t = s,\\; A_t = a      \\Bigr]. \\] <p>\u2022 Equivalently, since  </p> \\[ v_{\\pi}(S_{t+1})      \\;=\\;      \\sum_{a'} \\pi(a' \\mid S_{t+1})\\,q_{\\pi}(S_{t+1},\\,a'), \\] <p>we can expand the right-hand side to express \\(q_{\\pi}\\) entirely in terms of immediate rewards and future \\(q_{\\pi}\\)-values.</p> <p>5) Why \\(q_{\\pi}\\) Matters    \u2022 Many RL algorithms, such as Q-learning and SARSA, center around learning an approximation of \\(q_{\\pi}(s,a)\\).    \u2022 Once \\(q_{\\pi}\\) is known (or estimated), choosing actions becomes straightforward: one can compare the values of different actions in the same state to see which leads to the greatest expected return.</p> <p>6) Summary    \u2022 The state-value function \\(v_{\\pi}(s)\\) follows a policy everywhere.    \u2022 The action-value function \\(q_{\\pi}(s,a)\\) is a finer-grained measure that tells us the expected return if we pick a specific action \\(a\\) now, then continue with \\(\\pi\\).    \u2022 Both functions are crucial to understanding and devising strategies in Reinforcement Learning.</p>"},{"location":"chapter1/#bellman-equations-for-q_pi-and-v_pi","title":"Bellman Equations for \\(q_{\\pi}\\) and \\(v_{\\pi}\\)","text":"<p>The step 4 above on the Bellman equation for \\(q_{\\pi}\\). Derive this with details and show the connection to the Bellman equation for \\(v_{\\pi}\\).</p> <p>Answer</p> <p>Below is a detailed derivation of the Bellman equation for the action-value function, denoted by \\(q_{\\pi}(s,a)\\),  showing its relationship to the Bellman equation for the state-value function \\(v_{\\pi}(s)\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>1) Recall the Definitions</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 State-Value Function (under policy \\(\\pi\\)):  </p> \\[ v_{\\pi}(s)   \\;=\\;   \\mathbb{E}\\bigl[G_t      \\;\\big\\vert\\;     S_t = s   \\bigr], \\] <p>which is the expected return when starting in state \\(s\\) at time \\(t\\) and thereafter following \\(\\pi\\).</p> <p>\u2022 Action-Value Function (under policy \\(\\pi\\)):  </p> \\[ q_{\\pi}(s,a)   \\;=\\;   \\mathbb{E}\\bigl[G_t     \\;\\big\\vert\\;     S_t = s,\\;     A_t = a,\\;     \\pi   \\bigr], \\] <p>which is the expected return when starting in state \\(s\\) at time \\(t\\), taking action \\(a\\) immediately, and then following \\(\\pi\\) thereafter.</p> <p>\u2022 Return \\(G_t\\):  </p> \\[ G_t   \\;=\\;   R_{t+1} \\;+\\;\\gamma R_{t+2} \\;+\\;\\gamma^2 R_{t+3} \\;+\\;\\dots   \\;=\\;   \\sum_{k=0}^{\\infty}   \\gamma^k \\, R_{t+k+1}. \\] <p>Here, \\(0 \\le \\gamma \\le 1\\) is the discount factor.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>2 One-Step Decomposition of the Return</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>We start with the relation  </p> \\[ G_t \\;=\\; R_{t+1} \\;+\\; \\gamma\\,G_{t+1}. \\] <p>Taking the conditional expectation on both sides given \\(\\bigl(S_t = s,\\; A_t = a,\\; \\pi\\bigr)\\):</p> \\[ q_{\\pi}(s,a) \\;=\\; \\mathbb{E}\\bigl[   G_t   \\;\\big\\vert\\;   S_t = s,\\;   A_t = a,\\;   \\pi \\bigr] \\;=\\; \\mathbb{E}\\Bigl[   R_{t+1} \\;+\\; \\gamma\\,G_{t+1}   \\;\\Big\\vert\\;   S_t = s,\\;   A_t = a,\\;   \\pi \\Bigr]. \\] <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>3 Separate Immediate Reward and Future Return</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>We can split the expectation:</p> \\[ q_{\\pi}(s,a) \\;=\\; \\underbrace{   \\mathbb{E}\\bigl[     R_{t+1}     \\,\\big\\vert\\,     S_t = s,\\;     A_t = a,\\;     \\pi   \\bigr] }_{\\text{expected immediate reward}} \\;+\\; \\gamma \\underbrace{   \\mathbb{E}\\bigl[     G_{t+1}     \\,\\big\\vert\\,     S_t = s,\\;     A_t = a,\\;     \\pi   \\bigr] }_{\\text{expected future discounted return}} . \\] <ol> <li>The term \\(\\mathbb{E}[\\,R_{t+1}\\,\\vert\\, s,a,\\pi]\\) is the expected reward for taking action \\(a\\) in state \\(s\\).  </li> <li>The term \\(\\mathbb{E}[\\,G_{t+1}\\,\\vert\\, s,a,\\pi]\\) signals that once we have taken action \\(a\\) in state \\(s\\), we transition to some state \\(s'\\), and from \\(t+1\\) onward we follow policy \\(\\pi\\). The expected future return from \\(s'\\) under \\(\\pi\\) is \\(v_{\\pi}(s')\\).</li> </ol> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>4 Transition to the Next State</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>The environment\u2019s transition function \\(P(s' \\mid s,a)\\) gives the probability of ending up in state \\(s'\\) after taking action \\(a\\) in state \\(s\\). Hence,</p> \\[ \\mathbb{E}\\bigl[   R_{t+1}   \\;\\big\\vert\\;   S_t = s,\\;   A_t = a \\bigr] \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\;\\vert\\; s,a)\\,\\;R(s,a,s'), \\] <p>assuming \\(R(s,a,s')\\) is the associated immediate reward for the transition \\((s,a)\\to s'\\). Similarly,</p> \\[ \\mathbb{E}\\bigl[   G_{t+1}   \\;\\big\\vert\\;   S_t = s,\\;   A_t = a,\\;   \\pi \\bigr] \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\;\\vert\\; s,a) \\,\\underbrace{   \\mathbb{E}[\\,G_{t+1}\\,\\vert\\,S_{t+1} = s',\\,\\pi] }_{v_{\\pi}(s')} , \\] <p>since once we arrive in \\(s'\\), we follow \\(\\pi\\) and the expected return is \\(v_{\\pi}(s')\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>5 The Bellman Equation for \\(q_{\\pi}(s,a)\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Putting these pieces together:</p> \\[ q_{\\pi}(s,a) \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\,\\vert\\, s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma \\, v_{\\pi}(s') \\Bigr]. \\] <p>This is the Bellman expectation equation for \\(q_{\\pi}(s,a)\\). It shows that the action-value under \\(\\pi\\) is the expected immediate reward plus the discounted value of the next state, averaged over all possible next states \\(s'\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>6 Connection to the Bellman Equation for \\(v_{\\pi}(s)\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>The state-value function \\(v_{\\pi}(s)\\) can be expressed in terms of \\(q_{\\pi}(s,a)\\):</p> \\[ v_{\\pi}(s) \\;=\\; \\sum_{a \\,\\in\\, \\mathcal{A}} \\pi(a \\,\\vert\\, s)\\;q_{\\pi}(s,a). \\] <p>In other words, \\(v_{\\pi}(s)\\) is the expectation of \\(q_{\\pi}(s,a)\\) over the policy \\(\\pi(a \\mid s)\\). Conversely, when we write \\(q_{\\pi}(s,a)\\) in its Bellman form, we use \\(v_{\\pi}(s')\\) to handle the future return once the agent transitions to the next state \\(s'\\):</p> \\[ q_{\\pi}(s,a) \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\,\\vert\\, s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma \\,\\underbrace{     \\sum_{a' \\,\\in\\, \\mathcal{A}}     \\pi(a' \\,\\vert\\, s')\\,q_{\\pi}(s',a')   }_{v_{\\pi}(s')} \\Bigr]. \\] <p>Thus, \\(q_{\\pi}\\) and \\(v_{\\pi}\\) are tied together through these two Bellman equations.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>7 Summary</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 We started from the relation \\(G_t = R_{t+1} + \\gamma G_{t+1}\\), conditioning on \\(\\{S_t = s, A_t = a\\}\\) and policy \\(\\pi\\). \u2022 We split the expectation into the immediate reward plus the discounted future return. \u2022 By summing over next states \\(s'\\) and using the definition of \\(v_{\\pi}(s')\\), we arrived at</p> \\[ q_{\\pi}(s,a) \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\,\\vert\\, s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma\\,v_{\\pi}(s') \\Bigr]. \\] <p>\u2022 Since \\(v_{\\pi}(s') = \\sum_{a'} \\pi(a' \\mid s')\\,q_{\\pi}(s',a')\\), we see the close relationship between these two value functions. This completes the derivation of the Bellman equation for the action-value function \\(q_{\\pi}(s,a)\\) and clarifies its connection to the Bellman equation for \\(v_{\\pi}(s)\\).</p>"},{"location":"chapter1/#optimal-policy","title":"Optimal Policy","text":"<p>Now that we have the concepts of the state value \\(v_{\\pi}\\) and the return value \\(q_{\\pi}\\) for a given policy \\(\\pi\\), and the associated Bellman equations, there is the concept of the optimal policy \\(\\pi^*\\). There is an associated concept of the Bellman Optimality Equation. Explain these concepts.</p> <p>Answer</p> <p>Below is a step-by-step explanation of the concept of an optimal policy in Reinforcement Learning, and how it leads to the Bellman Optimality Equations.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>1 Policy and Value Functions</p> <p>\u2022 A policy \\(\\pi\\) is any mapping from states to action probabilities (or actions, if deterministic). \u2022 The state-value function for a given policy \\(\\pi\\) is  </p> \\[ v_{\\pi}(s) \\;=\\; \\mathbb{E}\\bigl[G_t \\;\\big\\vert\\; S_t = s,\\;\\pi \\bigr], \\] <p>where \\(G_t\\) is the return from time \\(t\\) onward. \u2022 The action-value function for \\(\\pi\\) is  </p> \\[ q_{\\pi}(s,a) \\;=\\; \\mathbb{E}\\bigl[G_t \\;\\big\\vert\\; S_t = s,\\; A_t = a,\\;\\pi \\bigr]. \\] <p>These value functions describe how \u201cgood\u201d it is to be in a particular state \\(s\\)\u2014or to take a particular action \\(a\\) in \\(s\\)\u2014under the policy \\(\\pi\\).</p> <p>2 Optimal Policy \\(\\pi^*\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 The goal in Reinforcement Learning is often to find a policy \\(\\pi^*\\) that maximizes the expected return from every state. \u2022 Formally, \\(\\pi^*\\) is said to be optimal if, for all states \\(s\\in \\mathcal{S}\\),</p> \\[ v_{\\pi^*}(s) \\;=\\; \\max_{\\pi} \\; v_{\\pi}(s). \\] <p>Equivalently, \\(\\pi^*\\) achieves or exceeds the performance of every other policy \\(\\pi\\). \u2022 There can be multiple optimal policies, but they share the same value functions, denoted \\(v_*\\) and \\(q_*\\), defined below.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>3 Optimal Value Functions</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>We define  </p> \\[ v_*(s) \\;=\\; \\max_{\\pi} \\;v_{\\pi}(s), \\] <p>and  </p> \\[ q_*(s,a) \\;=\\; \\max_{\\pi} \\;q_{\\pi}(s,a). \\] <p>\u2022 Thus, \\(v_*(s)\\) is the maximum possible expected return achievable from state \\(s\\), over all policies. \u2022 Similarly, \\(q_*(s,a)\\) is the maximum possible expected return when starting from \\(s\\), taking action \\(a\\), and thereafter following the best possible policy.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>4 Bellman Optimality Equations</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 Just as there are Bellman equations for \\(v_{\\pi}\\) and \\(q_{\\pi}\\) under a fixed policy \\(\\pi\\), there are corresponding Bellman Optimality Equations for \\(v_*\\) and \\(q_*\\). \u2022 For the optimal state-value function \\(v_*(s)\\), the Bellman Optimality Equation is:</p> \\[ v_*(s) \\;=\\; \\max_{a \\,\\in\\,\\mathcal{A}} \\;\\biggl[   \\sum_{s' \\,\\in\\,\\mathcal{S}}   P(s' \\mid s,a)\\,   \\Bigl(     R(s,a,s') \\;+\\;\\gamma\\,v_*(s')   \\Bigr) \\biggr]. \\] <p>\u2022 In words: from state \\(s\\), pick the action \\(a\\) that gives the highest expected immediate reward plus the discounted optimal value of the next state.</p> <p>\u2022 For the optimal action-value function \\(q_*(s,a)\\), the Bellman Optimality Equation is:</p> \\[ q_*(s,a) \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\mid s,a) \\Bigl[   R(s,a,s') \\;+\\;   \\gamma \\,\\max_{a'\\,\\in\\,\\mathcal{A}} \\,q_*(s',a') \\Bigr]. \\] <p>\u2022 Here, after taking action \\(a\\) in state \\(s\\) to reach \\(s'\\), we assume the agent chooses the (optimal) action \\(a' = \\arg\\max_{\\,a'} q_*(s',a')\\) thereafter.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>5 Connection to the Concept of Optimality</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 If the agent somehow knows \\(q_*\\), it can act optimally by choosing, in each state \\(s\\), the action \\(a\\) that maximizes \\(q_*(s,a)\\). \u2022 Equivalently, if the agent knows \\(v_*\\) and the environment\u2019s transition probabilities, then to select the optimal action from a state \\(s\\), one can compute the action that attains the maximum in</p> \\[ \\max_{a}\\;\\bigl[   \\sum_{s'}     P(s' \\mid s,a)\\,     (\\,R(s,a,s') + \\gamma\\,v_*(s')\\,) \\bigr]. \\] <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>6 Summary</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <ol> <li>An optimal policy \\(\\pi^*\\) is one that achieves the highest possible value in every state (or equivalently, for every state-action pair).  </li> <li>The optimal value functions \\(v_*\\) and \\(q_*\\) are defined as the supremum over all policies of \\(v_{\\pi}\\) and \\(q_{\\pi}\\), respectively.  </li> <li>They satisfy the Bellman Optimality Equations, which replace the explicit dependence on \\(\\pi\\) with a maximization over actions.  </li> <li>Solving these equations in principle yields \\(v_*\\) (or \\(q_*\\)) and hence identifies which actions are optimal in each state, leading to an optimal policy.</li> </ol>"},{"location":"chapter1/#bellman-optimality-equation","title":"Bellman Optimality Equation","text":"<p>The selection of an optimal policy implies choosing the maximum or supremum over all policies. However, in step 4 describing the Bellman Optimality Equation, there does not seem to be an explicit selection across the set of policies. Perhaps, this is an outcome. If so, derive in detail, how the Bellman Optimality Equation follows from the notion of optimal value functions.</p> <p>Answer</p> <p>Below is a detailed derivation showing how the notion of \u201cmaximizing over all policies\u201d leads to the Bellman Optimality Equations, using LaTeX for all mathematical symbols and expressions.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>1 Optimal Value Functions</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>We define the optimal state-value function and action-value function as follows:</p> <p>\u2022 Optimal state-value function:  </p> \\[ v_*(s)  \\;=\\;  \\max_{\\pi}\\;v_{\\pi}(s), \\] <p>where $v_{\\pi}(s) = \\mathbb{E}!\\bigl[G_t \\,\\vert\\,S_t = s,\\;\\pi\\bigr] $</p> <p>\u2022 Optimal action-value function:</p> \\[ q_*(s,a)  \\;=\\; \\max_{\\pi}\\;q_{\\pi}(s,a), \\] <p>where \\(q_{\\pi}(s,a) = \\mathbb{E}\\!\\bigl[G_t \\,\\vert\\,S_t = s,\\;A_t = a,\\;\\pi\\bigr]\\)</p> <p>The notation \u201c\\(\\max_{\\pi}\\)\u201d means we take the supremum (maximum) over all possible policies \\(\\pi\\). Under standard conditions (e.g., finite state space, bounded rewards, or \\(\\gamma&lt;1\\)), these maxima exist and define \\(v_*\\) and \\(q_*\\).</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>2 Bellman Equation for a Given Policy \\(\\pi\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>For reference, recall that for any fixed policy \\(\\pi\\), the Bellman expectation equations are:</p> <p>\u2022 State-value:</p> \\[ v_{\\pi}(s) \\;=\\; \\sum_{a \\,\\in\\, \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\,\\in\\, \\mathcal{S}} P\\bigl(s' \\mid s,a\\bigr)\\, \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma\\,v_{\\pi}(s') \\Bigr]. \\] <p>\u2022 Action-value:</p> \\[ q_{\\pi}(s,a) \\;=\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P\\bigl(s' \\mid s,a\\bigr) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma \\,v_{\\pi}(s') \\Bigr]. \\] <p>These follow directly from the fact that \\(G_t = R_{t+1} + \\gamma\\,G_{t+1}\\) and taking expectations conditioned on \\((S_t=s, A_t=a)\\) or \\((S_t=s)\\) respectively.</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>3 Maximizing Over Policies for \\(v_*\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>By definition,</p> \\[ v_*(s) \\;=\\; \\max_{\\pi}\\,v_{\\pi}(s). \\] <p>Substitute the Bellman equation for \\(v_{\\pi}(s)\\):</p> \\[ v_*(s) \\;=\\; \\max_{\\pi} \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a)\\, \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma\\,v_{\\pi}(s') \\Bigr]. \\] <p>Notice that for each fixed state \\(s\\), a policy \\(\\pi\\) chooses a distribution \\(\\{\\pi(a \\mid s)\\}\\) over actions \\(a\\). But taking a probability-weighted sum of values cannot exceed simply picking the single best action. Formally,</p> \\[ \\max_{\\pi}\\, \\sum_{a} \\pi(a \\mid s)\\,f(a) \\;=\\; \\max_{a}\\,f(a), \\] <p>for any function \\(f\\). Let</p> \\[ f(a) \\;=\\; \\sum_{s'} P(s' \\mid s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma \\,\\underbrace{v_{\\pi}(s')}_{\\le v_*(s')} \\Bigr]. \\] <p>Because \\(v_{\\pi}(s') \\le v_*(s')\\) for all \\(\\pi\\) and \\(s'\\), the best we can do is to replace \\(v_{\\pi}(s')\\) by \\(v_*(s')\\). Thus,</p> \\[ v_*(s) \\;=\\; \\max_{a} \\sum_{s'} P(s' \\mid s,a)\\, \\bigl[   R(s,a,s')   \\;+\\;   \\gamma\\,v_*(s') \\bigr]. \\] <p>This yields the Bellman Optimality Equation for \\(v_*(s)\\):</p> \\[ \\boxed{ v_*(s) \\;=\\; \\max_{a \\,\\in\\, \\mathcal{A}}\\; \\sum_{s' \\,\\in\\, \\mathcal{S}} P(s' \\mid s,a)\\; \\bigl[   R(s,a,s')   \\;+\\;\\gamma\\,v_*(s') \\bigr]. } \\] <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>4 Bellman Optimality Equation for \\(q_*\\)</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>Similarly, from</p> \\[ q_{\\pi}(s,a) \\;=\\; \\sum_{s'} P(s' \\mid s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma\\,v_{\\pi}(s') \\Bigr], \\] <p>we define</p> \\[ q_*(s,a) \\;=\\; \\max_{\\pi}\\; q_{\\pi}(s,a). \\] <p>But we already have</p> \\[ v_{\\pi}(s') \\;\\le\\; v_*(s'), \\] <p>and</p> \\[ v_*(s') \\;=\\; \\max_{a'}\\; q_*(s',a'). \\] <p>Hence, by substituting \\(v_*(s') = \\max_{a'} q_*(s',a')\\), we arrive at</p> \\[ q_*(s,a) \\;=\\; \\sum_{s'} P(s' \\mid s,a) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma\\;\\max_{a'}\\;q_*(s',a') \\Bigr]. \\] <p>This is the Bellman Optimality Equation for the action-value function:</p> \\[ \\boxed{ q_*(s,a) \\;=\\; \\sum_{s'\\,\\in\\,\\mathcal{S}} P\\bigl(s' \\mid s,a\\bigr) \\Bigl[   R(s,a,s')   \\;+\\;   \\gamma   \\,\\max_{a'\\,\\in\\,\\mathcal{A}}   q_*(s',a') \\Bigr]. } \\] <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>5 Interpretation and Summary</p> <p>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500</p> <p>\u2022 By defining \\(v_*\\) and \\(q_*\\) as the maximum over all policies of the corresponding value functions, we effectively pick the best possible action choices at each state. \u2022 The Markov Decision Process structure means that \u201cmaximizing over policies\u201d can be carried out state by state, leading to a pointwise maximization over actions in the Bellman Optimality Equations. \u2022 Once one computes or approximates \\(q_*\\), one obtains an optimal policy simply by greedily choosing the action that maximizes \\(q_*(s,a)\\) in each state \\(s\\). Equivalently, from \\(v_*\\) and knowledge of the environment\u2019s transitions and rewards, one can also identify the best actions.</p> <p>Thus, the Bellman Optimality Equations naturally arise from the desire to choose the best action in each state\u2014an outcome of maximizing over all possible policies in an MDP.</p>"},{"location":"chapter2/","title":"Getting Started","text":"<p>This chapter covers the basics.</p>"},{"location":"chapter2/#basic-math-examples","title":"Basic Math Examples","text":"<p>Inline math: \\(\\alpha^2 + \\beta^2 = \\gamma^2\\) or \\(E=mc^2\\).</p> <p>Display math:</p> \\[  \\int_0^1 x^2 dx = \\frac{1}{3} \\] <p>Or using LaTeX delimiters:</p> \\[ \\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6} \\] <p>Testing 1 2 3</p>"},{"location":"chapter2/#code-snippet","title":"Code Snippet","text":"<p>```python print(\"Hello, World!\")</p>"}]}